{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Hazem Bin Ryaz Patel (2200550)\n",
    "Class : DAAA/2B/07 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import visualkeras\n",
    "import keras\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.layers import (\n",
    "    AveragePooling2D,\n",
    "    ZeroPadding2D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    MaxPool2D,\n",
    "    Add,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_r = 42\n",
    "np.random.seed(seed_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Dataset for CA1 part A\"\n",
    "# image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir + \"/train\",\n",
    "    seed=seed_r,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir + \"/validation\",\n",
    "    seed=seed_r,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir + \"/test\",\n",
    "    seed=seed_r,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for images, labels in train_ds:\n",
    "    x_train.extend(images.numpy())\n",
    "    y_train.extend(labels.numpy())\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(8, 5), tight_layout=True)\n",
    "\n",
    "for label, subplot in enumerate(ax.ravel()):\n",
    "    subplot.axis(\"off\")\n",
    "    subplot.imshow(\n",
    "        x_train[y_train == label][\n",
    "            np.random.randint(0, len(x_train[y_train == label]))\n",
    "        ].astype(\"uint8\"),\n",
    "        cmap=\"Greys\",\n",
    "    )\n",
    "    subplot.set_title(class_names[label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for mislabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(15, 10, figsize=(15, 20))\n",
    "for i in range(15):\n",
    "    images = x_train[np.squeeze(y_train == i)].astype(\"uint8\")\n",
    "    random_index = np.random.choice(images.shape[0], 15, replace=False)\n",
    "    images = images[random_index]\n",
    "    label = class_names[i]\n",
    "    for j in range(10):\n",
    "        subplot = ax[i, j]\n",
    "        subplot.axis(\"off\")\n",
    "        subplot.imshow(images[j], cmap=\"Greys\")\n",
    "        subplot.set_title(label, fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : There isn't any mislabelling, so we don't need to re-label any of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(20, 10))\n",
    "\n",
    "for idx, subplot in enumerate(ax.ravel()):\n",
    "    avg_image = np.mean(x_train[np.squeeze(y_train == idx)], axis=0) / 255\n",
    "    subplot.imshow(avg_image, cmap=\"Greys\")\n",
    "    subplot.set_title(f\"{class_names[idx]}\")\n",
    "    subplot.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some exhibit discernible colors, most appear as a ball of vibrant green hues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"{class_names[label]}: {count}\")\n",
    "\n",
    "plt.barh(labels, counts, tick_label=class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling with the use of Data Augmentations\n",
    "One of the things that is important to address is the imbalance of data. I've chosen to augment the data to help with the disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {tf.Tensor.ref(img): label for img, label in train_ds.unbatch()}\n",
    "\n",
    "\n",
    "def data_augmentation(data):\n",
    "    imageArr = []\n",
    "    for images in data:\n",
    "        image = tf.image.random_flip_left_right(images)\n",
    "        image = tf.image.random_crop(image, size=(224, 224, 3))\n",
    "        imageArr.append(tf.reshape(image, (224, 224, 3)))\n",
    "    return np.array(imageArr)\n",
    "\n",
    "def augment_undersampled_vegs(img_labels, X_train, y_train):\n",
    "    undersampled_labels = []\n",
    "    undersampled_vegs = []\n",
    "    for veg_type in img_labels:\n",
    "        # Get all images of a veg type\n",
    "        veg_images = [\n",
    "            img.deref() for img, label in train_dict.items() if label == veg_type\n",
    "        ]\n",
    "        veg_labels = [label for img, label in train_dict.items() if label == veg_type]\n",
    "\n",
    "        if veg_type == img_labels[0]:\n",
    "            undersampled_vegs = veg_images\n",
    "            undersampled_labels = veg_labels\n",
    "        else:\n",
    "            undersampled_vegs = np.concatenate((undersampled_vegs, veg_images), axis=0)\n",
    "            undersampled_labels = np.concatenate(\n",
    "                (undersampled_labels, veg_labels), axis=0\n",
    "            )\n",
    "\n",
    "        veg_train_aug = data_augmentation(undersampled_vegs)\n",
    "\n",
    "    print(veg_train_aug.shape)\n",
    "    print(undersampled_labels.shape)\n",
    "\n",
    "    X_train = np.concatenate((X_train, veg_train_aug), axis=0)\n",
    "    y_train = np.concatenate((y_train, undersampled_labels), axis=0)\n",
    "    return X_train, y_train\n",
    "\n",
    "veg_types = [2, 5, 6, 7, 10, 11, 13]\n",
    "X_train_aug, y_train_aug = augment_undersampled_vegs(veg_types, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    train_ds_rebatch = tf.data.Dataset.from_tensor_slices((X_train_aug, y_train_aug))\n",
    "    train_ds_rebatch = train_ds_rebatch.shuffle(buffer_size=len(X_train_aug))  # Shuffle the data\n",
    "    train_ds_rebatch = train_ds_rebatch.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(ds):\n",
    "    ds = ds.map(lambda x, y: (tf.image.rgb_to_grayscale(x), y))\n",
    "    ds = ds.map(lambda x, y: (tf.image.resize(x, (128, 128)), y))\n",
    "    return ds\n",
    "\n",
    "train_non_aug_ds_128 = process(train_ds)\n",
    "train_ds_128 = process(train_ds_rebatch)\n",
    "val_ds_128 = process(val_ds)\n",
    "test_ds_128 = process(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA after pre-processing\n",
    "How do the images look now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_128 = []\n",
    "y_train_128 = []\n",
    "\n",
    "for images, labels in train_ds_128:\n",
    "    x_train_128.extend(images.numpy())\n",
    "    y_train_128.extend(labels.numpy())\n",
    "\n",
    "x_train_128 = np.array(x_train_128)\n",
    "y_train_128 = np.array(y_train_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(y_train_128, return_counts=True)\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"{class_names[label]}: {count}\")\n",
    "\n",
    "plt.barh(labels, counts, tick_label=class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(8, 5), tight_layout=True)\n",
    "\n",
    "for idx, subplot in enumerate(ax.ravel()):\n",
    "    avg_image = np.mean(x_train_128[np.squeeze(y_train_128 == idx)], axis=0) / 255\n",
    "    subplot.imshow(avg_image, cmap=\"Greys\")\n",
    "    subplot.set_title(f\"{class_names[idx]}\")\n",
    "    subplot.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(8, 5), tight_layout=True)\n",
    "\n",
    "for label, subplot in enumerate(ax.ravel()):\n",
    "    subplot.axis(\"off\")\n",
    "    subplot.imshow(\n",
    "        x_train_128[y_train_128 == label][\n",
    "            np.random.randint(0, len(x_train_128[y_train_128 == label]))\n",
    "        ],\n",
    "        cmap=\"Greys\",\n",
    "    )\n",
    "    subplot.set_title(class_names[label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for CNN 128\n",
    "Element of modelling to consider\n",
    "- With Augmentation or W/out\n",
    "- Does Class_Weight_Dict help the augmentation or not much difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes = np.unique(y_train_128), y = y_train_128)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(layer_number):\n",
    "    layer_number = 5 + layer_number\n",
    "    conv_filter1 = 2**layer_number\n",
    "    ks = (7,7) if layer_number == 0 else (5,5) if layer_number == 1 else (3,3)    \n",
    "    return Sequential(\n",
    "        [\n",
    "            Conv2D(conv_filter1, ks, padding=\"same\", activation=\"relu\"),\n",
    "            BatchNormalization()\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_128():  \n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(4):\n",
    "        model.add(conv2d_block(i))\n",
    "        if ((i+1) % 2 == 0):\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "            model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(15, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model_128 = base_model_128()\n",
    "history_128 = model_128.fit(\n",
    "    train_ds_128,\n",
    "    validation_data=val_ds_128,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min'), \n",
    "    ModelCheckpoint('model_128.h5', monitor='val_loss', save_best_only=True, verbose=1)]\n",
    ")\n",
    "model_128.evaluate(test_ds_128)\n",
    "model_128.summary()\n",
    "plot_model(model_128, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_128.history[\"accuracy\"])\n",
    "plt.plot(history_128.history[\"val_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_128.history[\"loss\"])\n",
    "plt.plot(history_128.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Augmentation / Class Weights help the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_128_model(batchsize, dropout, dense):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(64, 7, input_shape=(128, 128, 1), padding=\"same\", activation=\"relu\")\n",
    "    )\n",
    "\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, 5, padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "    model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Flatten the feature map\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add the fully connected layers\n",
    "    model.add(Dense(dense, activation=\"relu\"))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(15, activation=\"softmax\"))\n",
    "\n",
    "    # Compile your model with your optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_ds_128,\n",
    "        validation_data=val_ds_128,\n",
    "        epochs=20,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=5,\n",
    "                verbose=1,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Evaluate model on unseen data\n",
    "    scores = model.evaluate(test_ds_128)\n",
    "    testError = 100 - scores[1] * 100\n",
    "    return history, scores[1], testError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(batch_sizes, dropouts, dense_sizes):\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for dense_size in dense_sizes:\n",
    "                history, accuracy, test_error = tune_128_model(\n",
    "                    batch_size, dropout, dense_size\n",
    "                )\n",
    "                \n",
    "                # Store the results\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"dropout\": dropout,\n",
    "                        \"dense_size\": dense_size,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"test_error\": test_error,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "batch_sizes = [64, 128]\n",
    "dropouts = [0.1, 0.2, 0.5]\n",
    "dense_sizes = [256, 512, 1024]\n",
    "\n",
    "\n",
    "# Run the grid search\n",
    "results = grid_search(batch_sizes, dropouts, dense_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best score and best params\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "# Iterate over the results\n",
    "for result in results:\n",
    "    # If the current score is better than the best score\n",
    "    if result[\"accuracy\"] > best_score:\n",
    "        # Update the best score and best params\n",
    "        best_score = result[\"accuracy\"]\n",
    "        best_params = {\n",
    "            \"batch_size\": result[\"batch_size\"],\n",
    "            \"dropout\": result[\"dropout\"],\n",
    "            \"dense_size\": result[\"dense_size\"],\n",
    "        }\n",
    "\n",
    "# Print the best params\n",
    "print('Best params:', best_params)\n",
    "print('Best accuracy:', best_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the other dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalmodel():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(64, 7, input_shape=(128, 128, 1), padding=\"same\", activation=\"relu\")\n",
    "    )\n",
    "\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, 5, padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Flatten the feature map\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add the fully connected layers\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(15, activation=\"softmax\"))\n",
    "\n",
    "    # Compile your model with your optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_128_final = finalmodel()\n",
    "history_128_final = model_128_final.fit(\n",
    "    train_ds_128,\n",
    "    validation_data=val_ds_128,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, mode=\"min\"),\n",
    "        ModelCheckpoint(\n",
    "            \"model_128_final.h5\", monitor=\"val_loss\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in model from .h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Create a new instance of the model\n",
    "model = finalmodel()\n",
    "\n",
    "# Load the weights from the .h5 file\n",
    "model.load_weights(\"model_128_final_2.h5\")\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_ds_128)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "for i in range(12):  # Change this to the number of times you want to retrain the model\n",
    "    print(f\"Training run {i+1}\")\n",
    "    # Create a new instance of the model\n",
    "    model_128_final = finalmodel()\n",
    "\n",
    "    # Train the model\n",
    "    history_128_final = model_128_final.fit(\n",
    "        train_ds_128,\n",
    "        validation_data=val_ds_128,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1, mode=\"min\")\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Get the best validation loss in this training run\n",
    "    current_loss = min(history_128_final.history[\"val_loss\"])\n",
    "    print(f\"Current loss: {current_loss}\")\n",
    "\n",
    "    # If the current loss is better than the best loss seen so far, update the best loss and save the model\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        best_model = model_128_final\n",
    "        best_model.save(\"model_128_final_3.h5\")\n",
    "        print(f\"New best loss: {best_loss}\")\n",
    "\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
