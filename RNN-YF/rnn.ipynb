{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Radiate acceptance, and find peace in embracin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Quotes\n",
       "count                                                1000\n",
       "unique                                                890\n",
       "top     Radiate acceptance, and find peace in embracin...\n",
       "freq                                                    5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 quotes in the dataset, with some repeating quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove repeated quotes so as to not have biases\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Embrace the beauty of every sunrise; it's a fresh chance to paint your world with joy.\",\n",
       " 'Embrace challenges; they are the stepping stones to your greatest victories.',\n",
       " 'Embrace the rhythm of life and let it dance through your soul.',\n",
       " 'Embrace kindness, for it has the power to change the world one heart at a time.',\n",
       " 'Embrace the journey, for it leads to the destination of your dreams.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(data.Quotes.values)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation.\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer() # Instantiate a Tokenizer object.\n",
    "tokenizer.fit_on_texts(data) # Fit your Tokenizer onto dataset. This will map words in your dataset to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'your': 3,\n",
       " 'and': 4,\n",
       " 'a': 5,\n",
       " 'is': 6,\n",
       " 'in': 7,\n",
       " 'for': 8,\n",
       " 'let': 9,\n",
       " 'to': 10,\n",
       " 'it': 11,\n",
       " 'be': 12,\n",
       " 'every': 13,\n",
       " 'our': 14,\n",
       " 'you': 15,\n",
       " 'that': 16,\n",
       " 'embrace': 17,\n",
       " \"life's\": 18,\n",
       " 'this': 19,\n",
       " 'are': 20,\n",
       " 'morning': 21,\n",
       " 'with': 22,\n",
       " 'radiate': 23,\n",
       " 'dance': 24,\n",
       " 'heart': 25,\n",
       " 'believe': 26,\n",
       " 'yourself': 27,\n",
       " 'through': 28,\n",
       " \"planet's\": 29,\n",
       " 'will': 30,\n",
       " 'life': 31,\n",
       " 'love': 32,\n",
       " 'they': 33,\n",
       " \"singapore's\": 34,\n",
       " 'kindness': 35,\n",
       " 'power': 36,\n",
       " 'from': 37,\n",
       " 'dreams': 38,\n",
       " 'we': 39,\n",
       " 'soul': 40,\n",
       " 'symphony': 41,\n",
       " 'act': 42,\n",
       " 'find': 43,\n",
       " 'gratitude': 44,\n",
       " 'singapore': 45,\n",
       " 'world': 46,\n",
       " 'strength': 47,\n",
       " 'light': 48,\n",
       " 'beauty': 49,\n",
       " 'journey': 50,\n",
       " 'nature': 51,\n",
       " 'joy': 52,\n",
       " 'planet': 53,\n",
       " 'canvas': 54,\n",
       " 'colors': 55,\n",
       " 'way': 56,\n",
       " 'whispers': 57,\n",
       " 'where': 58,\n",
       " 'potential': 59,\n",
       " 'hope': 60,\n",
       " 'testament': 61,\n",
       " 'resilience': 62,\n",
       " 'towards': 63,\n",
       " 'true': 64,\n",
       " 'new': 65,\n",
       " 'compassion': 66,\n",
       " 'beacon': 67,\n",
       " 'actions': 68,\n",
       " 'future': 69,\n",
       " 'spirit': 70,\n",
       " 'step': 71,\n",
       " 'change': 72,\n",
       " 'wisdom': 73,\n",
       " 'moments': 74,\n",
       " 'promise': 75,\n",
       " 'garden': 76,\n",
       " 'sunrise': 77,\n",
       " 'laughter': 78,\n",
       " 'story': 79,\n",
       " 'hearts': 80,\n",
       " 'day': 81,\n",
       " 'hold': 82,\n",
       " 'an': 83,\n",
       " 'tapestry': 84,\n",
       " 'heartbeat': 85,\n",
       " 'compass': 86,\n",
       " 'gift': 87,\n",
       " 'force': 88,\n",
       " 'inner': 89,\n",
       " 'each': 90,\n",
       " 'on': 91,\n",
       " 'holds': 92,\n",
       " 'destiny': 93,\n",
       " 'into': 94,\n",
       " 'self': 95,\n",
       " 'legacy': 96,\n",
       " 'when': 97,\n",
       " 'lion': 98,\n",
       " 'city': 99,\n",
       " 'skyline': 100,\n",
       " 'paint': 101,\n",
       " 'challenge': 102,\n",
       " 'determination': 103,\n",
       " 'tranquility': 104,\n",
       " 'others': 105,\n",
       " 'moment': 106,\n",
       " 'forgiveness': 107,\n",
       " 'melody': 108,\n",
       " 'any': 109,\n",
       " 'opportunities': 110,\n",
       " 'seeds': 111,\n",
       " 'within': 112,\n",
       " 'chapters': 113,\n",
       " 'growth': 114,\n",
       " 'purpose': 115,\n",
       " 'authenticity': 116,\n",
       " 'peace': 117,\n",
       " 'brings': 118,\n",
       " 'rhythm': 119,\n",
       " 'universe': 120,\n",
       " 'can': 121,\n",
       " 'reality': 122,\n",
       " 'overcome': 123,\n",
       " 'empathy': 124,\n",
       " 'serenity': 125,\n",
       " 'leave': 126,\n",
       " 'vibrant': 127,\n",
       " 'aspirations': 128,\n",
       " 'breath': 129,\n",
       " 'watch': 130,\n",
       " 'blessings': 131,\n",
       " 'rain': 132,\n",
       " 'music': 133,\n",
       " 'beginnings': 134,\n",
       " 'treasure': 135,\n",
       " 'have': 136,\n",
       " 'shape': 137,\n",
       " 'possibility': 138,\n",
       " 'positivity': 139,\n",
       " 'create': 140,\n",
       " 'carries': 141,\n",
       " 'bridge': 142,\n",
       " 'path': 143,\n",
       " 'nurtured': 144,\n",
       " 'us': 145,\n",
       " 'open': 146,\n",
       " 'reminder': 147,\n",
       " 'challenges': 148,\n",
       " 'has': 149,\n",
       " 'joyful': 150,\n",
       " 'more': 151,\n",
       " 'transform': 152,\n",
       " 'brighter': 153,\n",
       " 'foundation': 154,\n",
       " 'confidence': 155,\n",
       " 'guides': 156,\n",
       " 'guiding': 157,\n",
       " 'progress': 158,\n",
       " 'painted': 159,\n",
       " 'threads': 160,\n",
       " 'opportunity': 161,\n",
       " 'conservation': 162,\n",
       " 'fresh': 163,\n",
       " 'key': 164,\n",
       " 'success': 165,\n",
       " 'carry': 166,\n",
       " 'smile': 167,\n",
       " 'up': 168,\n",
       " 'its': 169,\n",
       " 'connection': 170,\n",
       " 'diversity': 171,\n",
       " 'touch': 172,\n",
       " 'masterpiece': 173,\n",
       " 'possibilities': 174,\n",
       " 'around': 175,\n",
       " 'sanctuary': 176,\n",
       " 'boundless': 177,\n",
       " 'star': 178,\n",
       " 'take': 179,\n",
       " 'shine': 180,\n",
       " 'by': 181,\n",
       " 'flourishes': 182,\n",
       " \"park's\": 183,\n",
       " 'hear': 184,\n",
       " 'stillness': 185,\n",
       " 'than': 186,\n",
       " 'experience': 187,\n",
       " 'wind': 188,\n",
       " 'lead': 189,\n",
       " 'magic': 190,\n",
       " 'energy': 191,\n",
       " 'being': 192,\n",
       " 'passions': 193,\n",
       " 'truest': 194,\n",
       " 'presence': 195,\n",
       " 'behind': 196,\n",
       " 'courage': 197,\n",
       " 'essence': 198,\n",
       " 'ignites': 199,\n",
       " 'grace': 200,\n",
       " 'echoes': 201,\n",
       " 'forward': 202,\n",
       " 'woven': 203,\n",
       " \"someone's\": 204,\n",
       " 'generosity': 205,\n",
       " 'innovation': 206,\n",
       " 'sunset': 207,\n",
       " 'pulau': 208,\n",
       " 'paints': 209,\n",
       " \"reserve's\": 210,\n",
       " 'present': 211,\n",
       " 'how': 212,\n",
       " 'language': 213,\n",
       " 'even': 214,\n",
       " 'darkest': 215,\n",
       " 'days': 216,\n",
       " 'unfold': 217,\n",
       " 'song': 218,\n",
       " 'turns': 219,\n",
       " 'adventure': 220,\n",
       " 'stories': 221,\n",
       " 'word': 222,\n",
       " 'transformation': 223,\n",
       " 'souls': 224,\n",
       " 'faith': 225,\n",
       " 'inspire': 226,\n",
       " 'abundance': 227,\n",
       " 'mark': 228,\n",
       " 'spark': 229,\n",
       " 'endless': 230,\n",
       " 'storms': 231,\n",
       " 'flight': 232,\n",
       " 'stronger': 233,\n",
       " 'dream': 234,\n",
       " 'source': 235,\n",
       " 'experiences': 236,\n",
       " 'made': 237,\n",
       " 'brightens': 238,\n",
       " 'connects': 239,\n",
       " 'tall': 240,\n",
       " 'reminds': 241,\n",
       " 'warmth': 242,\n",
       " 'dawn': 243,\n",
       " 'biodiversity': 244,\n",
       " 'protect': 245,\n",
       " 'offers': 246,\n",
       " 'leads': 247,\n",
       " 'words': 248,\n",
       " 'simplicity': 249,\n",
       " 'secrets': 250,\n",
       " 'positive': 251,\n",
       " 'well': 252,\n",
       " 'stars': 253,\n",
       " 'intuition': 254,\n",
       " 'humanity': 255,\n",
       " 'driving': 256,\n",
       " 'inspiration': 257,\n",
       " 'discovery': 258,\n",
       " 'care': 259,\n",
       " 'expression': 260,\n",
       " 'armor': 261,\n",
       " 'curiosity': 262,\n",
       " 'enthusiasm': 263,\n",
       " 'echo': 264,\n",
       " 'capable': 265,\n",
       " 'obstacle': 266,\n",
       " 'worthy': 267,\n",
       " 'discover': 268,\n",
       " 'composed': 269,\n",
       " 'blooms': 270,\n",
       " 'strokes': 271,\n",
       " 'mosaic': 272,\n",
       " 'classroom': 273,\n",
       " 'learn': 274,\n",
       " 'book': 275,\n",
       " 'written': 276,\n",
       " 'knows': 277,\n",
       " 'happiness': 278,\n",
       " 'creates': 279,\n",
       " 'faced': 280,\n",
       " 'creating': 281,\n",
       " 'start': 282,\n",
       " 'effort': 283,\n",
       " 'friendship': 284,\n",
       " 'formed': 285,\n",
       " 'choice': 286,\n",
       " 'brushstroke': 287,\n",
       " 'thought': 288,\n",
       " 'liberation': 289,\n",
       " 'investment': 290,\n",
       " 'tribute': 291,\n",
       " 'marvel': 292,\n",
       " 'gentle': 293,\n",
       " 'diverse': 294,\n",
       " 'bukit': 295,\n",
       " 'unity': 296,\n",
       " 'coastal': 297,\n",
       " \"nature's\": 298,\n",
       " 'time': 299,\n",
       " 'truly': 300,\n",
       " 'unlocking': 301,\n",
       " 'free': 302,\n",
       " 'ever': 303,\n",
       " 'beautiful': 304,\n",
       " 'genuine': 305,\n",
       " 'guide': 306,\n",
       " 'what': 307,\n",
       " 'lived': 308,\n",
       " 'pages': 309,\n",
       " 'heal': 310,\n",
       " 'flow': 311,\n",
       " 'learning': 312,\n",
       " 'seasons': 313,\n",
       " 'reflection': 314,\n",
       " 'creativity': 315,\n",
       " 'give': 316,\n",
       " 'bring': 317,\n",
       " 'lives': 318,\n",
       " 'solace': 319,\n",
       " 'voice': 320,\n",
       " 'precious': 321,\n",
       " 'adventures': 322,\n",
       " 'humility': 323,\n",
       " 'fuel': 324,\n",
       " 'become': 325,\n",
       " 'propels': 326,\n",
       " 'character': 327,\n",
       " 'greatness': 328,\n",
       " 'giving': 329,\n",
       " 'treasures': 330,\n",
       " 'good': 331,\n",
       " 'lights': 332,\n",
       " 'miracles': 333,\n",
       " 'make': 334,\n",
       " 'created': 335,\n",
       " 'celebration': 336,\n",
       " 'reveal': 337,\n",
       " 'tended': 338,\n",
       " 'cleanses': 339,\n",
       " 'resonates': 340,\n",
       " 'go': 341,\n",
       " 'understanding': 342,\n",
       " 'no': 343,\n",
       " 'wings': 344,\n",
       " 'waves': 345,\n",
       " 'wonder': 346,\n",
       " 'beneath': 347,\n",
       " 'goodness': 348,\n",
       " 'setup': 349,\n",
       " 'or': 350,\n",
       " 'person': 351,\n",
       " 'encouragement': 352,\n",
       " 'memories': 353,\n",
       " 'chapter': 354,\n",
       " 'encounter': 355,\n",
       " 'gem': 356,\n",
       " 'treasury': 357,\n",
       " 'harmony': 358,\n",
       " 'charm': 359,\n",
       " 'stands': 360,\n",
       " 'nation': 361,\n",
       " \"ubin's\": 362,\n",
       " 'vision': 363,\n",
       " 'heritage': 364,\n",
       " 'labrador': 365,\n",
       " 'vitality': 366,\n",
       " 'breathtaking': 367,\n",
       " 'lungs': 368,\n",
       " 'leaves': 369,\n",
       " 'arms': 370,\n",
       " 'welcome': 371,\n",
       " \"it's\": 372,\n",
       " 'stepping': 373,\n",
       " 'greatest': 374,\n",
       " 'one': 375,\n",
       " 'fears': 376,\n",
       " 'nourishes': 377,\n",
       " 'sets': 378,\n",
       " 'past': 379,\n",
       " 'louder': 380,\n",
       " 'own': 381,\n",
       " 'imagination': 382,\n",
       " 'enough': 383,\n",
       " 'keys': 384,\n",
       " 'warm': 385,\n",
       " 'wounds': 386,\n",
       " 'brilliance': 387,\n",
       " 'community': 388,\n",
       " 'grateful': 389,\n",
       " 'night': 390,\n",
       " 'mirror': 391,\n",
       " 'tomorrow': 392,\n",
       " 'wounded': 393,\n",
       " 'resides': 394,\n",
       " 'harmonious': 395,\n",
       " 'most': 396,\n",
       " 'illuminates': 397,\n",
       " 'those': 398,\n",
       " 'shields': 399,\n",
       " 'bitterness': 400,\n",
       " 'embracing': 401,\n",
       " 'haven': 402,\n",
       " 'like': 403,\n",
       " 'heals': 404,\n",
       " 'gifts': 405,\n",
       " 'fragrance': 406,\n",
       " 'chorus': 407,\n",
       " 'lighthouse': 408,\n",
       " 'follow': 409,\n",
       " 'soar': 410,\n",
       " 'break': 411,\n",
       " 'many': 412,\n",
       " 'extraordinary': 413,\n",
       " 'them': 414,\n",
       " 'passion': 415,\n",
       " 'share': 416,\n",
       " 'pursuit': 417,\n",
       " \"heart's\": 418,\n",
       " 'desires': 419,\n",
       " 'existence': 420,\n",
       " 'unique': 421,\n",
       " 'hues': 422,\n",
       " 'acts': 423,\n",
       " 'reminders': 424,\n",
       " 'gardens': 425,\n",
       " 'cherish': 426,\n",
       " 'full': 427,\n",
       " 'leaving': 428,\n",
       " 'write': 429,\n",
       " 'intention': 430,\n",
       " 'letting': 431,\n",
       " 'their': 432,\n",
       " 'sea': 433,\n",
       " 'ripple': 434,\n",
       " 'home': 435,\n",
       " 'fuels': 436,\n",
       " 'soothes': 437,\n",
       " 'wonders': 438,\n",
       " 'embodiment': 439,\n",
       " 'setback': 440,\n",
       " 'comeback': 441,\n",
       " 'bloom': 442,\n",
       " 'taken': 443,\n",
       " 'whether': 444,\n",
       " 'painful': 445,\n",
       " 'transitions': 446,\n",
       " 'directed': 447,\n",
       " 'indomitable': 448,\n",
       " 'wave': 449,\n",
       " 'witnessing': 450,\n",
       " 'cherished': 451,\n",
       " 'bay': 452,\n",
       " 'ambition': 453,\n",
       " 'modernity': 454,\n",
       " 'perfect': 455,\n",
       " \"chinatown's\": 456,\n",
       " 'pulse': 457,\n",
       " 'waiting': 458,\n",
       " \"gardens'\": 459,\n",
       " 'history': 460,\n",
       " 'sky': 461,\n",
       " 'quiet': 462,\n",
       " \"nation's\": 463,\n",
       " 'jurong': 464,\n",
       " 'wildlife': 465,\n",
       " 'sungei': 466,\n",
       " 'buloh': 467,\n",
       " 'wetland': 468,\n",
       " 'landscapes': 469,\n",
       " 'rugged': 470,\n",
       " 'witness': 471,\n",
       " 'face': 472,\n",
       " 'ancient': 473,\n",
       " 'survival': 474,\n",
       " 'species': 475,\n",
       " 'reefs': 476,\n",
       " 'cradle': 477,\n",
       " 'artistry': 478,\n",
       " 'rustle': 479,\n",
       " 'springs': 480,\n",
       " \"day's\": 481,\n",
       " 'fills': 482,\n",
       " 'air': 483,\n",
       " 'clarity': 484,\n",
       " 'fill': 485,\n",
       " 'chance': 486,\n",
       " 'stones': 487,\n",
       " 'at': 488,\n",
       " 'uniqueness': 489,\n",
       " 'only': 490,\n",
       " 'multiplies': 491,\n",
       " 'chains': 492,\n",
       " 'could': 493,\n",
       " 'small': 494,\n",
       " 'blueprints': 495,\n",
       " 'cornerstone': 496,\n",
       " 'gateway': 497,\n",
       " 'chest': 498,\n",
       " 'patience': 499,\n",
       " 'far': 500,\n",
       " 'birthplace': 501,\n",
       " 'thoughts': 502,\n",
       " 'nights': 503,\n",
       " 'mend': 504,\n",
       " 'everyday': 505,\n",
       " 'alchemy': 506,\n",
       " 'conductor': 507,\n",
       " 'cosmos': 508,\n",
       " 'kind': 509,\n",
       " 'remind': 510,\n",
       " 'emotions': 511,\n",
       " 'ordinary': 512,\n",
       " 'mind': 513,\n",
       " 'cycles': 514,\n",
       " 'weave': 515,\n",
       " 'cleanse': 516,\n",
       " 'fulfilled': 517,\n",
       " 'burdens': 518,\n",
       " 'healing': 519,\n",
       " 'bright': 520,\n",
       " 'acceptance': 521,\n",
       " 'powerful': 522,\n",
       " \"you'll\": 523,\n",
       " 'contagious': 524,\n",
       " 'set': 525,\n",
       " 'spreads': 526,\n",
       " 'wildfire': 527,\n",
       " 'sails': 528,\n",
       " 'fortress': 529,\n",
       " 'balm': 530,\n",
       " 'old': 531,\n",
       " 'sparks': 532,\n",
       " 'transcends': 533,\n",
       " 'all': 534,\n",
       " 'barriers': 535,\n",
       " 'drives': 536,\n",
       " 'ages': 537,\n",
       " 'powers': 538,\n",
       " 'conquer': 539,\n",
       " 'achieve': 540,\n",
       " 'difference': 541,\n",
       " 'adversity': 542,\n",
       " 'things': 543,\n",
       " 'agent': 544,\n",
       " 'anything': 545,\n",
       " 'melodies': 546,\n",
       " 'eyes': 547,\n",
       " 'deepest': 548,\n",
       " 'intentions': 549,\n",
       " 'away': 550,\n",
       " 'choices': 551,\n",
       " 'spread': 552,\n",
       " 'receive': 553,\n",
       " 'return': 554,\n",
       " 'makes': 555,\n",
       " 'grow': 556,\n",
       " 'plays': 557,\n",
       " 'watered': 558,\n",
       " 'renews': 559,\n",
       " 'sense': 560,\n",
       " 'forth': 561,\n",
       " 'differences': 562,\n",
       " 'resonate': 563,\n",
       " 'sowing': 564,\n",
       " 'lies': 565,\n",
       " 'freedom': 566,\n",
       " 'painting': 567,\n",
       " 'together': 568,\n",
       " 'truth': 569,\n",
       " 'rich': 570,\n",
       " 'waters': 571,\n",
       " 'transforms': 572,\n",
       " 'speak': 573,\n",
       " 'north': 574,\n",
       " 'which': 575,\n",
       " 'stand': 576,\n",
       " 'fire': 577,\n",
       " 'reflects': 578,\n",
       " 'infectious': 579,\n",
       " 'spreading': 580,\n",
       " 'sword': 581,\n",
       " 'cuts': 582,\n",
       " 'connections': 583,\n",
       " 'decision': 584,\n",
       " 'wellspring': 585,\n",
       " 'seas': 586,\n",
       " 'ripples': 587,\n",
       " 'preciousness': 588,\n",
       " 'uplift': 589,\n",
       " 'shapes': 590,\n",
       " 'between': 591,\n",
       " 'meet': 592,\n",
       " 'deserves': 593,\n",
       " 'becoming': 594,\n",
       " 'blossoming': 595,\n",
       " 'alter': 596,\n",
       " 'course': 597,\n",
       " 'reverberates': 598,\n",
       " 'stroke': 599,\n",
       " 'brush': 600,\n",
       " 'tale': 601,\n",
       " 'state': 602,\n",
       " 'contentment': 603,\n",
       " 'reside': 604,\n",
       " 'wide': 605,\n",
       " 'comes': 606,\n",
       " 'cultures': 607,\n",
       " 'bounds': 608,\n",
       " 'marina': 609,\n",
       " 'orchard': 610,\n",
       " \"sentosa's\": 611,\n",
       " 'corner': 612,\n",
       " 'merlion': 613,\n",
       " 'symbol': 614,\n",
       " 'timah': 615,\n",
       " 'reaches': 616,\n",
       " 'defines': 617,\n",
       " 'intertwine': 618,\n",
       " 'tells': 619,\n",
       " 'alive': 620,\n",
       " 'tales': 621,\n",
       " 'southern': 622,\n",
       " 'bird': 623,\n",
       " \"safari's\": 624,\n",
       " 'mysteries': 625,\n",
       " 'endeavor': 626,\n",
       " 'excellence': 627,\n",
       " 'changi': 628,\n",
       " 'vibrancy': 629,\n",
       " 'macritchie': 630,\n",
       " \"reservoir's\": 631,\n",
       " 'picture': 632,\n",
       " 'reaching': 633,\n",
       " 'diligence': 634,\n",
       " 'views': 635,\n",
       " 'embodies': 636,\n",
       " 'brand': 637,\n",
       " 'lake': 638,\n",
       " 'destinies': 639,\n",
       " 'chek': 640,\n",
       " \"jawa's\": 641,\n",
       " 'unwavering': 642,\n",
       " 'batok': 643,\n",
       " 'marine': 644,\n",
       " 'trove': 645,\n",
       " 'flourish': 646,\n",
       " 'stewardship': 647,\n",
       " 'forests': 648,\n",
       " 'breathe': 649,\n",
       " 'rivers': 650,\n",
       " 'skies': 651,\n",
       " 'cities': 652,\n",
       " 'footprint': 653,\n",
       " 'wetlands': 654,\n",
       " 'see': 655,\n",
       " 'drop': 656,\n",
       " \"earth's\": 657,\n",
       " 'savannas': 658,\n",
       " 'brushstrokes': 659,\n",
       " 'fiery': 660,\n",
       " 'pledge': 661,\n",
       " 'silent': 662,\n",
       " 'grass': 663,\n",
       " 'caves': 664,\n",
       " 'endurance': 665,\n",
       " 'renewal': 666,\n",
       " 'delicate': 667,\n",
       " 'ecosystems': 668,\n",
       " 'nurseries': 669,\n",
       " 'sand': 670,\n",
       " 'inhale': 671,\n",
       " 'veins': 672,\n",
       " 'land': 673,\n",
       " 'rising': 674,\n",
       " 'sun': 675,\n",
       " 'today': 676,\n",
       " 'presents': 677,\n",
       " 'reflect': 678,\n",
       " 'chambers': 679,\n",
       " 'soundtrack': 680,\n",
       " 'simple': 681,\n",
       " 'joys': 682,\n",
       " 'navigate': 683,\n",
       " 'illuminate': 684,\n",
       " 'fully': 685,\n",
       " 'reminding': 686,\n",
       " 'victories': 687,\n",
       " 'destination': 688,\n",
       " 'fingerprint': 689,\n",
       " 'exists': 690,\n",
       " 'silence': 691,\n",
       " 'speaks': 692,\n",
       " 'often': 693,\n",
       " 'significance': 694,\n",
       " 'constant': 695,\n",
       " 'chaos': 696,\n",
       " 'unknown': 697,\n",
       " 'midst': 698,\n",
       " 'peaceful': 699,\n",
       " 'elders': 700,\n",
       " 'allows': 701,\n",
       " 'sing': 702,\n",
       " 'turn': 703,\n",
       " 'off': 704,\n",
       " 'lands': 705,\n",
       " 'vulnerability': 706,\n",
       " 'ability': 707,\n",
       " 'itself': 708,\n",
       " 'without': 709,\n",
       " 'lullaby': 710,\n",
       " 'hug': 711,\n",
       " 'shadows': 712,\n",
       " 'expanding': 713,\n",
       " 'moon': 714,\n",
       " 'loving': 715,\n",
       " 'cannot': 716,\n",
       " 'affirmations': 717,\n",
       " 'solitude': 718,\n",
       " 'another': 719,\n",
       " 'raindrops': 720,\n",
       " 'nurture': 721,\n",
       " 'restore': 722,\n",
       " 'portrait': 723,\n",
       " 'radiates': 724,\n",
       " 'nurtures': 725,\n",
       " 'body': 726,\n",
       " 'deed': 727,\n",
       " 'restless': 728,\n",
       " 'balance': 729,\n",
       " 'gloomiest': 730,\n",
       " 'trust': 731,\n",
       " 'relationships': 732,\n",
       " 'minds': 733,\n",
       " 'attract': 734,\n",
       " 'great': 735,\n",
       " 'weight': 736,\n",
       " 'grudges': 737,\n",
       " 'keeps': 738,\n",
       " 'signature': 739,\n",
       " 'simplest': 740,\n",
       " 'anchor': 741,\n",
       " 'exploration': 742,\n",
       " 'smallest': 743,\n",
       " 'refuge': 744,\n",
       " 'weary': 745,\n",
       " 'bedrock': 746,\n",
       " 'generations': 747,\n",
       " 'unlocks': 748,\n",
       " 'mends': 749,\n",
       " 'unstoppable': 750,\n",
       " 'happen': 751,\n",
       " 'impossible': 752,\n",
       " 'think': 753,\n",
       " 'just': 754,\n",
       " 'as': 755,\n",
       " 'becomes': 756,\n",
       " 'conspire': 757,\n",
       " 'favor': 758,\n",
       " 'making': 759,\n",
       " 'footprints': 760,\n",
       " 'barrier': 761,\n",
       " 'forge': 762,\n",
       " 'limitless': 763,\n",
       " 'too': 764,\n",
       " 'unlock': 765,\n",
       " 'doors': 766,\n",
       " 'destined': 767,\n",
       " 'persevere': 768,\n",
       " 'darkness': 769,\n",
       " 'seek': 770,\n",
       " 'work': 771,\n",
       " 'art': 772,\n",
       " 'amazing': 773,\n",
       " 'limitation': 774,\n",
       " 'achieving': 775,\n",
       " 'architect': 776,\n",
       " 'brightest': 777,\n",
       " 'followed': 778,\n",
       " 'rainbows': 779,\n",
       " 'plant': 780,\n",
       " 'move': 781,\n",
       " 'points': 782,\n",
       " 'filled': 783,\n",
       " 'blend': 784,\n",
       " 'illuminated': 785,\n",
       " 'reflected': 786,\n",
       " 'multiply': 787,\n",
       " 'pieces': 788,\n",
       " 'unconditionally': 789,\n",
       " 'ink': 790,\n",
       " 'deeds': 791,\n",
       " 'calling': 792,\n",
       " 'disguise': 793,\n",
       " 'washes': 794,\n",
       " 'define': 795,\n",
       " 'forgive': 796,\n",
       " 'pen': 797,\n",
       " 'nudges': 798,\n",
       " 'triumphs': 799,\n",
       " 'choose': 800,\n",
       " 'found': 801,\n",
       " 'meaning': 802,\n",
       " 'ourselves': 803,\n",
       " 'guidance': 804,\n",
       " \"we've\": 805,\n",
       " 'traveled': 806,\n",
       " 'hopes': 807,\n",
       " 'offer': 808,\n",
       " 'pass': 809,\n",
       " 'uncertainty': 810,\n",
       " 'prelude': 811,\n",
       " 'savoring': 812,\n",
       " 'other': 813,\n",
       " 'side': 814,\n",
       " 'sweetest': 815,\n",
       " 'twist': 816,\n",
       " 'brightness': 817,\n",
       " 'notes': 818,\n",
       " 'tending': 819,\n",
       " 'steps': 820,\n",
       " 'limits': 821,\n",
       " 'lift': 822,\n",
       " 'weaves': 823,\n",
       " 'cultivating': 824,\n",
       " 'renew': 825,\n",
       " 'fruits': 826,\n",
       " 'pillars': 827,\n",
       " 'unveil': 828,\n",
       " 'weaving': 829,\n",
       " 'heights': 830,\n",
       " 'halls': 831,\n",
       " 'so': 832,\n",
       " 'brightly': 833,\n",
       " 'anthem': 834,\n",
       " 'against': 835,\n",
       " 'rock': 836,\n",
       " 'wildest': 837,\n",
       " 'binds': 838,\n",
       " 'wherever': 839,\n",
       " 'uplifts': 840,\n",
       " 'fear': 841,\n",
       " 'action': 842,\n",
       " 'stormy': 843,\n",
       " 'fosters': 844,\n",
       " 'achievements': 845,\n",
       " 'currency': 846,\n",
       " 'interactions': 847,\n",
       " 'unites': 848,\n",
       " 'principles': 849,\n",
       " 'engine': 850,\n",
       " 'contagion': 851,\n",
       " 'doubt': 852,\n",
       " 'salve': 853,\n",
       " 'turbulent': 854,\n",
       " 'protects': 855,\n",
       " 'values': 856,\n",
       " 'victory': 857,\n",
       " 'shared': 858,\n",
       " 'beam': 859,\n",
       " 'frees': 860,\n",
       " 'spoken': 861,\n",
       " 'counts': 862,\n",
       " 'thread': 863,\n",
       " 'human': 864,\n",
       " 'bad': 865,\n",
       " 'lesson': 866,\n",
       " 'embraced': 867,\n",
       " 'contented': 868,\n",
       " 'shown': 869,\n",
       " 'forged': 870,\n",
       " 'miracle': 871,\n",
       " 'fate': 872,\n",
       " 'mold': 873,\n",
       " 'glorious': 874,\n",
       " 'narrative': 875,\n",
       " 'farewells': 876,\n",
       " 'enlightenment': 877,\n",
       " 'out': 878,\n",
       " 'influence': 879,\n",
       " 'triumphant': 880,\n",
       " 'determined': 881,\n",
       " 'joyous': 882,\n",
       " 'worth': 883,\n",
       " 'telling': 884,\n",
       " 'ebb': 885,\n",
       " 'goodbyes': 886,\n",
       " 'transcending': 887,\n",
       " 'fostering': 888,\n",
       " 'serene': 889,\n",
       " 'blooming': 890,\n",
       " 'significant': 891,\n",
       " 'part': 892,\n",
       " 'turning': 893,\n",
       " 'point': 894,\n",
       " 'defy': 895,\n",
       " 'logic': 896,\n",
       " 'evolving': 897,\n",
       " 'wiser': 898,\n",
       " 'resilient': 899,\n",
       " 'version': 900,\n",
       " 'release': 901,\n",
       " 'resentment': 902,\n",
       " 'providing': 903,\n",
       " 'assurance': 904,\n",
       " 'not': 905,\n",
       " 'alone': 906,\n",
       " 'realization': 907,\n",
       " 'fulfillment': 908,\n",
       " 'richness': 909,\n",
       " 'meaningful': 910,\n",
       " 'connect': 911,\n",
       " 'affection': 912,\n",
       " 'deliberate': 913,\n",
       " 'shaping': 914,\n",
       " 'living': 915,\n",
       " 'proof': 916,\n",
       " 'conquering': 917,\n",
       " 'converge': 918,\n",
       " 'road': 919,\n",
       " 'roars': 920,\n",
       " 'embraces': 921,\n",
       " 'traditions': 922,\n",
       " 'reach': 923,\n",
       " 'beaches': 924,\n",
       " 'alleys': 925,\n",
       " 'southeast': 926,\n",
       " 'asia': 927,\n",
       " 'beats': 928,\n",
       " 'strong': 929,\n",
       " 'raffles': 930,\n",
       " 'place': 931,\n",
       " 'clarke': 932,\n",
       " 'quay': 933,\n",
       " 'told': 934,\n",
       " 'hawker': 935,\n",
       " 'centers': 936,\n",
       " 'fine': 937,\n",
       " 'dining': 938,\n",
       " 'flavors': 939,\n",
       " 'delectable': 940,\n",
       " 'ideas': 941,\n",
       " 'little': 942,\n",
       " \"india's\": 943,\n",
       " 'botanic': 944,\n",
       " 'glistens': 945,\n",
       " 'universal': 946,\n",
       " 'studios': 947,\n",
       " 'attractions': 948,\n",
       " 'class': 949,\n",
       " 'futures': 950,\n",
       " 'built': 951,\n",
       " 'park': 952,\n",
       " 'reserve': 953,\n",
       " 'heavens': 954,\n",
       " 'island': 955,\n",
       " \"road's\": 956,\n",
       " 'shopping': 957,\n",
       " 'shores': 958,\n",
       " 'trails': 959,\n",
       " 'leap': 960,\n",
       " 'sands': 961,\n",
       " 'flyer': 962,\n",
       " \"city's\": 963,\n",
       " 'landmarks': 964,\n",
       " 'lanterns': 965,\n",
       " 'kampong': 966,\n",
       " \"glam's\": 967,\n",
       " \"esplanade's\": 968,\n",
       " 'performances': 969,\n",
       " 'haw': 970,\n",
       " 'par': 971,\n",
       " \"villa's\": 972,\n",
       " 'culture': 973,\n",
       " 'thriving': 974,\n",
       " \"tekong's\": 975,\n",
       " \"islands'\": 976,\n",
       " 'testifies': 977,\n",
       " 'fort': 978,\n",
       " \"canning's\": 979,\n",
       " 'ruggedness': 980,\n",
       " 'dedication': 981,\n",
       " 'stone': 982,\n",
       " 'stretches': 983,\n",
       " 'horizon': 984,\n",
       " 'reign': 985,\n",
       " 'east': 986,\n",
       " 'coast': 987,\n",
       " 'breezes': 988,\n",
       " \"semakau's\": 989,\n",
       " 'talents': 990,\n",
       " 'honed': 991,\n",
       " 'peranakan': 992,\n",
       " 'houses': 993,\n",
       " 'colonial': 994,\n",
       " 'architecture': 995,\n",
       " 'preserved': 996,\n",
       " 'pride': 997,\n",
       " \"airport's\": 998,\n",
       " 'efficiency': 999,\n",
       " \"river's\": 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 index is saved for paddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connector and filler words are most common<br>\n",
    "Most common nouns/verbs/adjectives include dance, life's/life is, embrace<br>\n",
    "Model is most likely to predict words related to embracing life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n"
     ]
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index) + 1 # Because index 0 is reserved for padding.\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embrace the beauty of every sunrise; it's a fresh chance to paint your world with joy.\n",
      "[17, 1, 49, 2, 13, 77, 372, 5, 163, 486, 10, 101, 3, 46, 22, 52]\n",
      "\n",
      "Embrace challenges; they are the stepping stones to your greatest victories.\n",
      "[17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687]\n",
      "\n",
      "Embrace the rhythm of life and let it dance through your soul.\n",
      "[17, 1, 119, 2, 31, 4, 9, 11, 24, 28, 3, 40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(tokenizer.texts_to_sequences([data[0]])[0])\n",
    "print()\n",
    "\n",
    "print(data[1])\n",
    "print(tokenizer.texts_to_sequences([data[1]])[0])\n",
    "print()\n",
    "\n",
    "print(data[2])\n",
    "print(tokenizer.texts_to_sequences([data[2]])[0]) # Extra [0] at the end, because the output is a list of list.\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When prompted with embrace, the model may predict 1, or 'the', a lot of the time, looking at the sample tokenised texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of quotes: 890\n"
     ]
    }
   ],
   "source": [
    "# converts each of the 1000 quotes from texts to integers.\n",
    "\n",
    "input_sequence = []\n",
    "\n",
    "for datapoint in data:\n",
    "    token_list = tokenizer.texts_to_sequences([datapoint])[0] \n",
    "    input_sequence.append(token_list)\n",
    "\n",
    "print(f'Total number of quotes: {len(input_sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 9729\n"
     ]
    }
   ],
   "source": [
    "# Build input-output pairs where the input has 2 words.\n",
    "# The input is 1 word and the corresponding output is the next word.\n",
    "# Create a list of 2 words\n",
    "\n",
    "input_output_pair1 = []\n",
    "for datapoint in input_sequence:\n",
    "    for i in range(len(datapoint) - 1): \n",
    "        input_output_pair1.append(datapoint[i:i + 2])\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pair1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 8839\n"
     ]
    }
   ],
   "source": [
    "# Build input-output pairs where the input has 3 words.\n",
    "# The input is 2 words and the corresponding output is the next word.\n",
    "# Create a list of 3 words\n",
    "\n",
    "input_output_pair2 = []\n",
    "for datapoint in input_sequence:\n",
    "    for i in range(len(datapoint) - 2): # Why -2?\n",
    "        input_output_pair2.append(datapoint[i:i + 3])\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pair2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 890\n"
     ]
    }
   ],
   "source": [
    "# Build input-output pairs where the input has N-1 words.\n",
    "# The input is every N-1 words and the corresponding output is the last word.\n",
    "# Create a list of \"N words\", i.e. the entire quote.\n",
    "\n",
    "input_output_pairN = []\n",
    "for datapoint in input_sequence:\n",
    "    input_output_pairN.append(datapoint) # Why no nested for-loop?\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pairN)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original quotes:\n",
      "[[17, 1, 49, 2, 13, 77, 372, 5, 163, 486, 10, 101, 3, 46, 22, 52], [17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687]]\n",
      "\n",
      "Each quote chopped into phrases of two words:\n",
      "[[17, 1], [1, 49], [49, 2], [2, 13], [13, 77], [77, 372], [372, 5], [5, 163], [163, 486], [486, 10], [10, 101], [101, 3], [3, 46], [46, 22], [22, 52], [17, 148], [148, 33], [33, 20], [20, 1], [1, 373]]\n",
      "\n",
      "Each quote chopped into phrases of three words:\n",
      "[[17, 1, 49], [1, 49, 2], [49, 2, 13], [2, 13, 77], [13, 77, 372], [77, 372, 5], [372, 5, 163], [5, 163, 486], [163, 486, 10], [486, 10, 101], [10, 101, 3], [101, 3, 46], [3, 46, 22], [46, 22, 52], [17, 148, 33], [148, 33, 20], [33, 20, 1], [20, 1, 373], [1, 373, 487], [373, 487, 10]]\n",
      "\n",
      "Each quote itself:\n",
      "[[17, 1, 49, 2, 13, 77, 372, 5, 163, 486, 10, 101, 3, 46, 22, 52], [17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687], [17, 1, 119, 2, 31, 4, 9, 11, 24, 28, 3, 40], [17, 35, 8, 11, 149, 1, 36, 10, 72, 1, 46, 375, 25, 488, 5, 299], [17, 1, 50, 8, 11, 247, 10, 1, 688, 2, 3, 38], [17, 3, 489, 8, 11, 6, 1, 689, 2, 3, 40, 91, 1, 120], [17, 1, 211, 106, 8, 11, 6, 1, 490, 375, 16, 300, 690], [17, 3, 376, 8, 33, 82, 1, 164, 10, 301, 3, 64, 59], [17, 44, 4, 130, 212, 11, 491, 1, 131, 7, 3, 31], [17, 1, 132, 8, 11, 377, 1, 111, 2, 3, 69, 165], [17, 1, 57, 2, 3, 25, 33, 166, 1, 73, 2, 1, 120], [17, 78, 8, 11, 6, 1, 133, 2, 5, 150, 25], [17, 1, 36, 2, 107, 8, 11, 378, 15, 302, 37, 1, 492, 2, 1, 379], [17, 1, 691, 11, 692, 380, 186, 248, 303, 493], [17, 1, 494, 74, 8, 33, 693, 82, 1, 374, 694], [17, 32, 8, 11, 6, 1, 213, 2, 1, 40], [17, 72, 8, 11, 6, 1, 490, 695, 7, 18, 304, 696], [17, 1, 697, 8, 11, 92, 1, 75, 2, 65, 134], [17, 3, 38, 8, 33, 20, 1, 495, 2, 3, 93], [17, 1, 55, 2, 31, 214, 7, 1, 698, 2, 1, 215, 216]]\n"
     ]
    }
   ],
   "source": [
    "print('Original quotes:')\n",
    "print(input_sequence[0:2])\n",
    "print()\n",
    "print('Each quote chopped into phrases of two words:')\n",
    "print(input_output_pair1[:20])\n",
    "print()\n",
    "print('Each quote chopped into phrases of three words:')\n",
    "print(input_output_pair2[:20])\n",
    "print()\n",
    "print('Each quote itself:')\n",
    "print(input_output_pairN[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0  17   1]\n",
      " [  0   0   0 ...   0   1  49]\n",
      " [  0   0   0 ...   0  49   2]\n",
      " ...\n",
      " [  0   0   0 ...   7   1 106]\n",
      " [  0   0   0 ... 604 112  15]\n",
      " [  0   0   0 ...   2  13  81]]\n"
     ]
    }
   ],
   "source": [
    "# Generate more data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "input_output_pair = input_output_pair1 + input_output_pair2 + input_output_pairN\n",
    "max_sequence_len = max([len(x) for x in input_output_pair]) # This is to get the longest sequence of texts.\n",
    "                                                            # And pad all datapoints to be of this length.\n",
    "input_output_pair = np.array(pad_sequences(input_output_pair, maxlen=max_sequence_len, padding='pre'))\n",
    "print(input_output_pair) # All datapoints now have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0  17]\n",
      " [  0   0   0 ...   0   0   1]\n",
      " [  0   0   0 ...   0   0  49]\n",
      " ...\n",
      " [  0   0   0 ... 211   7   1]\n",
      " [  0   0   0 ...  16 604 112]\n",
      " [  0   0   0 ... 588   2  13]]\n",
      "[  1  49   2 ... 106  15  81]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have an array, slice the last word as the output.\n",
    "# The preceding words are the input.\n",
    "\n",
    "X = input_output_pair[:,:-1]\n",
    "y = input_output_pair[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to be a one-hot array.\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "y = utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n",
      "(19458, 34)\n",
      "(19458, 1199)\n"
     ]
    }
   ],
   "source": [
    "print(total_words)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# # Input is max sequence length - 1, as we've removed the last word for the label.\n",
    "# input_len = max_sequence_len - 1 \n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add input embedding layer.\n",
    "# model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# # Add LSTM layer with 256 units.\n",
    "# model.add(LSTM(256))\n",
    "\n",
    "# # Control overfitting.\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# # Add output layer\n",
    "# model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y, epochs=30, verbose=1)\n",
    "# model.save_weights('rnn_weights1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('rnn_weights1.h5')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_texts = ['embrace each day',               \n",
    "              'radiate some',               \n",
    "              'believe that',               \n",
    "              \"life's actual purpose is\",               \n",
    "              'dance through each and every',               \n",
    "              'let your time and energy',               \n",
    "              'every person is',               \n",
    "              'our country Singapore is',               \n",
    "              'planet earth is',               \n",
    "              'morning and evening would make it'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parametric experiment to tune number of LSTM layers\n",
    "def createAndTrain_model_lstmLayers(num_layers):\n",
    "    input_len = max_sequence_len - 1 \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add input embedding layer.\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "    # Add N LSTM layers with 256 units.\n",
    "    for i in range(num_layers):\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "        \n",
    "    # Add output layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, verbose=1, batch_size=32, shuffle=True)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1199) and (None, 34, 1199) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hazem\\Desktop\\DELE\\RNN-YF\\rnn.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# loop over layers\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m bi \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(layers)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     history \u001b[39m=\u001b[39m createAndTrain_model_lstmLayers(layers[bi])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# store results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     lossResultsLayers[:,bi] \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\hazem\\Desktop\\DELE\\RNN-YF\\rnn.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(total_words, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49mepochs, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/DELE/RNN-YF/rnn.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileowww4435.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\hazem\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1199) and (None, 34, 1199) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# range of layers\n",
    "layers = np.arange(2, 11)\n",
    "\n",
    "# initialize output results matrices\n",
    "lossResultsLayers = np.zeros((epochs, len(layers)))\n",
    "\n",
    "# loop over layers\n",
    "for bi in range(len(layers)):\n",
    "    \n",
    "    history = createAndTrain_model_lstmLayers(layers[bi])\n",
    "\n",
    "    # store results\n",
    "    lossResultsLayers[:,bi] = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossResultsLayers)\n",
    "plt.title('Model Loss')\n",
    "plt.legend(lossResultsLayers)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parametric experiment to tune batchsize\n",
    "def createAndTrain_model_batchsize(num_layers, batchsize):\n",
    "    input_len = max_sequence_len - 1 \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add input embedding layer.\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "    # Add N LSTM layers with 256 units.\n",
    "    for i in range(num_layers):\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "        \n",
    "    # Add output layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, verbose=1, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of batch sizes\n",
    "batchsizes = 2**np.arange(4, 9)\n",
    "numLayers =\n",
    "\n",
    "# initialize output results matrices\n",
    "lossResultsBatchsizes = np.zeros((epochs, len(batchsizes)))\n",
    "\n",
    "# loop over layers\n",
    "for bi in range(len(batchsizes)):\n",
    "    history = createAndTrain_model_batchsize(numLayers, batchsizes[bi])\n",
    "\n",
    "    # store results\n",
    "    lossResultsBatchsizes[:,bi] = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossResultsBatchsizes)\n",
    "plt.title('Model Loss')\n",
    "plt.legend(lossResultsBatchsizes)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "monitor_val_acc = EarlyStopping(monitor='loss', patience=15)\n",
    "model_checkpoint = ModelCheckpoint('best_RNN_version1.h5', save_best_only = True)\n",
    "\n",
    "def create_bestModel(numLayers, batchsize):\n",
    "    input_len = max_sequence_len - 1 \n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add input embedding layer.\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "    # Add N LSTM layers with 256 units.\n",
    "    for i in range(numLayers):\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "        \n",
    "    # Add output layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, verbose=1, batch_size=batchsize, shuffle=True, callbacks=[monitor_val_acc, model_checkpoint])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = \n",
    "\n",
    "final_model = create_bestModel(numLayers, batchsize)\n",
    "final_model.load_weights('best_RNN_version1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_next_N_words(input_texts, N_words=10):\n",
    "#     predictions_list = []\n",
    "#     for input_text in input_texts:\n",
    "#         predicted_words = \"\"\n",
    "#         tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
    "#         for _ in range(N_words):\n",
    "#             tokens = pad_sequences([tokens], maxlen=max_sequence_len-1, padding='pre')\n",
    "#             prediction = np.argmax(final_model.predict(tokens), axis=1)\n",
    "#             predicted_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "#             predicted_words += predicted_word + \" \"\n",
    "#             tokens = np.append(tokens, prediction)\n",
    "#         predicted_words = input_text + \" \" + predicted_words\n",
    "#         predictions_list.append(predicted_words)\n",
    "            \n",
    "#     return predictions_list\n",
    "\n",
    "def predict_next_N_words_unique(seed_texts, top_p=1, N_words=10):\n",
    "    generated_texts = []\n",
    "\n",
    "    for seed_text in seed_texts:\n",
    "        current_generated_text = seed_text\n",
    "        for _ in range(N_words):\n",
    "            # Tokenize the input sequence\n",
    "            seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "            # Pad the sequence if needed\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([seed_sequence], maxlen=max_sequence_len-1)\n",
    "\n",
    "            # Get the model's prediction for the next word\n",
    "            predictions = final_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "            # Apply top-p sampling\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            cumulative_probs = np.cumsum(predictions[sorted_indices])\n",
    "            selected_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            # Normalize probabilities\n",
    "            selected_probs = predictions[selected_indices] / np.sum(predictions[selected_indices])\n",
    "\n",
    "            # Sample from the selected indices based on the normalized probabilities\n",
    "            next_index = np.random.choice(selected_indices, p=selected_probs)\n",
    "\n",
    "            # Convert the index back to a word\n",
    "            next_word = tokenizer.index_word[next_index]\n",
    "            # print(next_word)\n",
    "            # Break if the generated text is too long or if an end token is predicted\n",
    "            if next_word is None or next_word == 'end_token' or len(current_generated_text.split()) >= N_words + len(seed_text):\n",
    "                break\n",
    "\n",
    "            # Update the generated text and seed_text for the next iteration\n",
    "            current_generated_text += \" \" + next_word\n",
    "            seed_text += \" \" + next_word\n",
    "\n",
    "        generated_texts.append(current_generated_text)\n",
    "\n",
    "    return generated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embrace each day and carry that you nature call defy wisdom beneath than',\n",
       " 'radiate some is believe and our source its springs bay echoes are',\n",
       " 'believe that our reside discovery a destinies of believe and carry touch',\n",
       " \"life's actual purpose is soothes conservation in a they blooms reveal breath truly of\",\n",
       " 'dance through each and every light determination beginnings our authenticity its your inner source be',\n",
       " 'let your time and energy to our truly its in a speak secrets affection and',\n",
       " 'every person is the garden of carry we stands desires and journey our',\n",
       " 'our country Singapore is you reminds brush and wisdom efficiency simplicity of wisdom wisdom',\n",
       " \"planet earth is life's progress success touch be a guiding of joy in\",\n",
       " 'morning and evening would make it in the testament be the blessings soul intention and success']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_next_N_words_unique(seed_texts)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embrace each day of your they and beautiful and fuels efficiency grudges dawn',\n",
       " 'radiate some is believe and our truly and beauty efficiency on should',\n",
       " 'believe that our reside discovery a true be energy truly of morning',\n",
       " \"life's actual purpose is the blessings of intention and when to the find that\",\n",
       " 'dance through each and every stories guides on be our truly determination fresh fire pursuit',\n",
       " 'let your time and energy wisdom tending wisdom halls should is grass serenity chapter bay',\n",
       " 'every person is your kindness source be wisdom blossoming effort our grateful hold',\n",
       " 'our country Singapore is the emotions of city and deserves beautiful desires that effort',\n",
       " 'planet earth is the blessings and one of create plateaus on serenity chapters',\n",
       " 'morning and evening would make it sand singapore be a preciousness fresh us face a night']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2 = predict_next_N_words_unique(seed_texts)\n",
    "prediction2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
