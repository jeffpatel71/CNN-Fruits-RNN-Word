{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, GRU, SimpleRNN\n",
    "from keras.layers import Dropout, LayerNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Quotes\n",
      "0  Embrace the beauty of every sunrise; it's a fr...\n",
      "1  Embrace challenges; they are the stepping ston...\n",
      "2  Embrace the rhythm of life and let it dance th...\n",
      "3  Embrace kindness, for it has the power to chan...\n",
      "4  Embrace the journey, for it leads to the desti...\n",
      "Data shape: (1000, 1)\n",
      "Missing values: Quotes    0\n",
      "dtype: int64\n",
      "                                                   Quotes\n",
      "count                                                1000\n",
      "unique                                                890\n",
      "top     Radiate acceptance, and find peace in embracin...\n",
      "freq                                                    5\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "print(data.head())\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = list(data['Quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'your': 3,\n",
       " 'and': 4,\n",
       " 'a': 5,\n",
       " 'is': 6,\n",
       " 'in': 7,\n",
       " 'for': 8,\n",
       " 'let': 9,\n",
       " 'to': 10,\n",
       " 'it': 11,\n",
       " 'be': 12,\n",
       " 'every': 13,\n",
       " 'our': 14,\n",
       " 'you': 15,\n",
       " 'that': 16,\n",
       " 'embrace': 17,\n",
       " \"life's\": 18,\n",
       " 'this': 19,\n",
       " 'are': 20,\n",
       " 'morning': 21,\n",
       " 'with': 22,\n",
       " 'radiate': 23,\n",
       " 'dance': 24,\n",
       " 'heart': 25,\n",
       " 'believe': 26,\n",
       " 'yourself': 27,\n",
       " 'through': 28,\n",
       " \"planet's\": 29,\n",
       " 'will': 30,\n",
       " 'life': 31,\n",
       " 'love': 32,\n",
       " 'they': 33,\n",
       " \"singapore's\": 34,\n",
       " 'kindness': 35,\n",
       " 'power': 36,\n",
       " 'from': 37,\n",
       " 'dreams': 38,\n",
       " 'we': 39,\n",
       " 'soul': 40,\n",
       " 'symphony': 41,\n",
       " 'act': 42,\n",
       " 'find': 43,\n",
       " 'gratitude': 44,\n",
       " 'singapore': 45,\n",
       " 'world': 46,\n",
       " 'strength': 47,\n",
       " 'light': 48,\n",
       " 'beauty': 49,\n",
       " 'journey': 50,\n",
       " 'nature': 51,\n",
       " 'joy': 52,\n",
       " 'planet': 53,\n",
       " 'canvas': 54,\n",
       " 'colors': 55,\n",
       " 'way': 56,\n",
       " 'whispers': 57,\n",
       " 'where': 58,\n",
       " 'potential': 59,\n",
       " 'hope': 60,\n",
       " 'testament': 61,\n",
       " 'resilience': 62,\n",
       " 'towards': 63,\n",
       " 'true': 64,\n",
       " 'new': 65,\n",
       " 'compassion': 66,\n",
       " 'beacon': 67,\n",
       " 'actions': 68,\n",
       " 'future': 69,\n",
       " 'spirit': 70,\n",
       " 'step': 71,\n",
       " 'change': 72,\n",
       " 'wisdom': 73,\n",
       " 'moments': 74,\n",
       " 'promise': 75,\n",
       " 'garden': 76,\n",
       " 'sunrise': 77,\n",
       " 'laughter': 78,\n",
       " 'story': 79,\n",
       " 'hearts': 80,\n",
       " 'day': 81,\n",
       " 'hold': 82,\n",
       " 'an': 83,\n",
       " 'tapestry': 84,\n",
       " 'heartbeat': 85,\n",
       " 'compass': 86,\n",
       " 'gift': 87,\n",
       " 'force': 88,\n",
       " 'inner': 89,\n",
       " 'each': 90,\n",
       " 'on': 91,\n",
       " 'holds': 92,\n",
       " 'destiny': 93,\n",
       " 'into': 94,\n",
       " 'self': 95,\n",
       " 'legacy': 96,\n",
       " 'when': 97,\n",
       " 'lion': 98,\n",
       " 'city': 99,\n",
       " 'skyline': 100,\n",
       " 'paint': 101,\n",
       " 'challenge': 102,\n",
       " 'determination': 103,\n",
       " 'tranquility': 104,\n",
       " 'others': 105,\n",
       " 'moment': 106,\n",
       " 'forgiveness': 107,\n",
       " 'melody': 108,\n",
       " 'any': 109,\n",
       " 'opportunities': 110,\n",
       " 'seeds': 111,\n",
       " 'within': 112,\n",
       " 'chapters': 113,\n",
       " 'growth': 114,\n",
       " 'purpose': 115,\n",
       " 'authenticity': 116,\n",
       " 'peace': 117,\n",
       " 'brings': 118,\n",
       " 'rhythm': 119,\n",
       " 'universe': 120,\n",
       " 'can': 121,\n",
       " 'reality': 122,\n",
       " 'overcome': 123,\n",
       " 'empathy': 124,\n",
       " 'serenity': 125,\n",
       " 'leave': 126,\n",
       " 'vibrant': 127,\n",
       " 'aspirations': 128,\n",
       " 'breath': 129,\n",
       " 'watch': 130,\n",
       " 'blessings': 131,\n",
       " 'rain': 132,\n",
       " 'music': 133,\n",
       " 'beginnings': 134,\n",
       " 'treasure': 135,\n",
       " 'have': 136,\n",
       " 'shape': 137,\n",
       " 'possibility': 138,\n",
       " 'positivity': 139,\n",
       " 'create': 140,\n",
       " 'carries': 141,\n",
       " 'bridge': 142,\n",
       " 'path': 143,\n",
       " 'nurtured': 144,\n",
       " 'us': 145,\n",
       " 'open': 146,\n",
       " 'reminder': 147,\n",
       " 'challenges': 148,\n",
       " 'has': 149,\n",
       " 'joyful': 150,\n",
       " 'more': 151,\n",
       " 'transform': 152,\n",
       " 'brighter': 153,\n",
       " 'foundation': 154,\n",
       " 'confidence': 155,\n",
       " 'guides': 156,\n",
       " 'guiding': 157,\n",
       " 'progress': 158,\n",
       " 'painted': 159,\n",
       " 'threads': 160,\n",
       " 'opportunity': 161,\n",
       " 'conservation': 162,\n",
       " 'fresh': 163,\n",
       " 'key': 164,\n",
       " 'success': 165,\n",
       " 'carry': 166,\n",
       " 'smile': 167,\n",
       " 'up': 168,\n",
       " 'its': 169,\n",
       " 'connection': 170,\n",
       " 'diversity': 171,\n",
       " 'touch': 172,\n",
       " 'masterpiece': 173,\n",
       " 'possibilities': 174,\n",
       " 'around': 175,\n",
       " 'sanctuary': 176,\n",
       " 'boundless': 177,\n",
       " 'star': 178,\n",
       " 'take': 179,\n",
       " 'shine': 180,\n",
       " 'by': 181,\n",
       " 'flourishes': 182,\n",
       " \"park's\": 183,\n",
       " 'hear': 184,\n",
       " 'stillness': 185,\n",
       " 'than': 186,\n",
       " 'experience': 187,\n",
       " 'wind': 188,\n",
       " 'lead': 189,\n",
       " 'magic': 190,\n",
       " 'energy': 191,\n",
       " 'being': 192,\n",
       " 'passions': 193,\n",
       " 'truest': 194,\n",
       " 'presence': 195,\n",
       " 'behind': 196,\n",
       " 'courage': 197,\n",
       " 'essence': 198,\n",
       " 'ignites': 199,\n",
       " 'grace': 200,\n",
       " 'echoes': 201,\n",
       " 'forward': 202,\n",
       " 'woven': 203,\n",
       " \"someone's\": 204,\n",
       " 'generosity': 205,\n",
       " 'innovation': 206,\n",
       " 'sunset': 207,\n",
       " 'pulau': 208,\n",
       " 'paints': 209,\n",
       " \"reserve's\": 210,\n",
       " 'present': 211,\n",
       " 'how': 212,\n",
       " 'language': 213,\n",
       " 'even': 214,\n",
       " 'darkest': 215,\n",
       " 'days': 216,\n",
       " 'unfold': 217,\n",
       " 'song': 218,\n",
       " 'turns': 219,\n",
       " 'adventure': 220,\n",
       " 'stories': 221,\n",
       " 'word': 222,\n",
       " 'transformation': 223,\n",
       " 'souls': 224,\n",
       " 'faith': 225,\n",
       " 'inspire': 226,\n",
       " 'abundance': 227,\n",
       " 'mark': 228,\n",
       " 'spark': 229,\n",
       " 'endless': 230,\n",
       " 'storms': 231,\n",
       " 'flight': 232,\n",
       " 'stronger': 233,\n",
       " 'dream': 234,\n",
       " 'source': 235,\n",
       " 'experiences': 236,\n",
       " 'made': 237,\n",
       " 'brightens': 238,\n",
       " 'connects': 239,\n",
       " 'tall': 240,\n",
       " 'reminds': 241,\n",
       " 'warmth': 242,\n",
       " 'dawn': 243,\n",
       " 'biodiversity': 244,\n",
       " 'protect': 245,\n",
       " 'offers': 246,\n",
       " 'leads': 247,\n",
       " 'words': 248,\n",
       " 'simplicity': 249,\n",
       " 'secrets': 250,\n",
       " 'positive': 251,\n",
       " 'well': 252,\n",
       " 'stars': 253,\n",
       " 'intuition': 254,\n",
       " 'humanity': 255,\n",
       " 'driving': 256,\n",
       " 'inspiration': 257,\n",
       " 'discovery': 258,\n",
       " 'care': 259,\n",
       " 'expression': 260,\n",
       " 'armor': 261,\n",
       " 'curiosity': 262,\n",
       " 'enthusiasm': 263,\n",
       " 'echo': 264,\n",
       " 'capable': 265,\n",
       " 'obstacle': 266,\n",
       " 'worthy': 267,\n",
       " 'discover': 268,\n",
       " 'composed': 269,\n",
       " 'blooms': 270,\n",
       " 'strokes': 271,\n",
       " 'mosaic': 272,\n",
       " 'classroom': 273,\n",
       " 'learn': 274,\n",
       " 'book': 275,\n",
       " 'written': 276,\n",
       " 'knows': 277,\n",
       " 'happiness': 278,\n",
       " 'creates': 279,\n",
       " 'faced': 280,\n",
       " 'creating': 281,\n",
       " 'start': 282,\n",
       " 'effort': 283,\n",
       " 'friendship': 284,\n",
       " 'formed': 285,\n",
       " 'choice': 286,\n",
       " 'brushstroke': 287,\n",
       " 'thought': 288,\n",
       " 'liberation': 289,\n",
       " 'investment': 290,\n",
       " 'tribute': 291,\n",
       " 'marvel': 292,\n",
       " 'gentle': 293,\n",
       " 'diverse': 294,\n",
       " 'bukit': 295,\n",
       " 'unity': 296,\n",
       " 'coastal': 297,\n",
       " \"nature's\": 298,\n",
       " 'time': 299,\n",
       " 'truly': 300,\n",
       " 'unlocking': 301,\n",
       " 'free': 302,\n",
       " 'ever': 303,\n",
       " 'beautiful': 304,\n",
       " 'genuine': 305,\n",
       " 'guide': 306,\n",
       " 'what': 307,\n",
       " 'lived': 308,\n",
       " 'pages': 309,\n",
       " 'heal': 310,\n",
       " 'flow': 311,\n",
       " 'learning': 312,\n",
       " 'seasons': 313,\n",
       " 'reflection': 314,\n",
       " 'creativity': 315,\n",
       " 'give': 316,\n",
       " 'bring': 317,\n",
       " 'lives': 318,\n",
       " 'solace': 319,\n",
       " 'voice': 320,\n",
       " 'precious': 321,\n",
       " 'adventures': 322,\n",
       " 'humility': 323,\n",
       " 'fuel': 324,\n",
       " 'become': 325,\n",
       " 'propels': 326,\n",
       " 'character': 327,\n",
       " 'greatness': 328,\n",
       " 'giving': 329,\n",
       " 'treasures': 330,\n",
       " 'good': 331,\n",
       " 'lights': 332,\n",
       " 'miracles': 333,\n",
       " 'make': 334,\n",
       " 'created': 335,\n",
       " 'celebration': 336,\n",
       " 'reveal': 337,\n",
       " 'tended': 338,\n",
       " 'cleanses': 339,\n",
       " 'resonates': 340,\n",
       " 'go': 341,\n",
       " 'understanding': 342,\n",
       " 'no': 343,\n",
       " 'wings': 344,\n",
       " 'waves': 345,\n",
       " 'wonder': 346,\n",
       " 'beneath': 347,\n",
       " 'goodness': 348,\n",
       " 'setup': 349,\n",
       " 'or': 350,\n",
       " 'person': 351,\n",
       " 'encouragement': 352,\n",
       " 'memories': 353,\n",
       " 'chapter': 354,\n",
       " 'encounter': 355,\n",
       " 'gem': 356,\n",
       " 'treasury': 357,\n",
       " 'harmony': 358,\n",
       " 'charm': 359,\n",
       " 'stands': 360,\n",
       " 'nation': 361,\n",
       " \"ubin's\": 362,\n",
       " 'vision': 363,\n",
       " 'heritage': 364,\n",
       " 'labrador': 365,\n",
       " 'vitality': 366,\n",
       " 'breathtaking': 367,\n",
       " 'lungs': 368,\n",
       " 'leaves': 369,\n",
       " 'arms': 370,\n",
       " 'welcome': 371,\n",
       " \"it's\": 372,\n",
       " 'stepping': 373,\n",
       " 'greatest': 374,\n",
       " 'one': 375,\n",
       " 'fears': 376,\n",
       " 'nourishes': 377,\n",
       " 'sets': 378,\n",
       " 'past': 379,\n",
       " 'louder': 380,\n",
       " 'own': 381,\n",
       " 'imagination': 382,\n",
       " 'enough': 383,\n",
       " 'keys': 384,\n",
       " 'warm': 385,\n",
       " 'wounds': 386,\n",
       " 'brilliance': 387,\n",
       " 'community': 388,\n",
       " 'grateful': 389,\n",
       " 'night': 390,\n",
       " 'mirror': 391,\n",
       " 'tomorrow': 392,\n",
       " 'wounded': 393,\n",
       " 'resides': 394,\n",
       " 'harmonious': 395,\n",
       " 'most': 396,\n",
       " 'illuminates': 397,\n",
       " 'those': 398,\n",
       " 'shields': 399,\n",
       " 'bitterness': 400,\n",
       " 'embracing': 401,\n",
       " 'haven': 402,\n",
       " 'like': 403,\n",
       " 'heals': 404,\n",
       " 'gifts': 405,\n",
       " 'fragrance': 406,\n",
       " 'chorus': 407,\n",
       " 'lighthouse': 408,\n",
       " 'follow': 409,\n",
       " 'soar': 410,\n",
       " 'break': 411,\n",
       " 'many': 412,\n",
       " 'extraordinary': 413,\n",
       " 'them': 414,\n",
       " 'passion': 415,\n",
       " 'share': 416,\n",
       " 'pursuit': 417,\n",
       " \"heart's\": 418,\n",
       " 'desires': 419,\n",
       " 'existence': 420,\n",
       " 'unique': 421,\n",
       " 'hues': 422,\n",
       " 'acts': 423,\n",
       " 'reminders': 424,\n",
       " 'gardens': 425,\n",
       " 'cherish': 426,\n",
       " 'full': 427,\n",
       " 'leaving': 428,\n",
       " 'write': 429,\n",
       " 'intention': 430,\n",
       " 'letting': 431,\n",
       " 'their': 432,\n",
       " 'sea': 433,\n",
       " 'ripple': 434,\n",
       " 'home': 435,\n",
       " 'fuels': 436,\n",
       " 'soothes': 437,\n",
       " 'wonders': 438,\n",
       " 'embodiment': 439,\n",
       " 'setback': 440,\n",
       " 'comeback': 441,\n",
       " 'bloom': 442,\n",
       " 'taken': 443,\n",
       " 'whether': 444,\n",
       " 'painful': 445,\n",
       " 'transitions': 446,\n",
       " 'directed': 447,\n",
       " 'indomitable': 448,\n",
       " 'wave': 449,\n",
       " 'witnessing': 450,\n",
       " 'cherished': 451,\n",
       " 'bay': 452,\n",
       " 'ambition': 453,\n",
       " 'modernity': 454,\n",
       " 'perfect': 455,\n",
       " \"chinatown's\": 456,\n",
       " 'pulse': 457,\n",
       " 'waiting': 458,\n",
       " \"gardens'\": 459,\n",
       " 'history': 460,\n",
       " 'sky': 461,\n",
       " 'quiet': 462,\n",
       " \"nation's\": 463,\n",
       " 'jurong': 464,\n",
       " 'wildlife': 465,\n",
       " 'sungei': 466,\n",
       " 'buloh': 467,\n",
       " 'wetland': 468,\n",
       " 'landscapes': 469,\n",
       " 'rugged': 470,\n",
       " 'witness': 471,\n",
       " 'face': 472,\n",
       " 'ancient': 473,\n",
       " 'survival': 474,\n",
       " 'species': 475,\n",
       " 'reefs': 476,\n",
       " 'cradle': 477,\n",
       " 'artistry': 478,\n",
       " 'rustle': 479,\n",
       " 'springs': 480,\n",
       " \"day's\": 481,\n",
       " 'fills': 482,\n",
       " 'air': 483,\n",
       " 'clarity': 484,\n",
       " 'fill': 485,\n",
       " 'chance': 486,\n",
       " 'stones': 487,\n",
       " 'at': 488,\n",
       " 'uniqueness': 489,\n",
       " 'only': 490,\n",
       " 'multiplies': 491,\n",
       " 'chains': 492,\n",
       " 'could': 493,\n",
       " 'small': 494,\n",
       " 'blueprints': 495,\n",
       " 'cornerstone': 496,\n",
       " 'gateway': 497,\n",
       " 'chest': 498,\n",
       " 'patience': 499,\n",
       " 'far': 500,\n",
       " 'birthplace': 501,\n",
       " 'thoughts': 502,\n",
       " 'nights': 503,\n",
       " 'mend': 504,\n",
       " 'everyday': 505,\n",
       " 'alchemy': 506,\n",
       " 'conductor': 507,\n",
       " 'cosmos': 508,\n",
       " 'kind': 509,\n",
       " 'remind': 510,\n",
       " 'emotions': 511,\n",
       " 'ordinary': 512,\n",
       " 'mind': 513,\n",
       " 'cycles': 514,\n",
       " 'weave': 515,\n",
       " 'cleanse': 516,\n",
       " 'fulfilled': 517,\n",
       " 'burdens': 518,\n",
       " 'healing': 519,\n",
       " 'bright': 520,\n",
       " 'acceptance': 521,\n",
       " 'powerful': 522,\n",
       " \"you'll\": 523,\n",
       " 'contagious': 524,\n",
       " 'set': 525,\n",
       " 'spreads': 526,\n",
       " 'wildfire': 527,\n",
       " 'sails': 528,\n",
       " 'fortress': 529,\n",
       " 'balm': 530,\n",
       " 'old': 531,\n",
       " 'sparks': 532,\n",
       " 'transcends': 533,\n",
       " 'all': 534,\n",
       " 'barriers': 535,\n",
       " 'drives': 536,\n",
       " 'ages': 537,\n",
       " 'powers': 538,\n",
       " 'conquer': 539,\n",
       " 'achieve': 540,\n",
       " 'difference': 541,\n",
       " 'adversity': 542,\n",
       " 'things': 543,\n",
       " 'agent': 544,\n",
       " 'anything': 545,\n",
       " 'melodies': 546,\n",
       " 'eyes': 547,\n",
       " 'deepest': 548,\n",
       " 'intentions': 549,\n",
       " 'away': 550,\n",
       " 'choices': 551,\n",
       " 'spread': 552,\n",
       " 'receive': 553,\n",
       " 'return': 554,\n",
       " 'makes': 555,\n",
       " 'grow': 556,\n",
       " 'plays': 557,\n",
       " 'watered': 558,\n",
       " 'renews': 559,\n",
       " 'sense': 560,\n",
       " 'forth': 561,\n",
       " 'differences': 562,\n",
       " 'resonate': 563,\n",
       " 'sowing': 564,\n",
       " 'lies': 565,\n",
       " 'freedom': 566,\n",
       " 'painting': 567,\n",
       " 'together': 568,\n",
       " 'truth': 569,\n",
       " 'rich': 570,\n",
       " 'waters': 571,\n",
       " 'transforms': 572,\n",
       " 'speak': 573,\n",
       " 'north': 574,\n",
       " 'which': 575,\n",
       " 'stand': 576,\n",
       " 'fire': 577,\n",
       " 'reflects': 578,\n",
       " 'infectious': 579,\n",
       " 'spreading': 580,\n",
       " 'sword': 581,\n",
       " 'cuts': 582,\n",
       " 'connections': 583,\n",
       " 'decision': 584,\n",
       " 'wellspring': 585,\n",
       " 'seas': 586,\n",
       " 'ripples': 587,\n",
       " 'preciousness': 588,\n",
       " 'uplift': 589,\n",
       " 'shapes': 590,\n",
       " 'between': 591,\n",
       " 'meet': 592,\n",
       " 'deserves': 593,\n",
       " 'becoming': 594,\n",
       " 'blossoming': 595,\n",
       " 'alter': 596,\n",
       " 'course': 597,\n",
       " 'reverberates': 598,\n",
       " 'stroke': 599,\n",
       " 'brush': 600,\n",
       " 'tale': 601,\n",
       " 'state': 602,\n",
       " 'contentment': 603,\n",
       " 'reside': 604,\n",
       " 'wide': 605,\n",
       " 'comes': 606,\n",
       " 'cultures': 607,\n",
       " 'bounds': 608,\n",
       " 'marina': 609,\n",
       " 'orchard': 610,\n",
       " \"sentosa's\": 611,\n",
       " 'corner': 612,\n",
       " 'merlion': 613,\n",
       " 'symbol': 614,\n",
       " 'timah': 615,\n",
       " 'reaches': 616,\n",
       " 'defines': 617,\n",
       " 'intertwine': 618,\n",
       " 'tells': 619,\n",
       " 'alive': 620,\n",
       " 'tales': 621,\n",
       " 'southern': 622,\n",
       " 'bird': 623,\n",
       " \"safari's\": 624,\n",
       " 'mysteries': 625,\n",
       " 'endeavor': 626,\n",
       " 'excellence': 627,\n",
       " 'changi': 628,\n",
       " 'vibrancy': 629,\n",
       " 'macritchie': 630,\n",
       " \"reservoir's\": 631,\n",
       " 'picture': 632,\n",
       " 'reaching': 633,\n",
       " 'diligence': 634,\n",
       " 'views': 635,\n",
       " 'embodies': 636,\n",
       " 'brand': 637,\n",
       " 'lake': 638,\n",
       " 'destinies': 639,\n",
       " 'chek': 640,\n",
       " \"jawa's\": 641,\n",
       " 'unwavering': 642,\n",
       " 'batok': 643,\n",
       " 'marine': 644,\n",
       " 'trove': 645,\n",
       " 'flourish': 646,\n",
       " 'stewardship': 647,\n",
       " 'forests': 648,\n",
       " 'breathe': 649,\n",
       " 'rivers': 650,\n",
       " 'skies': 651,\n",
       " 'cities': 652,\n",
       " 'footprint': 653,\n",
       " 'wetlands': 654,\n",
       " 'see': 655,\n",
       " 'drop': 656,\n",
       " \"earth's\": 657,\n",
       " 'savannas': 658,\n",
       " 'brushstrokes': 659,\n",
       " 'fiery': 660,\n",
       " 'pledge': 661,\n",
       " 'silent': 662,\n",
       " 'grass': 663,\n",
       " 'caves': 664,\n",
       " 'endurance': 665,\n",
       " 'renewal': 666,\n",
       " 'delicate': 667,\n",
       " 'ecosystems': 668,\n",
       " 'nurseries': 669,\n",
       " 'sand': 670,\n",
       " 'inhale': 671,\n",
       " 'veins': 672,\n",
       " 'land': 673,\n",
       " 'rising': 674,\n",
       " 'sun': 675,\n",
       " 'today': 676,\n",
       " 'presents': 677,\n",
       " 'reflect': 678,\n",
       " 'chambers': 679,\n",
       " 'soundtrack': 680,\n",
       " 'simple': 681,\n",
       " 'joys': 682,\n",
       " 'navigate': 683,\n",
       " 'illuminate': 684,\n",
       " 'fully': 685,\n",
       " 'reminding': 686,\n",
       " 'victories': 687,\n",
       " 'destination': 688,\n",
       " 'fingerprint': 689,\n",
       " 'exists': 690,\n",
       " 'silence': 691,\n",
       " 'speaks': 692,\n",
       " 'often': 693,\n",
       " 'significance': 694,\n",
       " 'constant': 695,\n",
       " 'chaos': 696,\n",
       " 'unknown': 697,\n",
       " 'midst': 698,\n",
       " 'peaceful': 699,\n",
       " 'elders': 700,\n",
       " 'allows': 701,\n",
       " 'sing': 702,\n",
       " 'turn': 703,\n",
       " 'off': 704,\n",
       " 'lands': 705,\n",
       " 'vulnerability': 706,\n",
       " 'ability': 707,\n",
       " 'itself': 708,\n",
       " 'without': 709,\n",
       " 'lullaby': 710,\n",
       " 'hug': 711,\n",
       " 'shadows': 712,\n",
       " 'expanding': 713,\n",
       " 'moon': 714,\n",
       " 'loving': 715,\n",
       " 'cannot': 716,\n",
       " 'affirmations': 717,\n",
       " 'solitude': 718,\n",
       " 'another': 719,\n",
       " 'raindrops': 720,\n",
       " 'nurture': 721,\n",
       " 'restore': 722,\n",
       " 'portrait': 723,\n",
       " 'radiates': 724,\n",
       " 'nurtures': 725,\n",
       " 'body': 726,\n",
       " 'deed': 727,\n",
       " 'restless': 728,\n",
       " 'balance': 729,\n",
       " 'gloomiest': 730,\n",
       " 'trust': 731,\n",
       " 'relationships': 732,\n",
       " 'minds': 733,\n",
       " 'attract': 734,\n",
       " 'great': 735,\n",
       " 'weight': 736,\n",
       " 'grudges': 737,\n",
       " 'keeps': 738,\n",
       " 'signature': 739,\n",
       " 'simplest': 740,\n",
       " 'anchor': 741,\n",
       " 'exploration': 742,\n",
       " 'smallest': 743,\n",
       " 'refuge': 744,\n",
       " 'weary': 745,\n",
       " 'bedrock': 746,\n",
       " 'generations': 747,\n",
       " 'unlocks': 748,\n",
       " 'mends': 749,\n",
       " 'unstoppable': 750,\n",
       " 'happen': 751,\n",
       " 'impossible': 752,\n",
       " 'think': 753,\n",
       " 'just': 754,\n",
       " 'as': 755,\n",
       " 'becomes': 756,\n",
       " 'conspire': 757,\n",
       " 'favor': 758,\n",
       " 'making': 759,\n",
       " 'footprints': 760,\n",
       " 'barrier': 761,\n",
       " 'forge': 762,\n",
       " 'limitless': 763,\n",
       " 'too': 764,\n",
       " 'unlock': 765,\n",
       " 'doors': 766,\n",
       " 'destined': 767,\n",
       " 'persevere': 768,\n",
       " 'darkness': 769,\n",
       " 'seek': 770,\n",
       " 'work': 771,\n",
       " 'art': 772,\n",
       " 'amazing': 773,\n",
       " 'limitation': 774,\n",
       " 'achieving': 775,\n",
       " 'architect': 776,\n",
       " 'brightest': 777,\n",
       " 'followed': 778,\n",
       " 'rainbows': 779,\n",
       " 'plant': 780,\n",
       " 'move': 781,\n",
       " 'points': 782,\n",
       " 'filled': 783,\n",
       " 'blend': 784,\n",
       " 'illuminated': 785,\n",
       " 'reflected': 786,\n",
       " 'multiply': 787,\n",
       " 'pieces': 788,\n",
       " 'unconditionally': 789,\n",
       " 'ink': 790,\n",
       " 'deeds': 791,\n",
       " 'calling': 792,\n",
       " 'disguise': 793,\n",
       " 'washes': 794,\n",
       " 'define': 795,\n",
       " 'forgive': 796,\n",
       " 'pen': 797,\n",
       " 'nudges': 798,\n",
       " 'triumphs': 799,\n",
       " 'choose': 800,\n",
       " 'found': 801,\n",
       " 'meaning': 802,\n",
       " 'ourselves': 803,\n",
       " 'guidance': 804,\n",
       " \"we've\": 805,\n",
       " 'traveled': 806,\n",
       " 'hopes': 807,\n",
       " 'offer': 808,\n",
       " 'pass': 809,\n",
       " 'uncertainty': 810,\n",
       " 'prelude': 811,\n",
       " 'savoring': 812,\n",
       " 'other': 813,\n",
       " 'side': 814,\n",
       " 'sweetest': 815,\n",
       " 'twist': 816,\n",
       " 'brightness': 817,\n",
       " 'notes': 818,\n",
       " 'tending': 819,\n",
       " 'steps': 820,\n",
       " 'limits': 821,\n",
       " 'lift': 822,\n",
       " 'weaves': 823,\n",
       " 'cultivating': 824,\n",
       " 'renew': 825,\n",
       " 'fruits': 826,\n",
       " 'pillars': 827,\n",
       " 'unveil': 828,\n",
       " 'weaving': 829,\n",
       " 'heights': 830,\n",
       " 'halls': 831,\n",
       " 'so': 832,\n",
       " 'brightly': 833,\n",
       " 'anthem': 834,\n",
       " 'against': 835,\n",
       " 'rock': 836,\n",
       " 'wildest': 837,\n",
       " 'binds': 838,\n",
       " 'wherever': 839,\n",
       " 'uplifts': 840,\n",
       " 'fear': 841,\n",
       " 'action': 842,\n",
       " 'stormy': 843,\n",
       " 'fosters': 844,\n",
       " 'achievements': 845,\n",
       " 'currency': 846,\n",
       " 'interactions': 847,\n",
       " 'unites': 848,\n",
       " 'principles': 849,\n",
       " 'engine': 850,\n",
       " 'contagion': 851,\n",
       " 'doubt': 852,\n",
       " 'salve': 853,\n",
       " 'turbulent': 854,\n",
       " 'protects': 855,\n",
       " 'values': 856,\n",
       " 'victory': 857,\n",
       " 'shared': 858,\n",
       " 'beam': 859,\n",
       " 'frees': 860,\n",
       " 'spoken': 861,\n",
       " 'counts': 862,\n",
       " 'thread': 863,\n",
       " 'human': 864,\n",
       " 'bad': 865,\n",
       " 'lesson': 866,\n",
       " 'embraced': 867,\n",
       " 'contented': 868,\n",
       " 'shown': 869,\n",
       " 'forged': 870,\n",
       " 'miracle': 871,\n",
       " 'fate': 872,\n",
       " 'mold': 873,\n",
       " 'glorious': 874,\n",
       " 'narrative': 875,\n",
       " 'farewells': 876,\n",
       " 'enlightenment': 877,\n",
       " 'out': 878,\n",
       " 'influence': 879,\n",
       " 'triumphant': 880,\n",
       " 'determined': 881,\n",
       " 'joyous': 882,\n",
       " 'worth': 883,\n",
       " 'telling': 884,\n",
       " 'ebb': 885,\n",
       " 'goodbyes': 886,\n",
       " 'transcending': 887,\n",
       " 'fostering': 888,\n",
       " 'serene': 889,\n",
       " 'blooming': 890,\n",
       " 'significant': 891,\n",
       " 'part': 892,\n",
       " 'turning': 893,\n",
       " 'point': 894,\n",
       " 'defy': 895,\n",
       " 'logic': 896,\n",
       " 'evolving': 897,\n",
       " 'wiser': 898,\n",
       " 'resilient': 899,\n",
       " 'version': 900,\n",
       " 'release': 901,\n",
       " 'resentment': 902,\n",
       " 'providing': 903,\n",
       " 'assurance': 904,\n",
       " 'not': 905,\n",
       " 'alone': 906,\n",
       " 'realization': 907,\n",
       " 'fulfillment': 908,\n",
       " 'richness': 909,\n",
       " 'meaningful': 910,\n",
       " 'connect': 911,\n",
       " 'affection': 912,\n",
       " 'deliberate': 913,\n",
       " 'shaping': 914,\n",
       " 'living': 915,\n",
       " 'proof': 916,\n",
       " 'conquering': 917,\n",
       " 'converge': 918,\n",
       " 'road': 919,\n",
       " 'roars': 920,\n",
       " 'embraces': 921,\n",
       " 'traditions': 922,\n",
       " 'reach': 923,\n",
       " 'beaches': 924,\n",
       " 'alleys': 925,\n",
       " 'southeast': 926,\n",
       " 'asia': 927,\n",
       " 'beats': 928,\n",
       " 'strong': 929,\n",
       " 'raffles': 930,\n",
       " 'place': 931,\n",
       " 'clarke': 932,\n",
       " 'quay': 933,\n",
       " 'told': 934,\n",
       " 'hawker': 935,\n",
       " 'centers': 936,\n",
       " 'fine': 937,\n",
       " 'dining': 938,\n",
       " 'flavors': 939,\n",
       " 'delectable': 940,\n",
       " 'ideas': 941,\n",
       " 'little': 942,\n",
       " \"india's\": 943,\n",
       " 'botanic': 944,\n",
       " 'glistens': 945,\n",
       " 'universal': 946,\n",
       " 'studios': 947,\n",
       " 'attractions': 948,\n",
       " 'class': 949,\n",
       " 'futures': 950,\n",
       " 'built': 951,\n",
       " 'park': 952,\n",
       " 'reserve': 953,\n",
       " 'heavens': 954,\n",
       " 'island': 955,\n",
       " \"road's\": 956,\n",
       " 'shopping': 957,\n",
       " 'shores': 958,\n",
       " 'trails': 959,\n",
       " 'leap': 960,\n",
       " 'sands': 961,\n",
       " 'flyer': 962,\n",
       " \"city's\": 963,\n",
       " 'landmarks': 964,\n",
       " 'lanterns': 965,\n",
       " 'kampong': 966,\n",
       " \"glam's\": 967,\n",
       " \"esplanade's\": 968,\n",
       " 'performances': 969,\n",
       " 'haw': 970,\n",
       " 'par': 971,\n",
       " \"villa's\": 972,\n",
       " 'culture': 973,\n",
       " 'thriving': 974,\n",
       " \"tekong's\": 975,\n",
       " \"islands'\": 976,\n",
       " 'testifies': 977,\n",
       " 'fort': 978,\n",
       " \"canning's\": 979,\n",
       " 'ruggedness': 980,\n",
       " 'dedication': 981,\n",
       " 'stone': 982,\n",
       " 'stretches': 983,\n",
       " 'horizon': 984,\n",
       " 'reign': 985,\n",
       " 'east': 986,\n",
       " 'coast': 987,\n",
       " 'breezes': 988,\n",
       " \"semakau's\": 989,\n",
       " 'talents': 990,\n",
       " 'honed': 991,\n",
       " 'peranakan': 992,\n",
       " 'houses': 993,\n",
       " 'colonial': 994,\n",
       " 'architecture': 995,\n",
       " 'preserved': 996,\n",
       " 'pride': 997,\n",
       " \"airport's\": 998,\n",
       " 'efficiency': 999,\n",
       " \"river's\": 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embrace the beauty of every sunrise; it's a fresh chance to paint your world with joy.\n",
      "[[17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687]]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(tokenizer.texts_to_sequences([data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "padded_sequences = pad_sequences(sequences, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 869), ('of', 663), ('your', 350), ('and', 322), ('a', 307), ('is', 253), ('in', 249), ('for', 201), ('let', 187), ('to', 180)]\n",
      "There are 1198 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_freq = tokenizer.word_counts\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_word_freq[:10])\n",
    "\n",
    "num_unique_words = len(tokenizer.word_index)\n",
    "print(f\"There are {num_unique_words} unique words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 890 Quotes and 1198 Unique Words. In those 890 quotes we have 869 'the' and 663 'of' it's very likely that the model we build will often predict those 2 phrases very often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with 'the' and 'of'\n",
    "We can create new data, that is less biased towards the 'the' and 'of'.  \n",
    "Slice the quotes into different sizes and then train the model on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 2, 3 words\n",
    "phrases = []\n",
    "for quote in sequences:\n",
    "    for phrase_length in range(2, 8):  \n",
    "        for i in range(len(quote) - phrase_length + 1):\n",
    "            phrases.append(quote[i : i + phrase_length])  \n",
    "\n",
    "total_data = phrases + sequences\n",
    "max_sequence_len = max([len(x) for x in total_data])\n",
    "padded_sequences = pad_sequences(total_data, maxlen=max_sequence_len, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45914, 34)\n",
      "(45914, 1199)\n"
     ]
    }
   ],
   "source": [
    "x = padded_sequences[:, :-1]\n",
    "labels = padded_sequences[:, -1]\n",
    "labels_encoded = tf.keras.utils.to_categorical(labels, num_classes=num_unique_words + 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(labels_encoded.shape)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the inital model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "359/359 [==============================] - 12s 32ms/step - loss: 5.2299 - accuracy: 0.1283\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 3.9796 - accuracy: 0.2503\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 3.2911 - accuracy: 0.3289\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 11s 32ms/step - loss: 2.8366 - accuracy: 0.3888\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 2.5077 - accuracy: 0.4396\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 2.2621 - accuracy: 0.4844\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 2.0755 - accuracy: 0.5184\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.9337 - accuracy: 0.5412\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.8188 - accuracy: 0.5634\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 11s 32ms/step - loss: 1.7330 - accuracy: 0.5791\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.6573 - accuracy: 0.5958\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.6004 - accuracy: 0.6048\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.5403 - accuracy: 0.6162\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4887 - accuracy: 0.6275\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4515 - accuracy: 0.6327\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4202 - accuracy: 0.6432\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4170 - accuracy: 0.6427\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.3810 - accuracy: 0.6468\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.3460 - accuracy: 0.6566\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.3287 - accuracy: 0.6593\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.3178 - accuracy: 0.6645\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2949 - accuracy: 0.6658\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2799 - accuracy: 0.6703\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2661 - accuracy: 0.6719\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2538 - accuracy: 0.6751\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2474 - accuracy: 0.6750\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2337 - accuracy: 0.6773\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2209 - accuracy: 0.6836\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2307 - accuracy: 0.6790\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2125 - accuracy: 0.6807\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 11s 29ms/step - loss: 1.2094 - accuracy: 0.6818\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1964 - accuracy: 0.6886\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1934 - accuracy: 0.6861\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1869 - accuracy: 0.6871\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1874 - accuracy: 0.6889\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 10s 29ms/step - loss: 1.1812 - accuracy: 0.6893\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1737 - accuracy: 0.6906\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1770 - accuracy: 0.6900\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1718 - accuracy: 0.6912\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1708 - accuracy: 0.6912\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1598 - accuracy: 0.6925\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1595 - accuracy: 0.6919\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1565 - accuracy: 0.6928\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1554 - accuracy: 0.6936\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1557 - accuracy: 0.6943\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 11s 29ms/step - loss: 1.1459 - accuracy: 0.6952\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1441 - accuracy: 0.6946\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1534 - accuracy: 0.6957\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1383 - accuracy: 0.6979\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1403 - accuracy: 0.6969\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1396 - accuracy: 0.6977\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1399 - accuracy: 0.6981\n",
      "Epoch 1/100\n",
      "359/359 [==============================] - 4s 8ms/step - loss: 5.3793 - accuracy: 0.0984\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 4.6542 - accuracy: 0.1671\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 4.1431 - accuracy: 0.2208\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.7981 - accuracy: 0.2617\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.4979 - accuracy: 0.2987\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.2244 - accuracy: 0.3367\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.9956 - accuracy: 0.3656\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.8050 - accuracy: 0.3882\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.6322 - accuracy: 0.4148\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.6345 - accuracy: 0.4202\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.4048 - accuracy: 0.4483\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.2914 - accuracy: 0.4672\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.1913 - accuracy: 0.4812\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.0973 - accuracy: 0.4991\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.0190 - accuracy: 0.5154\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.9494 - accuracy: 0.5240\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.8839 - accuracy: 0.5391\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.8305 - accuracy: 0.5472\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.7773 - accuracy: 0.5585\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.7287 - accuracy: 0.5689\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6833 - accuracy: 0.5769\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6455 - accuracy: 0.5841\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6083 - accuracy: 0.5906\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5702 - accuracy: 0.5963\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5457 - accuracy: 0.6007\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5103 - accuracy: 0.6120\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4809 - accuracy: 0.6155\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4547 - accuracy: 0.6219\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4339 - accuracy: 0.6244\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4087 - accuracy: 0.6293\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3889 - accuracy: 0.6339\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3706 - accuracy: 0.6373\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3501 - accuracy: 0.6421\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3357 - accuracy: 0.6463\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3192 - accuracy: 0.6498\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3010 - accuracy: 0.6509\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2865 - accuracy: 0.6565\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2750 - accuracy: 0.6564\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2639 - accuracy: 0.6611\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2517 - accuracy: 0.6619\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2426 - accuracy: 0.6629\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2275 - accuracy: 0.6673\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2210 - accuracy: 0.6686\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2089 - accuracy: 0.6698\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1992 - accuracy: 0.6737\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1895 - accuracy: 0.6759\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1821 - accuracy: 0.6765\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1743 - accuracy: 0.6789\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1648 - accuracy: 0.6796\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1608 - accuracy: 0.6782\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1554 - accuracy: 0.6799\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1483 - accuracy: 0.6829\n",
      "Epoch 53/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1399 - accuracy: 0.6864\n",
      "Epoch 54/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1341 - accuracy: 0.6854\n",
      "Epoch 55/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1253 - accuracy: 0.6873\n",
      "Epoch 56/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1173 - accuracy: 0.6887\n",
      "Epoch 57/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1159 - accuracy: 0.6895\n",
      "Epoch 58/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1067 - accuracy: 0.6908\n",
      "Epoch 59/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1066 - accuracy: 0.6905\n",
      "Epoch 60/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1008 - accuracy: 0.6928\n",
      "Epoch 61/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0956 - accuracy: 0.6958\n",
      "Epoch 62/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0937 - accuracy: 0.6939\n",
      "Epoch 63/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0851 - accuracy: 0.6953\n",
      "Epoch 64/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0847 - accuracy: 0.6967\n",
      "Epoch 65/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0793 - accuracy: 0.6982\n",
      "Epoch 66/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0746 - accuracy: 0.6965\n",
      "Epoch 67/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0674 - accuracy: 0.6991\n",
      "Epoch 68/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0685 - accuracy: 0.6989\n",
      "Epoch 69/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0616 - accuracy: 0.6998\n",
      "Epoch 70/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0598 - accuracy: 0.7018\n",
      "Epoch 71/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0563 - accuracy: 0.7005\n",
      "Epoch 72/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0539 - accuracy: 0.7021\n",
      "Epoch 73/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0514 - accuracy: 0.7020\n",
      "Epoch 74/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0474 - accuracy: 0.7048\n",
      "Epoch 75/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0466 - accuracy: 0.7041\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0409 - accuracy: 0.7038\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0362 - accuracy: 0.7055\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0361 - accuracy: 0.7046\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0338 - accuracy: 0.7057\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0311 - accuracy: 0.7051\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0303 - accuracy: 0.7065\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0250 - accuracy: 0.7066\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0210 - accuracy: 0.7068\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0190 - accuracy: 0.7088\n",
      "Epoch 85/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0156 - accuracy: 0.7093\n",
      "Epoch 86/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0168 - accuracy: 0.7099\n",
      "Epoch 87/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0145 - accuracy: 0.7101\n",
      "Epoch 88/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0129 - accuracy: 0.7090\n",
      "Epoch 89/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0096 - accuracy: 0.7080\n",
      "Epoch 90/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0052 - accuracy: 0.7130\n",
      "Epoch 91/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0058 - accuracy: 0.7096\n",
      "Epoch 92/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0001 - accuracy: 0.7139\n",
      "Epoch 93/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0020 - accuracy: 0.7090\n",
      "Epoch 94/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0009 - accuracy: 0.7107\n",
      "Epoch 95/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9982 - accuracy: 0.7106\n",
      "Epoch 96/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.9947 - accuracy: 0.7129\n",
      "Epoch 97/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9951 - accuracy: 0.7127\n",
      "Epoch 98/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9941 - accuracy: 0.7104\n",
      "Epoch 99/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9917 - accuracy: 0.7134\n",
      "Epoch 100/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9911 - accuracy: 0.7128\n",
      "Epoch 1/100\n",
      "359/359 [==============================] - 4s 7ms/step - loss: 5.2148 - accuracy: 0.1144\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 4.0232 - accuracy: 0.2356\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.2861 - accuracy: 0.3319\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.8097 - accuracy: 0.3946\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.4747 - accuracy: 0.4469\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.2298 - accuracy: 0.4856\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.0469 - accuracy: 0.5209\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.9012 - accuracy: 0.5496\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.7880 - accuracy: 0.5699\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.6948 - accuracy: 0.5892\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.6199 - accuracy: 0.6039\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5571 - accuracy: 0.6171\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5040 - accuracy: 0.6256\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4569 - accuracy: 0.6341\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4197 - accuracy: 0.6428\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3835 - accuracy: 0.6520\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3540 - accuracy: 0.6555\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3270 - accuracy: 0.6611\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3068 - accuracy: 0.6641\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2896 - accuracy: 0.6662\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2663 - accuracy: 0.6728\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2514 - accuracy: 0.6764\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2363 - accuracy: 0.6779\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2249 - accuracy: 0.6803\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2119 - accuracy: 0.6837\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1979 - accuracy: 0.6865\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1898 - accuracy: 0.6881\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1849 - accuracy: 0.6886\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1760 - accuracy: 0.6901\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1624 - accuracy: 0.6915\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1550 - accuracy: 0.6940\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1457 - accuracy: 0.6968\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1410 - accuracy: 0.6981\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1347 - accuracy: 0.6973\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1315 - accuracy: 0.6983\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1221 - accuracy: 0.6982\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1126 - accuracy: 0.7000\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1158 - accuracy: 0.7002\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1087 - accuracy: 0.7017\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1038 - accuracy: 0.7018\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1024 - accuracy: 0.7032\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1001 - accuracy: 0.7046\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0914 - accuracy: 0.7058\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0893 - accuracy: 0.7035\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0855 - accuracy: 0.7046\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0885 - accuracy: 0.7045\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0837 - accuracy: 0.7046\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0775 - accuracy: 0.7044\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0750 - accuracy: 0.7082\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0745 - accuracy: 0.7065\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0716 - accuracy: 0.7067\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0635 - accuracy: 0.7063\n",
      "Epoch 53/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0633 - accuracy: 0.7094\n",
      "Epoch 54/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0614 - accuracy: 0.7091\n",
      "Epoch 55/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0621 - accuracy: 0.7075\n",
      "Epoch 56/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0604 - accuracy: 0.7079\n",
      "Epoch 57/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0600 - accuracy: 0.7076\n",
      "Epoch 58/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0513 - accuracy: 0.7102\n",
      "Epoch 59/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0501 - accuracy: 0.7105\n",
      "Epoch 60/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0489 - accuracy: 0.7102\n",
      "Epoch 61/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0515 - accuracy: 0.7083\n",
      "Epoch 62/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0478 - accuracy: 0.7097\n",
      "Epoch 63/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0477 - accuracy: 0.7082\n",
      "Epoch 64/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0429 - accuracy: 0.7083\n",
      "Epoch 65/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0424 - accuracy: 0.7096\n",
      "Epoch 66/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0425 - accuracy: 0.7095\n",
      "Epoch 67/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0378 - accuracy: 0.7112\n",
      "Epoch 68/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0322 - accuracy: 0.7122\n",
      "Epoch 69/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0337 - accuracy: 0.7105\n",
      "Epoch 70/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0344 - accuracy: 0.7139\n",
      "Epoch 71/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0287 - accuracy: 0.7111\n",
      "Epoch 72/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0308 - accuracy: 0.7123\n",
      "Epoch 73/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0320 - accuracy: 0.7111\n",
      "Epoch 74/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0271 - accuracy: 0.7102\n",
      "Epoch 75/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0236 - accuracy: 0.7121\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0264 - accuracy: 0.7126\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0263 - accuracy: 0.7115\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0234 - accuracy: 0.7114\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0234 - accuracy: 0.7121\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0240 - accuracy: 0.7104\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0174 - accuracy: 0.7137\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0196 - accuracy: 0.7119\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0194 - accuracy: 0.7103\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0190 - accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "def first_model_LSTM():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def first_model_GRU():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(GRU(256, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def basic_rnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(SimpleRNN(256))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "basic_rnn = basic_rnn_model()\n",
    "base_LSTM = first_model_LSTM()\n",
    "base_GRU = first_model_GRU()\n",
    "\n",
    "\n",
    "history_basic_rnn_model = basic_rnn.fit(x, labels_encoded, batch_size=128,epochs=100,verbose=1,shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_rnn.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_base_LSTM = first_model_LSTM().fit(x, labels_encoded, batch_size=128, epochs=100, verbose=1, shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_LSTM.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_base_GRU = base_GRU.fit(x, labels_encoded, batch_size=128, epochs=100, verbose=1, shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_GRU.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_base = pd.DataFrame(\n",
    "    [{\n",
    "        \"basic_rnn\": min(history_basic_rnn_model.history[\"loss\"]),\n",
    "        \"base_LSTM\": min(history_base_LSTM.history[\"loss\"]),\n",
    "        \"base_GRU\": min(history_base_GRU.history[\"loss\"]),\n",
    "    }]\n",
    ")\n",
    "plt.bar(results_base.columns, results_base.iloc[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_layers(number_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    if number_layers != 0:\n",
    "        for i in range(number_layers):\n",
    "            model.add(LSTM(256, return_sequences=True))\n",
    "            model.add(LayerNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x,\n",
    "        labels_encoded,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "718/718 [==============================] - 6s 7ms/step - loss: 5.2025 - accuracy: 0.1099\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 4.4856 - accuracy: 0.1642\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 3.8169 - accuracy: 0.2593\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 3.2557 - accuracy: 0.3364\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 7s 10ms/step - loss: 2.8887 - accuracy: 0.3802\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.6179 - accuracy: 0.4165\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.4243 - accuracy: 0.4484\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.2433 - accuracy: 0.4768\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.1010 - accuracy: 0.5018\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.9832 - accuracy: 0.5255\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.8848 - accuracy: 0.5434\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.7907 - accuracy: 0.5610\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.7224 - accuracy: 0.5739\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.6596 - accuracy: 0.5876\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.6045 - accuracy: 0.5979\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.5455 - accuracy: 0.6118\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.5022 - accuracy: 0.6180\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.6258\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.4295 - accuracy: 0.6341\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3990 - accuracy: 0.6372\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3636 - accuracy: 0.6442\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3414 - accuracy: 0.6488\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3167 - accuracy: 0.6537\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.2971 - accuracy: 0.6580\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.2766 - accuracy: 0.6632\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2542 - accuracy: 0.6664\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2393 - accuracy: 0.6715\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2211 - accuracy: 0.6740\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 6s 9ms/step - loss: 1.2106 - accuracy: 0.6757\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.1957 - accuracy: 0.6793\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1839 - accuracy: 0.6808\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1721 - accuracy: 0.6814\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1620 - accuracy: 0.6842\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.1544 - accuracy: 0.6848\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.1455 - accuracy: 0.6870\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 6s 9ms/step - loss: 1.1330 - accuracy: 0.6906\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1246 - accuracy: 0.6925\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1178 - accuracy: 0.6930\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1129 - accuracy: 0.6943\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1030 - accuracy: 0.6952\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0967 - accuracy: 0.6964\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0895 - accuracy: 0.6989\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0880 - accuracy: 0.6985\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0803 - accuracy: 0.6995\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0765 - accuracy: 0.7027\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0727 - accuracy: 0.7004\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0658 - accuracy: 0.7023\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0629 - accuracy: 0.7042\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0553 - accuracy: 0.7063\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0559 - accuracy: 0.7030\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0514 - accuracy: 0.7050\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0469 - accuracy: 0.7069\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0443 - accuracy: 0.7060\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0404 - accuracy: 0.7075\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0369 - accuracy: 0.7081\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0345 - accuracy: 0.7079\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0287 - accuracy: 0.7086\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0286 - accuracy: 0.7092\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0270 - accuracy: 0.7089\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0213 - accuracy: 0.7099\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0211 - accuracy: 0.7096\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.0161 - accuracy: 0.7103\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0128 - accuracy: 0.7118\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0143 - accuracy: 0.7112\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0093 - accuracy: 0.7107\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0091 - accuracy: 0.7120\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0068 - accuracy: 0.7114\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0054 - accuracy: 0.7115\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0021 - accuracy: 0.7123\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.0012 - accuracy: 0.7145\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9965 - accuracy: 0.7127\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9972 - accuracy: 0.7127\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 7s 10ms/step - loss: 0.9905 - accuracy: 0.7142\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9932 - accuracy: 0.7128\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9920 - accuracy: 0.7126\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9882 - accuracy: 0.7153\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9891 - accuracy: 0.7139\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9812 - accuracy: 0.7158\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9833 - accuracy: 0.7156\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9809 - accuracy: 0.7163\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9822 - accuracy: 0.7144\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9815 - accuracy: 0.7157\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9790 - accuracy: 0.7165\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9756 - accuracy: 0.7158\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9770 - accuracy: 0.7156\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9760 - accuracy: 0.7154\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9721 - accuracy: 0.7180\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9740 - accuracy: 0.7176\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9743 - accuracy: 0.7160\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9654 - accuracy: 0.7176\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9670 - accuracy: 0.7185\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9660 - accuracy: 0.7176\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9647 - accuracy: 0.7175\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9656 - accuracy: 0.7178\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9647 - accuracy: 0.7176\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9634 - accuracy: 0.7178\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9637 - accuracy: 0.7165\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9591 - accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9614 - accuracy: 0.7177\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9595 - accuracy: 0.7171\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 11s 13ms/step - loss: 5.1643 - accuracy: 0.1108\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 4.2784 - accuracy: 0.1825\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.7957 - accuracy: 0.2424\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 3.3792 - accuracy: 0.2994\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 3.0763 - accuracy: 0.3378\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 2.8538 - accuracy: 0.3659\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.6820 - accuracy: 0.3877\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.5387 - accuracy: 0.4087\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.4197 - accuracy: 0.4281\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.3055 - accuracy: 0.4440\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.2078 - accuracy: 0.4632\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.1215 - accuracy: 0.4798\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.0423 - accuracy: 0.4917\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.9699 - accuracy: 0.5053\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.9020 - accuracy: 0.5202\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.8465 - accuracy: 0.5292\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.7878 - accuracy: 0.5422\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.7366 - accuracy: 0.5530\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.6916 - accuracy: 0.5632\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.6544 - accuracy: 0.5702\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.6155 - accuracy: 0.5766\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5797 - accuracy: 0.5847\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.5511 - accuracy: 0.5915\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.5202 - accuracy: 0.5989\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.4935 - accuracy: 0.6021\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4658 - accuracy: 0.6082\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.4461 - accuracy: 0.6134\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.4219 - accuracy: 0.6200\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.4020 - accuracy: 0.6255\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3820 - accuracy: 0.6277\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3661 - accuracy: 0.6339\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3480 - accuracy: 0.6364\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3338 - accuracy: 0.6390\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3162 - accuracy: 0.6451\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3151 - accuracy: 0.6439\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2958 - accuracy: 0.6487\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2828 - accuracy: 0.6495\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2765 - accuracy: 0.6517\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2562 - accuracy: 0.6552\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2492 - accuracy: 0.6599\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2375 - accuracy: 0.6617\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2299 - accuracy: 0.6622\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2279 - accuracy: 0.6625\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2092 - accuracy: 0.6670\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2058 - accuracy: 0.6658\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1980 - accuracy: 0.6699\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1876 - accuracy: 0.6706\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1774 - accuracy: 0.6715\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1770 - accuracy: 0.6732\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1636 - accuracy: 0.6759\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1586 - accuracy: 0.6792\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1534 - accuracy: 0.6788\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1473 - accuracy: 0.6798\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1501 - accuracy: 0.6796\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1364 - accuracy: 0.6820\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1315 - accuracy: 0.6818\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1285 - accuracy: 0.6844\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.1242 - accuracy: 0.6865\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.1178 - accuracy: 0.6868\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.1114 - accuracy: 0.6900\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1083 - accuracy: 0.6879\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1024 - accuracy: 0.6917\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1007 - accuracy: 0.6916\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0965 - accuracy: 0.6921\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0880 - accuracy: 0.6941\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0852 - accuracy: 0.6956\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0811 - accuracy: 0.6962\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0795 - accuracy: 0.6950\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0805 - accuracy: 0.6941\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0774 - accuracy: 0.6942\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0732 - accuracy: 0.6962\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0693 - accuracy: 0.6977\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0655 - accuracy: 0.6973\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0589 - accuracy: 0.6994\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0607 - accuracy: 0.6985\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0611 - accuracy: 0.6987\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0570 - accuracy: 0.6981\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0531 - accuracy: 0.6988\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0499 - accuracy: 0.7014\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0511 - accuracy: 0.7007\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0467 - accuracy: 0.7017\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0428 - accuracy: 0.7027\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0391 - accuracy: 0.7049\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0422 - accuracy: 0.7022\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0359 - accuracy: 0.7056\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0348 - accuracy: 0.7027\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0370 - accuracy: 0.7030\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0304 - accuracy: 0.7053\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0292 - accuracy: 0.7051\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0269 - accuracy: 0.7051\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.0240 - accuracy: 0.7058\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0236 - accuracy: 0.7042\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0212 - accuracy: 0.7060\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0153 - accuracy: 0.7077\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0167 - accuracy: 0.7075\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0202 - accuracy: 0.7067\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0193 - accuracy: 0.7076\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0156 - accuracy: 0.7077\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0104 - accuracy: 0.7091\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0137 - accuracy: 0.7074\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 15s 16ms/step - loss: 5.5007 - accuracy: 0.0798\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 4.7745 - accuracy: 0.1584\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 3.3810 - accuracy: 0.3316\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 2.7865 - accuracy: 0.3977\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.4364 - accuracy: 0.4432\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.1879 - accuracy: 0.4844\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.9919 - accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.8418 - accuracy: 0.5445\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.7147 - accuracy: 0.5659\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6190 - accuracy: 0.5868\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.5362 - accuracy: 0.6035\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4723 - accuracy: 0.6156\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4160 - accuracy: 0.6281\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.3704 - accuracy: 0.6356\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3372 - accuracy: 0.6413\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.3047 - accuracy: 0.6486\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2741 - accuracy: 0.6549\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2483 - accuracy: 0.6599\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.2310 - accuracy: 0.6643\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.2128 - accuracy: 0.6674\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1939 - accuracy: 0.6704\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1774 - accuracy: 0.6746\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1678 - accuracy: 0.6780\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1577 - accuracy: 0.6777\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1383 - accuracy: 0.6826\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1317 - accuracy: 0.6827\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1213 - accuracy: 0.6857\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1154 - accuracy: 0.6872\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1092 - accuracy: 0.6878\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1020 - accuracy: 0.6904\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0935 - accuracy: 0.6918\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0892 - accuracy: 0.6908\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0806 - accuracy: 0.6931\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0754 - accuracy: 0.6945\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0724 - accuracy: 0.6955\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.0674 - accuracy: 0.6955\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.0593 - accuracy: 0.6973\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0582 - accuracy: 0.6986\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0568 - accuracy: 0.6982\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0520 - accuracy: 0.6972\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0464 - accuracy: 0.7023\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0406 - accuracy: 0.7012\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0370 - accuracy: 0.7021\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0351 - accuracy: 0.7025\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0332 - accuracy: 0.7034\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0269 - accuracy: 0.7053\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0281 - accuracy: 0.7037\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0256 - accuracy: 0.7043\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0190 - accuracy: 0.7063\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0170 - accuracy: 0.7090\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0188 - accuracy: 0.7052\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0141 - accuracy: 0.7079\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0127 - accuracy: 0.7070\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0125 - accuracy: 0.7050\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0085 - accuracy: 0.7077\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0022 - accuracy: 0.7091\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9971 - accuracy: 0.7107\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9988 - accuracy: 0.7098\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0040 - accuracy: 0.7078\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9959 - accuracy: 0.7107\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9969 - accuracy: 0.7104\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9890 - accuracy: 0.7124\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9903 - accuracy: 0.7100\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9888 - accuracy: 0.7100\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9873 - accuracy: 0.7105\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9867 - accuracy: 0.7119\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9874 - accuracy: 0.7108\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9852 - accuracy: 0.7113\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9801 - accuracy: 0.7107\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9781 - accuracy: 0.7120\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9779 - accuracy: 0.7121\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9730 - accuracy: 0.7124\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9741 - accuracy: 0.7123\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9740 - accuracy: 0.7154\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9715 - accuracy: 0.7137\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9727 - accuracy: 0.7141\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9672 - accuracy: 0.7157\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9692 - accuracy: 0.7131\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9699 - accuracy: 0.7148\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9683 - accuracy: 0.7145\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9660 - accuracy: 0.7142\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9629 - accuracy: 0.7160\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9616 - accuracy: 0.7150\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9602 - accuracy: 0.7159\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9600 - accuracy: 0.7152\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9577 - accuracy: 0.7150\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9579 - accuracy: 0.7148\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9532 - accuracy: 0.7173\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9565 - accuracy: 0.7165\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9568 - accuracy: 0.7181\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9553 - accuracy: 0.7164\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9556 - accuracy: 0.7154\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9554 - accuracy: 0.7153\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9554 - accuracy: 0.7172\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9535 - accuracy: 0.7174\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9501 - accuracy: 0.7168\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9479 - accuracy: 0.7175\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 0.9486 - accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9467 - accuracy: 0.7172\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9493 - accuracy: 0.7179\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 19s 21ms/step - loss: 5.5020 - accuracy: 0.0788\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 5.4420 - accuracy: 0.0802\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 5.4348 - accuracy: 0.0818\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 5.0461 - accuracy: 0.1139\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 3.9920 - accuracy: 0.2306\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 3.3160 - accuracy: 0.3172\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.8672 - accuracy: 0.3724\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 2.5489 - accuracy: 0.4177\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.3185 - accuracy: 0.4550\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.1446 - accuracy: 0.4822\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.0052 - accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.8900 - accuracy: 0.5264\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.7915 - accuracy: 0.5457\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.7136 - accuracy: 0.5603\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.6507 - accuracy: 0.5719\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5939 - accuracy: 0.5835\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.5444 - accuracy: 0.5949\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.5061 - accuracy: 0.6023\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.4602 - accuracy: 0.6094\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.4327 - accuracy: 0.6168\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.4087 - accuracy: 0.6207\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.3822 - accuracy: 0.6259\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3590 - accuracy: 0.6300\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3380 - accuracy: 0.6365\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3198 - accuracy: 0.6392\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3031 - accuracy: 0.6439\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2862 - accuracy: 0.6463\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2734 - accuracy: 0.6493\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2581 - accuracy: 0.6524\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.2511 - accuracy: 0.6539\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2363 - accuracy: 0.6567\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 17s 23ms/step - loss: 1.2285 - accuracy: 0.6569\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.2178 - accuracy: 0.6615\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.2114 - accuracy: 0.6628\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1974 - accuracy: 0.6671\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1950 - accuracy: 0.6654\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1858 - accuracy: 0.6690\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1816 - accuracy: 0.6684\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.1709 - accuracy: 0.6722\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1635 - accuracy: 0.6728\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1533 - accuracy: 0.6767\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1556 - accuracy: 0.6750\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1462 - accuracy: 0.6765\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1455 - accuracy: 0.6760\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1367 - accuracy: 0.6784\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.1271 - accuracy: 0.6809\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1245 - accuracy: 0.6799\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1202 - accuracy: 0.6806\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.1174 - accuracy: 0.6827\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1155 - accuracy: 0.6832\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1092 - accuracy: 0.6841\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1079 - accuracy: 0.6850\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1013 - accuracy: 0.6855\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0949 - accuracy: 0.6856\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0985 - accuracy: 0.6858\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0972 - accuracy: 0.6879\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0866 - accuracy: 0.6898\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0833 - accuracy: 0.6910\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0869 - accuracy: 0.6882\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0753 - accuracy: 0.6915\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0744 - accuracy: 0.6918\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0712 - accuracy: 0.6923\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0677 - accuracy: 0.6935\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0694 - accuracy: 0.6921\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0687 - accuracy: 0.6931\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0593 - accuracy: 0.6946\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0569 - accuracy: 0.6959\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0584 - accuracy: 0.6972\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0567 - accuracy: 0.6963\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0503 - accuracy: 0.6972\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0520 - accuracy: 0.6980\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0491 - accuracy: 0.6975\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0437 - accuracy: 0.6972\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0423 - accuracy: 0.6961\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0417 - accuracy: 0.6972\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0425 - accuracy: 0.6976\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0405 - accuracy: 0.6992\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0351 - accuracy: 0.6983\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0320 - accuracy: 0.7020\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0295 - accuracy: 0.7017\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0295 - accuracy: 0.7028\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0243 - accuracy: 0.7031\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0278 - accuracy: 0.7019\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0303 - accuracy: 0.6996\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0231 - accuracy: 0.6998\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0239 - accuracy: 0.7019\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0153 - accuracy: 0.7048\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0170 - accuracy: 0.7034\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0207 - accuracy: 0.7030\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0186 - accuracy: 0.7029\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0134 - accuracy: 0.7042\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0151 - accuracy: 0.7037\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0128 - accuracy: 0.7038\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0078 - accuracy: 0.7058\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.0096 - accuracy: 0.7053\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.0050 - accuracy: 0.7068\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0031 - accuracy: 0.7069\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0029 - accuracy: 0.7054\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0031 - accuracy: 0.7065\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0032 - accuracy: 0.7069\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0, 5):\n",
    "    history_layers = tune_model_layers(i)\n",
    "    results.append({\n",
    "            \"layers\": i,\n",
    "            \"loss\": min(history_layers.history[\"loss\"]),\n",
    "            \"accuracy\": max(history_layers.history[\"accuracy\"]),\n",
    "        })\n",
    "    \n",
    "results_layers = pd.DataFrame(results)\n",
    "sns.lineplot(x=\"layers\", y=\"loss\", data=results_layers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_LSTM(lstm_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=max_sequence_len - 1))\n",
    "    for i in range(1):\n",
    "        model.add(LSTM(lstm_size, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "        \n",
    "    model.add(LSTM(lstm_size, return_sequences=False))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X,\n",
    "        labels_encoded,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=10, min_delta=0.0001, restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(30,120,10):\n",
    "    history_layers = tune_model_LSTM(i)\n",
    "    results.append({\n",
    "            \"units\": i,\n",
    "            \"loss\": min(history_layers.history[\"loss\"]),\n",
    "            \"accuracy\": max(history_layers.history[\"accuracy\"]),\n",
    "        })\n",
    "results_units = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='units', y=\"loss\", data=results_units)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=max_sequence_len - 1))\n",
    "    for i in range(1):\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "        \n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = final_model()\n",
    "final_model.fit(\n",
    "    X,\n",
    "    labels_encoded,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10, min_delta=0.0001, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\"final_model.h5\", monitor=\"loss\", save_best_only=True, verbose=1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_N_words_unique(seed_texts, top_p=1, N_words=10):\n",
    "    generated_texts = []\n",
    "\n",
    "    for seed_text in seed_texts:\n",
    "        current_generated_text = seed_text\n",
    "        for i in range(N_words):\n",
    "            seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([seed_sequence], maxlen=max_sequence_len - 1)\n",
    "            predictions = final_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            cumulative_probs = np.cumsum(predictions[sorted_indices])\n",
    "            selected_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            selected_probs = predictions[selected_indices] / np.sum(\n",
    "                predictions[selected_indices]\n",
    "            )\n",
    "\n",
    "            next_index = np.random.choice(selected_indices, p=selected_probs)\n",
    "            next_word = tokenizer.index_word[next_index]\n",
    "\n",
    "            if (\n",
    "                next_word is None\n",
    "                or next_word == \"end_token\"\n",
    "                or len(current_generated_text.split()) >= N_words + len(seed_text)\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            current_generated_text += \" \" + next_word\n",
    "            seed_text += \" \" + next_word\n",
    "\n",
    "        generated_texts.append(current_generated_text)\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_texts = [\n",
    "    \"embrace each day\",\n",
    "    \"radiate some\",\n",
    "    \"believe that\",\n",
    "    \"life's actual purpose is\",\n",
    "    \"dance through each and every\",\n",
    "    \"let your time and energy\",\n",
    "    \"every person is\",\n",
    "    \"our country Singapore is\",\n",
    "    \"planet earth is\",\n",
    "    \"morning and evening would make it\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_texts = predict_next_N_words_unique(seed_texts)\n",
    "for text in predicted_texts:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
