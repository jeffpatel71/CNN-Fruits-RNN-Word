{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Quotes\n",
      "0  Embrace the beauty of every sunrise; it's a fr...\n",
      "1  Embrace challenges; they are the stepping ston...\n",
      "2  Embrace the rhythm of life and let it dance th...\n",
      "3  Embrace kindness, for it has the power to chan...\n",
      "4  Embrace the journey, for it leads to the desti...\n",
      "Data shape: (1000, 1)\n",
      "Missing values: Quotes    0\n",
      "dtype: int64\n",
      "                                                   Quotes\n",
      "count                                                1000\n",
      "unique                                                890\n",
      "top     Radiate acceptance, and find peace in embracin...\n",
      "freq                                                    5\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "print(data.head())\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = list(data['Quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embrace the beauty of every sunrise; it's a fresh chance to paint your world with joy.\n",
      "[[17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687]]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(tokenizer.texts_to_sequences([data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "padded_sequences = pad_sequences(sequences, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 869), ('of', 663), ('your', 350), ('and', 322), ('a', 307), ('is', 253), ('in', 249), ('for', 201), ('let', 187), ('to', 180)]\n",
      "There are 1198 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_freq = tokenizer.word_counts\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_word_freq[:10])\n",
    "\n",
    "num_unique_words = len(tokenizer.word_index)\n",
    "print(f\"There are {num_unique_words} unique words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 890 Quotes and 1198 Unique Words. In those 890 quotes we have 869 'the' and 663 'of' it's very likely that the model we build will often predict those 2 phrases very often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with 'the' and 'of'\n",
    "We can create new data, that is less biased towards the 'the' and 'of'.  \n",
    "Slice the quotes into different sizes and then train the model on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 2, 3 words\n",
    "phrases = []\n",
    "for quote in sequences:\n",
    "    for i in range(len(quote) - 1):\n",
    "        phrases.append(quote[i : i + 2])  # create 2-word phrases\n",
    "    for i in range(len(quote) - 2):\n",
    "        phrases.append(quote[i : i + 3])  # create 3-word phrases\n",
    "\n",
    "total_data = phrases + sequences\n",
    "\n",
    "# pad the sequences\n",
    "max_sequence_len = max([len(x) for x in total_data])\n",
    "padded_sequences = pad_sequences(total_data, maxlen=max_sequence_len, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and Y\n",
    "X = padded_sequences[:, :-1]\n",
    "labels = padded_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19458, 34)\n",
      "(19458, 1199)\n"
     ]
    }
   ],
   "source": [
    "labels_encoded = tf.keras.utils.to_categorical(labels, num_classes=num_unique_words + 1)\n",
    "print(X.shape)\n",
    "print(labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the inital model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "def first_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = first_model()\n",
    "history = model.fit(\n",
    "    X, labels_encoded, batch_size=128, epochs=100, verbose=1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(units, batch_size, number_layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 50, input_length=max_sequence_len - 1))\n",
    "    for i in range(number_layers - 1):\n",
    "        model.add(LSTM(units, return_sequences=True))\n",
    "        model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(tf.keras.layers.LayerNormalization())\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        labels_encoded,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=3, min_delta=0.0001, restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def grid_search(units, batch_size, number_layers):\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        for number_layer in number_layers:\n",
    "            for unit in units:\n",
    "                history = tune_128_model(unit, batch_size, number_layer)\n",
    "                # Store the results\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"dropout\": dropout,\n",
    "                        \"dense_size\": dense_size,\n",
    "                        \"loss\": history.history[\"loss\"],\n",
    "                        \"accuracy\": history.history[\"accuracy\"],\n",
    "                    }\n",
    "                )\n",
    "    return results\n",
    "\n",
    "\n",
    "units = [32, 64, 128]\n",
    "batch_sizes = [32, 64, 128]\n",
    "number_layers = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search(units, batch_sizes, number_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_N_words_unique(seed_texts, top_p=1, N_words=10):\n",
    "    generated_texts = []\n",
    "\n",
    "    for seed_text in seed_texts:\n",
    "        current_generated_text = seed_text\n",
    "        for _ in range(N_words):\n",
    "            # Tokenize the input sequence\n",
    "            seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            # Pad the sequence if needed\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                [seed_sequence], maxlen=max_sequence_len - 1\n",
    "            )\n",
    "            # Get the model's prediction for the next word\n",
    "            predictions = final_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "            # Apply top-p sampling\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            cumulative_probs = np.cumsum(predictions[sorted_indices])\n",
    "            selected_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            # Normalize probabilities\n",
    "            selected_probs = predictions[selected_indices] / np.sum(\n",
    "                predictions[selected_indices]\n",
    "            )\n",
    "\n",
    "            # Sample from the selected indices based on the normalized probabilities\n",
    "            next_index = np.random.choice(selected_indices, p=selected_probs)\n",
    "\n",
    "            # Convert the index back to a word\n",
    "            next_word = tokenizer.index_word[next_index]\n",
    "            # print(next_word)\n",
    "            # Break if the generated text is too long or if an end token is predicted\n",
    "            if (\n",
    "                next_word is None\n",
    "                or next_word == \"end_token\"\n",
    "                or len(current_generated_text.split()) >= N_words + len(seed_text)\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            # Update the generated text and seed_text for the next iteration\n",
    "            current_generated_text += \" \" + next_word\n",
    "            seed_text += \" \" + next_word\n",
    "\n",
    "        generated_texts.append(current_generated_text)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_texts = [\n",
    "    \"embrace each day\",\n",
    "    \"radiate some\",\n",
    "    \"believe that\",\n",
    "    \"life's actual purpose is\",\n",
    "    \"dance through each and every\",\n",
    "    \"let your time and energy\",\n",
    "    \"every person is\",\n",
    "    \"our country Singapore is\",\n",
    "    \"planet earth is\",\n",
    "    \"morning and evening would make it\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_texts = predict_next_N_words_unique(seed_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
