{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, GRU, SimpleRNN\n",
    "from keras.layers import Dropout, LayerNormalization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Quotes\n",
      "0  Embrace the beauty of every sunrise; it's a fr...\n",
      "1  Embrace challenges; they are the stepping ston...\n",
      "2  Embrace the rhythm of life and let it dance th...\n",
      "3  Embrace kindness, for it has the power to chan...\n",
      "4  Embrace the journey, for it leads to the desti...\n",
      "Data shape: (1000, 1)\n",
      "Missing values: Quotes    0\n",
      "dtype: int64\n",
      "                                                   Quotes\n",
      "count                                                1000\n",
      "unique                                                890\n",
      "top     Radiate acceptance, and find peace in embracin...\n",
      "freq                                                    5\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "print(data.head())\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data = list(data['Quotes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'your': 3,\n",
       " 'and': 4,\n",
       " 'a': 5,\n",
       " 'is': 6,\n",
       " 'in': 7,\n",
       " 'for': 8,\n",
       " 'let': 9,\n",
       " 'to': 10,\n",
       " 'it': 11,\n",
       " 'be': 12,\n",
       " 'every': 13,\n",
       " 'our': 14,\n",
       " 'you': 15,\n",
       " 'that': 16,\n",
       " 'embrace': 17,\n",
       " \"life's\": 18,\n",
       " 'this': 19,\n",
       " 'are': 20,\n",
       " 'morning': 21,\n",
       " 'with': 22,\n",
       " 'radiate': 23,\n",
       " 'dance': 24,\n",
       " 'heart': 25,\n",
       " 'believe': 26,\n",
       " 'yourself': 27,\n",
       " 'through': 28,\n",
       " \"planet's\": 29,\n",
       " 'will': 30,\n",
       " 'life': 31,\n",
       " 'love': 32,\n",
       " 'they': 33,\n",
       " \"singapore's\": 34,\n",
       " 'kindness': 35,\n",
       " 'power': 36,\n",
       " 'from': 37,\n",
       " 'dreams': 38,\n",
       " 'we': 39,\n",
       " 'soul': 40,\n",
       " 'symphony': 41,\n",
       " 'act': 42,\n",
       " 'find': 43,\n",
       " 'gratitude': 44,\n",
       " 'singapore': 45,\n",
       " 'world': 46,\n",
       " 'strength': 47,\n",
       " 'light': 48,\n",
       " 'beauty': 49,\n",
       " 'journey': 50,\n",
       " 'nature': 51,\n",
       " 'joy': 52,\n",
       " 'planet': 53,\n",
       " 'canvas': 54,\n",
       " 'colors': 55,\n",
       " 'way': 56,\n",
       " 'whispers': 57,\n",
       " 'where': 58,\n",
       " 'potential': 59,\n",
       " 'hope': 60,\n",
       " 'testament': 61,\n",
       " 'resilience': 62,\n",
       " 'towards': 63,\n",
       " 'true': 64,\n",
       " 'new': 65,\n",
       " 'compassion': 66,\n",
       " 'beacon': 67,\n",
       " 'actions': 68,\n",
       " 'future': 69,\n",
       " 'spirit': 70,\n",
       " 'step': 71,\n",
       " 'change': 72,\n",
       " 'wisdom': 73,\n",
       " 'moments': 74,\n",
       " 'promise': 75,\n",
       " 'garden': 76,\n",
       " 'sunrise': 77,\n",
       " 'laughter': 78,\n",
       " 'story': 79,\n",
       " 'hearts': 80,\n",
       " 'day': 81,\n",
       " 'hold': 82,\n",
       " 'an': 83,\n",
       " 'tapestry': 84,\n",
       " 'heartbeat': 85,\n",
       " 'compass': 86,\n",
       " 'gift': 87,\n",
       " 'force': 88,\n",
       " 'inner': 89,\n",
       " 'each': 90,\n",
       " 'on': 91,\n",
       " 'holds': 92,\n",
       " 'destiny': 93,\n",
       " 'into': 94,\n",
       " 'self': 95,\n",
       " 'legacy': 96,\n",
       " 'when': 97,\n",
       " 'lion': 98,\n",
       " 'city': 99,\n",
       " 'skyline': 100,\n",
       " 'paint': 101,\n",
       " 'challenge': 102,\n",
       " 'determination': 103,\n",
       " 'tranquility': 104,\n",
       " 'others': 105,\n",
       " 'moment': 106,\n",
       " 'forgiveness': 107,\n",
       " 'melody': 108,\n",
       " 'any': 109,\n",
       " 'opportunities': 110,\n",
       " 'seeds': 111,\n",
       " 'within': 112,\n",
       " 'chapters': 113,\n",
       " 'growth': 114,\n",
       " 'purpose': 115,\n",
       " 'authenticity': 116,\n",
       " 'peace': 117,\n",
       " 'brings': 118,\n",
       " 'rhythm': 119,\n",
       " 'universe': 120,\n",
       " 'can': 121,\n",
       " 'reality': 122,\n",
       " 'overcome': 123,\n",
       " 'empathy': 124,\n",
       " 'serenity': 125,\n",
       " 'leave': 126,\n",
       " 'vibrant': 127,\n",
       " 'aspirations': 128,\n",
       " 'breath': 129,\n",
       " 'watch': 130,\n",
       " 'blessings': 131,\n",
       " 'rain': 132,\n",
       " 'music': 133,\n",
       " 'beginnings': 134,\n",
       " 'treasure': 135,\n",
       " 'have': 136,\n",
       " 'shape': 137,\n",
       " 'possibility': 138,\n",
       " 'positivity': 139,\n",
       " 'create': 140,\n",
       " 'carries': 141,\n",
       " 'bridge': 142,\n",
       " 'path': 143,\n",
       " 'nurtured': 144,\n",
       " 'us': 145,\n",
       " 'open': 146,\n",
       " 'reminder': 147,\n",
       " 'challenges': 148,\n",
       " 'has': 149,\n",
       " 'joyful': 150,\n",
       " 'more': 151,\n",
       " 'transform': 152,\n",
       " 'brighter': 153,\n",
       " 'foundation': 154,\n",
       " 'confidence': 155,\n",
       " 'guides': 156,\n",
       " 'guiding': 157,\n",
       " 'progress': 158,\n",
       " 'painted': 159,\n",
       " 'threads': 160,\n",
       " 'opportunity': 161,\n",
       " 'conservation': 162,\n",
       " 'fresh': 163,\n",
       " 'key': 164,\n",
       " 'success': 165,\n",
       " 'carry': 166,\n",
       " 'smile': 167,\n",
       " 'up': 168,\n",
       " 'its': 169,\n",
       " 'connection': 170,\n",
       " 'diversity': 171,\n",
       " 'touch': 172,\n",
       " 'masterpiece': 173,\n",
       " 'possibilities': 174,\n",
       " 'around': 175,\n",
       " 'sanctuary': 176,\n",
       " 'boundless': 177,\n",
       " 'star': 178,\n",
       " 'take': 179,\n",
       " 'shine': 180,\n",
       " 'by': 181,\n",
       " 'flourishes': 182,\n",
       " \"park's\": 183,\n",
       " 'hear': 184,\n",
       " 'stillness': 185,\n",
       " 'than': 186,\n",
       " 'experience': 187,\n",
       " 'wind': 188,\n",
       " 'lead': 189,\n",
       " 'magic': 190,\n",
       " 'energy': 191,\n",
       " 'being': 192,\n",
       " 'passions': 193,\n",
       " 'truest': 194,\n",
       " 'presence': 195,\n",
       " 'behind': 196,\n",
       " 'courage': 197,\n",
       " 'essence': 198,\n",
       " 'ignites': 199,\n",
       " 'grace': 200,\n",
       " 'echoes': 201,\n",
       " 'forward': 202,\n",
       " 'woven': 203,\n",
       " \"someone's\": 204,\n",
       " 'generosity': 205,\n",
       " 'innovation': 206,\n",
       " 'sunset': 207,\n",
       " 'pulau': 208,\n",
       " 'paints': 209,\n",
       " \"reserve's\": 210,\n",
       " 'present': 211,\n",
       " 'how': 212,\n",
       " 'language': 213,\n",
       " 'even': 214,\n",
       " 'darkest': 215,\n",
       " 'days': 216,\n",
       " 'unfold': 217,\n",
       " 'song': 218,\n",
       " 'turns': 219,\n",
       " 'adventure': 220,\n",
       " 'stories': 221,\n",
       " 'word': 222,\n",
       " 'transformation': 223,\n",
       " 'souls': 224,\n",
       " 'faith': 225,\n",
       " 'inspire': 226,\n",
       " 'abundance': 227,\n",
       " 'mark': 228,\n",
       " 'spark': 229,\n",
       " 'endless': 230,\n",
       " 'storms': 231,\n",
       " 'flight': 232,\n",
       " 'stronger': 233,\n",
       " 'dream': 234,\n",
       " 'source': 235,\n",
       " 'experiences': 236,\n",
       " 'made': 237,\n",
       " 'brightens': 238,\n",
       " 'connects': 239,\n",
       " 'tall': 240,\n",
       " 'reminds': 241,\n",
       " 'warmth': 242,\n",
       " 'dawn': 243,\n",
       " 'biodiversity': 244,\n",
       " 'protect': 245,\n",
       " 'offers': 246,\n",
       " 'leads': 247,\n",
       " 'words': 248,\n",
       " 'simplicity': 249,\n",
       " 'secrets': 250,\n",
       " 'positive': 251,\n",
       " 'well': 252,\n",
       " 'stars': 253,\n",
       " 'intuition': 254,\n",
       " 'humanity': 255,\n",
       " 'driving': 256,\n",
       " 'inspiration': 257,\n",
       " 'discovery': 258,\n",
       " 'care': 259,\n",
       " 'expression': 260,\n",
       " 'armor': 261,\n",
       " 'curiosity': 262,\n",
       " 'enthusiasm': 263,\n",
       " 'echo': 264,\n",
       " 'capable': 265,\n",
       " 'obstacle': 266,\n",
       " 'worthy': 267,\n",
       " 'discover': 268,\n",
       " 'composed': 269,\n",
       " 'blooms': 270,\n",
       " 'strokes': 271,\n",
       " 'mosaic': 272,\n",
       " 'classroom': 273,\n",
       " 'learn': 274,\n",
       " 'book': 275,\n",
       " 'written': 276,\n",
       " 'knows': 277,\n",
       " 'happiness': 278,\n",
       " 'creates': 279,\n",
       " 'faced': 280,\n",
       " 'creating': 281,\n",
       " 'start': 282,\n",
       " 'effort': 283,\n",
       " 'friendship': 284,\n",
       " 'formed': 285,\n",
       " 'choice': 286,\n",
       " 'brushstroke': 287,\n",
       " 'thought': 288,\n",
       " 'liberation': 289,\n",
       " 'investment': 290,\n",
       " 'tribute': 291,\n",
       " 'marvel': 292,\n",
       " 'gentle': 293,\n",
       " 'diverse': 294,\n",
       " 'bukit': 295,\n",
       " 'unity': 296,\n",
       " 'coastal': 297,\n",
       " \"nature's\": 298,\n",
       " 'time': 299,\n",
       " 'truly': 300,\n",
       " 'unlocking': 301,\n",
       " 'free': 302,\n",
       " 'ever': 303,\n",
       " 'beautiful': 304,\n",
       " 'genuine': 305,\n",
       " 'guide': 306,\n",
       " 'what': 307,\n",
       " 'lived': 308,\n",
       " 'pages': 309,\n",
       " 'heal': 310,\n",
       " 'flow': 311,\n",
       " 'learning': 312,\n",
       " 'seasons': 313,\n",
       " 'reflection': 314,\n",
       " 'creativity': 315,\n",
       " 'give': 316,\n",
       " 'bring': 317,\n",
       " 'lives': 318,\n",
       " 'solace': 319,\n",
       " 'voice': 320,\n",
       " 'precious': 321,\n",
       " 'adventures': 322,\n",
       " 'humility': 323,\n",
       " 'fuel': 324,\n",
       " 'become': 325,\n",
       " 'propels': 326,\n",
       " 'character': 327,\n",
       " 'greatness': 328,\n",
       " 'giving': 329,\n",
       " 'treasures': 330,\n",
       " 'good': 331,\n",
       " 'lights': 332,\n",
       " 'miracles': 333,\n",
       " 'make': 334,\n",
       " 'created': 335,\n",
       " 'celebration': 336,\n",
       " 'reveal': 337,\n",
       " 'tended': 338,\n",
       " 'cleanses': 339,\n",
       " 'resonates': 340,\n",
       " 'go': 341,\n",
       " 'understanding': 342,\n",
       " 'no': 343,\n",
       " 'wings': 344,\n",
       " 'waves': 345,\n",
       " 'wonder': 346,\n",
       " 'beneath': 347,\n",
       " 'goodness': 348,\n",
       " 'setup': 349,\n",
       " 'or': 350,\n",
       " 'person': 351,\n",
       " 'encouragement': 352,\n",
       " 'memories': 353,\n",
       " 'chapter': 354,\n",
       " 'encounter': 355,\n",
       " 'gem': 356,\n",
       " 'treasury': 357,\n",
       " 'harmony': 358,\n",
       " 'charm': 359,\n",
       " 'stands': 360,\n",
       " 'nation': 361,\n",
       " \"ubin's\": 362,\n",
       " 'vision': 363,\n",
       " 'heritage': 364,\n",
       " 'labrador': 365,\n",
       " 'vitality': 366,\n",
       " 'breathtaking': 367,\n",
       " 'lungs': 368,\n",
       " 'leaves': 369,\n",
       " 'arms': 370,\n",
       " 'welcome': 371,\n",
       " \"it's\": 372,\n",
       " 'stepping': 373,\n",
       " 'greatest': 374,\n",
       " 'one': 375,\n",
       " 'fears': 376,\n",
       " 'nourishes': 377,\n",
       " 'sets': 378,\n",
       " 'past': 379,\n",
       " 'louder': 380,\n",
       " 'own': 381,\n",
       " 'imagination': 382,\n",
       " 'enough': 383,\n",
       " 'keys': 384,\n",
       " 'warm': 385,\n",
       " 'wounds': 386,\n",
       " 'brilliance': 387,\n",
       " 'community': 388,\n",
       " 'grateful': 389,\n",
       " 'night': 390,\n",
       " 'mirror': 391,\n",
       " 'tomorrow': 392,\n",
       " 'wounded': 393,\n",
       " 'resides': 394,\n",
       " 'harmonious': 395,\n",
       " 'most': 396,\n",
       " 'illuminates': 397,\n",
       " 'those': 398,\n",
       " 'shields': 399,\n",
       " 'bitterness': 400,\n",
       " 'embracing': 401,\n",
       " 'haven': 402,\n",
       " 'like': 403,\n",
       " 'heals': 404,\n",
       " 'gifts': 405,\n",
       " 'fragrance': 406,\n",
       " 'chorus': 407,\n",
       " 'lighthouse': 408,\n",
       " 'follow': 409,\n",
       " 'soar': 410,\n",
       " 'break': 411,\n",
       " 'many': 412,\n",
       " 'extraordinary': 413,\n",
       " 'them': 414,\n",
       " 'passion': 415,\n",
       " 'share': 416,\n",
       " 'pursuit': 417,\n",
       " \"heart's\": 418,\n",
       " 'desires': 419,\n",
       " 'existence': 420,\n",
       " 'unique': 421,\n",
       " 'hues': 422,\n",
       " 'acts': 423,\n",
       " 'reminders': 424,\n",
       " 'gardens': 425,\n",
       " 'cherish': 426,\n",
       " 'full': 427,\n",
       " 'leaving': 428,\n",
       " 'write': 429,\n",
       " 'intention': 430,\n",
       " 'letting': 431,\n",
       " 'their': 432,\n",
       " 'sea': 433,\n",
       " 'ripple': 434,\n",
       " 'home': 435,\n",
       " 'fuels': 436,\n",
       " 'soothes': 437,\n",
       " 'wonders': 438,\n",
       " 'embodiment': 439,\n",
       " 'setback': 440,\n",
       " 'comeback': 441,\n",
       " 'bloom': 442,\n",
       " 'taken': 443,\n",
       " 'whether': 444,\n",
       " 'painful': 445,\n",
       " 'transitions': 446,\n",
       " 'directed': 447,\n",
       " 'indomitable': 448,\n",
       " 'wave': 449,\n",
       " 'witnessing': 450,\n",
       " 'cherished': 451,\n",
       " 'bay': 452,\n",
       " 'ambition': 453,\n",
       " 'modernity': 454,\n",
       " 'perfect': 455,\n",
       " \"chinatown's\": 456,\n",
       " 'pulse': 457,\n",
       " 'waiting': 458,\n",
       " \"gardens'\": 459,\n",
       " 'history': 460,\n",
       " 'sky': 461,\n",
       " 'quiet': 462,\n",
       " \"nation's\": 463,\n",
       " 'jurong': 464,\n",
       " 'wildlife': 465,\n",
       " 'sungei': 466,\n",
       " 'buloh': 467,\n",
       " 'wetland': 468,\n",
       " 'landscapes': 469,\n",
       " 'rugged': 470,\n",
       " 'witness': 471,\n",
       " 'face': 472,\n",
       " 'ancient': 473,\n",
       " 'survival': 474,\n",
       " 'species': 475,\n",
       " 'reefs': 476,\n",
       " 'cradle': 477,\n",
       " 'artistry': 478,\n",
       " 'rustle': 479,\n",
       " 'springs': 480,\n",
       " \"day's\": 481,\n",
       " 'fills': 482,\n",
       " 'air': 483,\n",
       " 'clarity': 484,\n",
       " 'fill': 485,\n",
       " 'chance': 486,\n",
       " 'stones': 487,\n",
       " 'at': 488,\n",
       " 'uniqueness': 489,\n",
       " 'only': 490,\n",
       " 'multiplies': 491,\n",
       " 'chains': 492,\n",
       " 'could': 493,\n",
       " 'small': 494,\n",
       " 'blueprints': 495,\n",
       " 'cornerstone': 496,\n",
       " 'gateway': 497,\n",
       " 'chest': 498,\n",
       " 'patience': 499,\n",
       " 'far': 500,\n",
       " 'birthplace': 501,\n",
       " 'thoughts': 502,\n",
       " 'nights': 503,\n",
       " 'mend': 504,\n",
       " 'everyday': 505,\n",
       " 'alchemy': 506,\n",
       " 'conductor': 507,\n",
       " 'cosmos': 508,\n",
       " 'kind': 509,\n",
       " 'remind': 510,\n",
       " 'emotions': 511,\n",
       " 'ordinary': 512,\n",
       " 'mind': 513,\n",
       " 'cycles': 514,\n",
       " 'weave': 515,\n",
       " 'cleanse': 516,\n",
       " 'fulfilled': 517,\n",
       " 'burdens': 518,\n",
       " 'healing': 519,\n",
       " 'bright': 520,\n",
       " 'acceptance': 521,\n",
       " 'powerful': 522,\n",
       " \"you'll\": 523,\n",
       " 'contagious': 524,\n",
       " 'set': 525,\n",
       " 'spreads': 526,\n",
       " 'wildfire': 527,\n",
       " 'sails': 528,\n",
       " 'fortress': 529,\n",
       " 'balm': 530,\n",
       " 'old': 531,\n",
       " 'sparks': 532,\n",
       " 'transcends': 533,\n",
       " 'all': 534,\n",
       " 'barriers': 535,\n",
       " 'drives': 536,\n",
       " 'ages': 537,\n",
       " 'powers': 538,\n",
       " 'conquer': 539,\n",
       " 'achieve': 540,\n",
       " 'difference': 541,\n",
       " 'adversity': 542,\n",
       " 'things': 543,\n",
       " 'agent': 544,\n",
       " 'anything': 545,\n",
       " 'melodies': 546,\n",
       " 'eyes': 547,\n",
       " 'deepest': 548,\n",
       " 'intentions': 549,\n",
       " 'away': 550,\n",
       " 'choices': 551,\n",
       " 'spread': 552,\n",
       " 'receive': 553,\n",
       " 'return': 554,\n",
       " 'makes': 555,\n",
       " 'grow': 556,\n",
       " 'plays': 557,\n",
       " 'watered': 558,\n",
       " 'renews': 559,\n",
       " 'sense': 560,\n",
       " 'forth': 561,\n",
       " 'differences': 562,\n",
       " 'resonate': 563,\n",
       " 'sowing': 564,\n",
       " 'lies': 565,\n",
       " 'freedom': 566,\n",
       " 'painting': 567,\n",
       " 'together': 568,\n",
       " 'truth': 569,\n",
       " 'rich': 570,\n",
       " 'waters': 571,\n",
       " 'transforms': 572,\n",
       " 'speak': 573,\n",
       " 'north': 574,\n",
       " 'which': 575,\n",
       " 'stand': 576,\n",
       " 'fire': 577,\n",
       " 'reflects': 578,\n",
       " 'infectious': 579,\n",
       " 'spreading': 580,\n",
       " 'sword': 581,\n",
       " 'cuts': 582,\n",
       " 'connections': 583,\n",
       " 'decision': 584,\n",
       " 'wellspring': 585,\n",
       " 'seas': 586,\n",
       " 'ripples': 587,\n",
       " 'preciousness': 588,\n",
       " 'uplift': 589,\n",
       " 'shapes': 590,\n",
       " 'between': 591,\n",
       " 'meet': 592,\n",
       " 'deserves': 593,\n",
       " 'becoming': 594,\n",
       " 'blossoming': 595,\n",
       " 'alter': 596,\n",
       " 'course': 597,\n",
       " 'reverberates': 598,\n",
       " 'stroke': 599,\n",
       " 'brush': 600,\n",
       " 'tale': 601,\n",
       " 'state': 602,\n",
       " 'contentment': 603,\n",
       " 'reside': 604,\n",
       " 'wide': 605,\n",
       " 'comes': 606,\n",
       " 'cultures': 607,\n",
       " 'bounds': 608,\n",
       " 'marina': 609,\n",
       " 'orchard': 610,\n",
       " \"sentosa's\": 611,\n",
       " 'corner': 612,\n",
       " 'merlion': 613,\n",
       " 'symbol': 614,\n",
       " 'timah': 615,\n",
       " 'reaches': 616,\n",
       " 'defines': 617,\n",
       " 'intertwine': 618,\n",
       " 'tells': 619,\n",
       " 'alive': 620,\n",
       " 'tales': 621,\n",
       " 'southern': 622,\n",
       " 'bird': 623,\n",
       " \"safari's\": 624,\n",
       " 'mysteries': 625,\n",
       " 'endeavor': 626,\n",
       " 'excellence': 627,\n",
       " 'changi': 628,\n",
       " 'vibrancy': 629,\n",
       " 'macritchie': 630,\n",
       " \"reservoir's\": 631,\n",
       " 'picture': 632,\n",
       " 'reaching': 633,\n",
       " 'diligence': 634,\n",
       " 'views': 635,\n",
       " 'embodies': 636,\n",
       " 'brand': 637,\n",
       " 'lake': 638,\n",
       " 'destinies': 639,\n",
       " 'chek': 640,\n",
       " \"jawa's\": 641,\n",
       " 'unwavering': 642,\n",
       " 'batok': 643,\n",
       " 'marine': 644,\n",
       " 'trove': 645,\n",
       " 'flourish': 646,\n",
       " 'stewardship': 647,\n",
       " 'forests': 648,\n",
       " 'breathe': 649,\n",
       " 'rivers': 650,\n",
       " 'skies': 651,\n",
       " 'cities': 652,\n",
       " 'footprint': 653,\n",
       " 'wetlands': 654,\n",
       " 'see': 655,\n",
       " 'drop': 656,\n",
       " \"earth's\": 657,\n",
       " 'savannas': 658,\n",
       " 'brushstrokes': 659,\n",
       " 'fiery': 660,\n",
       " 'pledge': 661,\n",
       " 'silent': 662,\n",
       " 'grass': 663,\n",
       " 'caves': 664,\n",
       " 'endurance': 665,\n",
       " 'renewal': 666,\n",
       " 'delicate': 667,\n",
       " 'ecosystems': 668,\n",
       " 'nurseries': 669,\n",
       " 'sand': 670,\n",
       " 'inhale': 671,\n",
       " 'veins': 672,\n",
       " 'land': 673,\n",
       " 'rising': 674,\n",
       " 'sun': 675,\n",
       " 'today': 676,\n",
       " 'presents': 677,\n",
       " 'reflect': 678,\n",
       " 'chambers': 679,\n",
       " 'soundtrack': 680,\n",
       " 'simple': 681,\n",
       " 'joys': 682,\n",
       " 'navigate': 683,\n",
       " 'illuminate': 684,\n",
       " 'fully': 685,\n",
       " 'reminding': 686,\n",
       " 'victories': 687,\n",
       " 'destination': 688,\n",
       " 'fingerprint': 689,\n",
       " 'exists': 690,\n",
       " 'silence': 691,\n",
       " 'speaks': 692,\n",
       " 'often': 693,\n",
       " 'significance': 694,\n",
       " 'constant': 695,\n",
       " 'chaos': 696,\n",
       " 'unknown': 697,\n",
       " 'midst': 698,\n",
       " 'peaceful': 699,\n",
       " 'elders': 700,\n",
       " 'allows': 701,\n",
       " 'sing': 702,\n",
       " 'turn': 703,\n",
       " 'off': 704,\n",
       " 'lands': 705,\n",
       " 'vulnerability': 706,\n",
       " 'ability': 707,\n",
       " 'itself': 708,\n",
       " 'without': 709,\n",
       " 'lullaby': 710,\n",
       " 'hug': 711,\n",
       " 'shadows': 712,\n",
       " 'expanding': 713,\n",
       " 'moon': 714,\n",
       " 'loving': 715,\n",
       " 'cannot': 716,\n",
       " 'affirmations': 717,\n",
       " 'solitude': 718,\n",
       " 'another': 719,\n",
       " 'raindrops': 720,\n",
       " 'nurture': 721,\n",
       " 'restore': 722,\n",
       " 'portrait': 723,\n",
       " 'radiates': 724,\n",
       " 'nurtures': 725,\n",
       " 'body': 726,\n",
       " 'deed': 727,\n",
       " 'restless': 728,\n",
       " 'balance': 729,\n",
       " 'gloomiest': 730,\n",
       " 'trust': 731,\n",
       " 'relationships': 732,\n",
       " 'minds': 733,\n",
       " 'attract': 734,\n",
       " 'great': 735,\n",
       " 'weight': 736,\n",
       " 'grudges': 737,\n",
       " 'keeps': 738,\n",
       " 'signature': 739,\n",
       " 'simplest': 740,\n",
       " 'anchor': 741,\n",
       " 'exploration': 742,\n",
       " 'smallest': 743,\n",
       " 'refuge': 744,\n",
       " 'weary': 745,\n",
       " 'bedrock': 746,\n",
       " 'generations': 747,\n",
       " 'unlocks': 748,\n",
       " 'mends': 749,\n",
       " 'unstoppable': 750,\n",
       " 'happen': 751,\n",
       " 'impossible': 752,\n",
       " 'think': 753,\n",
       " 'just': 754,\n",
       " 'as': 755,\n",
       " 'becomes': 756,\n",
       " 'conspire': 757,\n",
       " 'favor': 758,\n",
       " 'making': 759,\n",
       " 'footprints': 760,\n",
       " 'barrier': 761,\n",
       " 'forge': 762,\n",
       " 'limitless': 763,\n",
       " 'too': 764,\n",
       " 'unlock': 765,\n",
       " 'doors': 766,\n",
       " 'destined': 767,\n",
       " 'persevere': 768,\n",
       " 'darkness': 769,\n",
       " 'seek': 770,\n",
       " 'work': 771,\n",
       " 'art': 772,\n",
       " 'amazing': 773,\n",
       " 'limitation': 774,\n",
       " 'achieving': 775,\n",
       " 'architect': 776,\n",
       " 'brightest': 777,\n",
       " 'followed': 778,\n",
       " 'rainbows': 779,\n",
       " 'plant': 780,\n",
       " 'move': 781,\n",
       " 'points': 782,\n",
       " 'filled': 783,\n",
       " 'blend': 784,\n",
       " 'illuminated': 785,\n",
       " 'reflected': 786,\n",
       " 'multiply': 787,\n",
       " 'pieces': 788,\n",
       " 'unconditionally': 789,\n",
       " 'ink': 790,\n",
       " 'deeds': 791,\n",
       " 'calling': 792,\n",
       " 'disguise': 793,\n",
       " 'washes': 794,\n",
       " 'define': 795,\n",
       " 'forgive': 796,\n",
       " 'pen': 797,\n",
       " 'nudges': 798,\n",
       " 'triumphs': 799,\n",
       " 'choose': 800,\n",
       " 'found': 801,\n",
       " 'meaning': 802,\n",
       " 'ourselves': 803,\n",
       " 'guidance': 804,\n",
       " \"we've\": 805,\n",
       " 'traveled': 806,\n",
       " 'hopes': 807,\n",
       " 'offer': 808,\n",
       " 'pass': 809,\n",
       " 'uncertainty': 810,\n",
       " 'prelude': 811,\n",
       " 'savoring': 812,\n",
       " 'other': 813,\n",
       " 'side': 814,\n",
       " 'sweetest': 815,\n",
       " 'twist': 816,\n",
       " 'brightness': 817,\n",
       " 'notes': 818,\n",
       " 'tending': 819,\n",
       " 'steps': 820,\n",
       " 'limits': 821,\n",
       " 'lift': 822,\n",
       " 'weaves': 823,\n",
       " 'cultivating': 824,\n",
       " 'renew': 825,\n",
       " 'fruits': 826,\n",
       " 'pillars': 827,\n",
       " 'unveil': 828,\n",
       " 'weaving': 829,\n",
       " 'heights': 830,\n",
       " 'halls': 831,\n",
       " 'so': 832,\n",
       " 'brightly': 833,\n",
       " 'anthem': 834,\n",
       " 'against': 835,\n",
       " 'rock': 836,\n",
       " 'wildest': 837,\n",
       " 'binds': 838,\n",
       " 'wherever': 839,\n",
       " 'uplifts': 840,\n",
       " 'fear': 841,\n",
       " 'action': 842,\n",
       " 'stormy': 843,\n",
       " 'fosters': 844,\n",
       " 'achievements': 845,\n",
       " 'currency': 846,\n",
       " 'interactions': 847,\n",
       " 'unites': 848,\n",
       " 'principles': 849,\n",
       " 'engine': 850,\n",
       " 'contagion': 851,\n",
       " 'doubt': 852,\n",
       " 'salve': 853,\n",
       " 'turbulent': 854,\n",
       " 'protects': 855,\n",
       " 'values': 856,\n",
       " 'victory': 857,\n",
       " 'shared': 858,\n",
       " 'beam': 859,\n",
       " 'frees': 860,\n",
       " 'spoken': 861,\n",
       " 'counts': 862,\n",
       " 'thread': 863,\n",
       " 'human': 864,\n",
       " 'bad': 865,\n",
       " 'lesson': 866,\n",
       " 'embraced': 867,\n",
       " 'contented': 868,\n",
       " 'shown': 869,\n",
       " 'forged': 870,\n",
       " 'miracle': 871,\n",
       " 'fate': 872,\n",
       " 'mold': 873,\n",
       " 'glorious': 874,\n",
       " 'narrative': 875,\n",
       " 'farewells': 876,\n",
       " 'enlightenment': 877,\n",
       " 'out': 878,\n",
       " 'influence': 879,\n",
       " 'triumphant': 880,\n",
       " 'determined': 881,\n",
       " 'joyous': 882,\n",
       " 'worth': 883,\n",
       " 'telling': 884,\n",
       " 'ebb': 885,\n",
       " 'goodbyes': 886,\n",
       " 'transcending': 887,\n",
       " 'fostering': 888,\n",
       " 'serene': 889,\n",
       " 'blooming': 890,\n",
       " 'significant': 891,\n",
       " 'part': 892,\n",
       " 'turning': 893,\n",
       " 'point': 894,\n",
       " 'defy': 895,\n",
       " 'logic': 896,\n",
       " 'evolving': 897,\n",
       " 'wiser': 898,\n",
       " 'resilient': 899,\n",
       " 'version': 900,\n",
       " 'release': 901,\n",
       " 'resentment': 902,\n",
       " 'providing': 903,\n",
       " 'assurance': 904,\n",
       " 'not': 905,\n",
       " 'alone': 906,\n",
       " 'realization': 907,\n",
       " 'fulfillment': 908,\n",
       " 'richness': 909,\n",
       " 'meaningful': 910,\n",
       " 'connect': 911,\n",
       " 'affection': 912,\n",
       " 'deliberate': 913,\n",
       " 'shaping': 914,\n",
       " 'living': 915,\n",
       " 'proof': 916,\n",
       " 'conquering': 917,\n",
       " 'converge': 918,\n",
       " 'road': 919,\n",
       " 'roars': 920,\n",
       " 'embraces': 921,\n",
       " 'traditions': 922,\n",
       " 'reach': 923,\n",
       " 'beaches': 924,\n",
       " 'alleys': 925,\n",
       " 'southeast': 926,\n",
       " 'asia': 927,\n",
       " 'beats': 928,\n",
       " 'strong': 929,\n",
       " 'raffles': 930,\n",
       " 'place': 931,\n",
       " 'clarke': 932,\n",
       " 'quay': 933,\n",
       " 'told': 934,\n",
       " 'hawker': 935,\n",
       " 'centers': 936,\n",
       " 'fine': 937,\n",
       " 'dining': 938,\n",
       " 'flavors': 939,\n",
       " 'delectable': 940,\n",
       " 'ideas': 941,\n",
       " 'little': 942,\n",
       " \"india's\": 943,\n",
       " 'botanic': 944,\n",
       " 'glistens': 945,\n",
       " 'universal': 946,\n",
       " 'studios': 947,\n",
       " 'attractions': 948,\n",
       " 'class': 949,\n",
       " 'futures': 950,\n",
       " 'built': 951,\n",
       " 'park': 952,\n",
       " 'reserve': 953,\n",
       " 'heavens': 954,\n",
       " 'island': 955,\n",
       " \"road's\": 956,\n",
       " 'shopping': 957,\n",
       " 'shores': 958,\n",
       " 'trails': 959,\n",
       " 'leap': 960,\n",
       " 'sands': 961,\n",
       " 'flyer': 962,\n",
       " \"city's\": 963,\n",
       " 'landmarks': 964,\n",
       " 'lanterns': 965,\n",
       " 'kampong': 966,\n",
       " \"glam's\": 967,\n",
       " \"esplanade's\": 968,\n",
       " 'performances': 969,\n",
       " 'haw': 970,\n",
       " 'par': 971,\n",
       " \"villa's\": 972,\n",
       " 'culture': 973,\n",
       " 'thriving': 974,\n",
       " \"tekong's\": 975,\n",
       " \"islands'\": 976,\n",
       " 'testifies': 977,\n",
       " 'fort': 978,\n",
       " \"canning's\": 979,\n",
       " 'ruggedness': 980,\n",
       " 'dedication': 981,\n",
       " 'stone': 982,\n",
       " 'stretches': 983,\n",
       " 'horizon': 984,\n",
       " 'reign': 985,\n",
       " 'east': 986,\n",
       " 'coast': 987,\n",
       " 'breezes': 988,\n",
       " \"semakau's\": 989,\n",
       " 'talents': 990,\n",
       " 'honed': 991,\n",
       " 'peranakan': 992,\n",
       " 'houses': 993,\n",
       " 'colonial': 994,\n",
       " 'architecture': 995,\n",
       " 'preserved': 996,\n",
       " 'pride': 997,\n",
       " \"airport's\": 998,\n",
       " 'efficiency': 999,\n",
       " \"river's\": 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embrace the beauty of every sunrise; it's a fresh chance to paint your world with joy.\n",
      "[[17, 148, 33, 20, 1, 373, 487, 10, 3, 374, 687]]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(tokenizer.texts_to_sequences([data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "padded_sequences = pad_sequences(sequences, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 869), ('of', 663), ('your', 350), ('and', 322), ('a', 307), ('is', 253), ('in', 249), ('for', 201), ('let', 187), ('to', 180)]\n",
      "There are 1198 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_freq = tokenizer.word_counts\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_word_freq[:10])\n",
    "\n",
    "num_unique_words = len(tokenizer.word_index)\n",
    "print(f\"There are {num_unique_words} unique words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 890 Quotes and 1198 Unique Words. In those 890 quotes we have 869 'the' and 663 'of' it's very likely that the model we build will often predict those 2 phrases very often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with 'the' and 'of'\n",
    "We can create new data, that is less biased towards the 'the' and 'of'.  \n",
    "Slice the quotes into different sizes and then train the model on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 2, 3 words\n",
    "phrases = []\n",
    "for quote in sequences:\n",
    "    for phrase_length in range(2, 8):  \n",
    "        for i in range(len(quote) - phrase_length + 1):\n",
    "            phrases.append(quote[i : i + phrase_length])  \n",
    "\n",
    "total_data = phrases + sequences\n",
    "max_sequence_len = max([len(x) for x in total_data])\n",
    "padded_sequences = pad_sequences(total_data, maxlen=max_sequence_len, padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45914, 34)\n",
      "(45914, 1199)\n"
     ]
    }
   ],
   "source": [
    "x = padded_sequences[:, :-1]\n",
    "labels = padded_sequences[:, -1]\n",
    "labels_encoded = tf.keras.utils.to_categorical(labels, num_classes=num_unique_words + 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(labels_encoded.shape)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the inital model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "359/359 [==============================] - 12s 32ms/step - loss: 5.2299 - accuracy: 0.1283\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 3.9796 - accuracy: 0.2503\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 3.2911 - accuracy: 0.3289\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 11s 32ms/step - loss: 2.8366 - accuracy: 0.3888\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 2.5077 - accuracy: 0.4396\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 2.2621 - accuracy: 0.4844\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 2.0755 - accuracy: 0.5184\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.9337 - accuracy: 0.5412\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.8188 - accuracy: 0.5634\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 11s 32ms/step - loss: 1.7330 - accuracy: 0.5791\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.6573 - accuracy: 0.5958\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.6004 - accuracy: 0.6048\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.5403 - accuracy: 0.6162\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4887 - accuracy: 0.6275\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4515 - accuracy: 0.6327\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4202 - accuracy: 0.6432\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.4170 - accuracy: 0.6427\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.3810 - accuracy: 0.6468\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.3460 - accuracy: 0.6566\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.3287 - accuracy: 0.6593\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.3178 - accuracy: 0.6645\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2949 - accuracy: 0.6658\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2799 - accuracy: 0.6703\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.2661 - accuracy: 0.6719\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2538 - accuracy: 0.6751\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2474 - accuracy: 0.6750\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2337 - accuracy: 0.6773\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2209 - accuracy: 0.6836\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2307 - accuracy: 0.6790\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.2125 - accuracy: 0.6807\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 11s 29ms/step - loss: 1.2094 - accuracy: 0.6818\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1964 - accuracy: 0.6886\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1934 - accuracy: 0.6861\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1869 - accuracy: 0.6871\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1874 - accuracy: 0.6889\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 10s 29ms/step - loss: 1.1812 - accuracy: 0.6893\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1737 - accuracy: 0.6906\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1770 - accuracy: 0.6900\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1718 - accuracy: 0.6912\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1708 - accuracy: 0.6912\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1598 - accuracy: 0.6925\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 11s 31ms/step - loss: 1.1595 - accuracy: 0.6919\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1565 - accuracy: 0.6928\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1554 - accuracy: 0.6936\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1557 - accuracy: 0.6943\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 11s 29ms/step - loss: 1.1459 - accuracy: 0.6952\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1441 - accuracy: 0.6946\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1534 - accuracy: 0.6957\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1383 - accuracy: 0.6979\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1403 - accuracy: 0.6969\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1396 - accuracy: 0.6977\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 11s 30ms/step - loss: 1.1399 - accuracy: 0.6981\n",
      "Epoch 1/100\n",
      "359/359 [==============================] - 4s 8ms/step - loss: 5.3793 - accuracy: 0.0984\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 4.6542 - accuracy: 0.1671\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 4.1431 - accuracy: 0.2208\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.7981 - accuracy: 0.2617\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.4979 - accuracy: 0.2987\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.2244 - accuracy: 0.3367\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.9956 - accuracy: 0.3656\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.8050 - accuracy: 0.3882\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.6322 - accuracy: 0.4148\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.6345 - accuracy: 0.4202\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.4048 - accuracy: 0.4483\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.2914 - accuracy: 0.4672\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.1913 - accuracy: 0.4812\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.0973 - accuracy: 0.4991\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.0190 - accuracy: 0.5154\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.9494 - accuracy: 0.5240\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.8839 - accuracy: 0.5391\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.8305 - accuracy: 0.5472\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.7773 - accuracy: 0.5585\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.7287 - accuracy: 0.5689\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6833 - accuracy: 0.5769\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6455 - accuracy: 0.5841\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.6083 - accuracy: 0.5906\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5702 - accuracy: 0.5963\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5457 - accuracy: 0.6007\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5103 - accuracy: 0.6120\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4809 - accuracy: 0.6155\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4547 - accuracy: 0.6219\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4339 - accuracy: 0.6244\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4087 - accuracy: 0.6293\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3889 - accuracy: 0.6339\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3706 - accuracy: 0.6373\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3501 - accuracy: 0.6421\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3357 - accuracy: 0.6463\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3192 - accuracy: 0.6498\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3010 - accuracy: 0.6509\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2865 - accuracy: 0.6565\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2750 - accuracy: 0.6564\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2639 - accuracy: 0.6611\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2517 - accuracy: 0.6619\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2426 - accuracy: 0.6629\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2275 - accuracy: 0.6673\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2210 - accuracy: 0.6686\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2089 - accuracy: 0.6698\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1992 - accuracy: 0.6737\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1895 - accuracy: 0.6759\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1821 - accuracy: 0.6765\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1743 - accuracy: 0.6789\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1648 - accuracy: 0.6796\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1608 - accuracy: 0.6782\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1554 - accuracy: 0.6799\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1483 - accuracy: 0.6829\n",
      "Epoch 53/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1399 - accuracy: 0.6864\n",
      "Epoch 54/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1341 - accuracy: 0.6854\n",
      "Epoch 55/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1253 - accuracy: 0.6873\n",
      "Epoch 56/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1173 - accuracy: 0.6887\n",
      "Epoch 57/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1159 - accuracy: 0.6895\n",
      "Epoch 58/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1067 - accuracy: 0.6908\n",
      "Epoch 59/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1066 - accuracy: 0.6905\n",
      "Epoch 60/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1008 - accuracy: 0.6928\n",
      "Epoch 61/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0956 - accuracy: 0.6958\n",
      "Epoch 62/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0937 - accuracy: 0.6939\n",
      "Epoch 63/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0851 - accuracy: 0.6953\n",
      "Epoch 64/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0847 - accuracy: 0.6967\n",
      "Epoch 65/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0793 - accuracy: 0.6982\n",
      "Epoch 66/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0746 - accuracy: 0.6965\n",
      "Epoch 67/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0674 - accuracy: 0.6991\n",
      "Epoch 68/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0685 - accuracy: 0.6989\n",
      "Epoch 69/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0616 - accuracy: 0.6998\n",
      "Epoch 70/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0598 - accuracy: 0.7018\n",
      "Epoch 71/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0563 - accuracy: 0.7005\n",
      "Epoch 72/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0539 - accuracy: 0.7021\n",
      "Epoch 73/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0514 - accuracy: 0.7020\n",
      "Epoch 74/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0474 - accuracy: 0.7048\n",
      "Epoch 75/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0466 - accuracy: 0.7041\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0409 - accuracy: 0.7038\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0362 - accuracy: 0.7055\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0361 - accuracy: 0.7046\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0338 - accuracy: 0.7057\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0311 - accuracy: 0.7051\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0303 - accuracy: 0.7065\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0250 - accuracy: 0.7066\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0210 - accuracy: 0.7068\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0190 - accuracy: 0.7088\n",
      "Epoch 85/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0156 - accuracy: 0.7093\n",
      "Epoch 86/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0168 - accuracy: 0.7099\n",
      "Epoch 87/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0145 - accuracy: 0.7101\n",
      "Epoch 88/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0129 - accuracy: 0.7090\n",
      "Epoch 89/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0096 - accuracy: 0.7080\n",
      "Epoch 90/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0052 - accuracy: 0.7130\n",
      "Epoch 91/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0058 - accuracy: 0.7096\n",
      "Epoch 92/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0001 - accuracy: 0.7139\n",
      "Epoch 93/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0020 - accuracy: 0.7090\n",
      "Epoch 94/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0009 - accuracy: 0.7107\n",
      "Epoch 95/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9982 - accuracy: 0.7106\n",
      "Epoch 96/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 0.9947 - accuracy: 0.7129\n",
      "Epoch 97/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9951 - accuracy: 0.7127\n",
      "Epoch 98/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9941 - accuracy: 0.7104\n",
      "Epoch 99/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9917 - accuracy: 0.7134\n",
      "Epoch 100/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 0.9911 - accuracy: 0.7128\n",
      "Epoch 1/100\n",
      "359/359 [==============================] - 4s 7ms/step - loss: 5.2148 - accuracy: 0.1144\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 4.0232 - accuracy: 0.2356\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 3.2861 - accuracy: 0.3319\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.8097 - accuracy: 0.3946\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.4747 - accuracy: 0.4469\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 2.2298 - accuracy: 0.4856\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 2.0469 - accuracy: 0.5209\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.9012 - accuracy: 0.5496\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.7880 - accuracy: 0.5699\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.6948 - accuracy: 0.5892\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.6199 - accuracy: 0.6039\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5571 - accuracy: 0.6171\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.5040 - accuracy: 0.6256\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4569 - accuracy: 0.6341\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.4197 - accuracy: 0.6428\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3835 - accuracy: 0.6520\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3540 - accuracy: 0.6555\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.3270 - accuracy: 0.6611\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.3068 - accuracy: 0.6641\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2896 - accuracy: 0.6662\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2663 - accuracy: 0.6728\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2514 - accuracy: 0.6764\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.2363 - accuracy: 0.6779\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2249 - accuracy: 0.6803\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.2119 - accuracy: 0.6837\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1979 - accuracy: 0.6865\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1898 - accuracy: 0.6881\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1849 - accuracy: 0.6886\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1760 - accuracy: 0.6901\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1624 - accuracy: 0.6915\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1550 - accuracy: 0.6940\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1457 - accuracy: 0.6968\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1410 - accuracy: 0.6981\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1347 - accuracy: 0.6973\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1315 - accuracy: 0.6983\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1221 - accuracy: 0.6982\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1126 - accuracy: 0.7000\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1158 - accuracy: 0.7002\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1087 - accuracy: 0.7017\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.1038 - accuracy: 0.7018\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1024 - accuracy: 0.7032\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.1001 - accuracy: 0.7046\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0914 - accuracy: 0.7058\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0893 - accuracy: 0.7035\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0855 - accuracy: 0.7046\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0885 - accuracy: 0.7045\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0837 - accuracy: 0.7046\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0775 - accuracy: 0.7044\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0750 - accuracy: 0.7082\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0745 - accuracy: 0.7065\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0716 - accuracy: 0.7067\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0635 - accuracy: 0.7063\n",
      "Epoch 53/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0633 - accuracy: 0.7094\n",
      "Epoch 54/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0614 - accuracy: 0.7091\n",
      "Epoch 55/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0621 - accuracy: 0.7075\n",
      "Epoch 56/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0604 - accuracy: 0.7079\n",
      "Epoch 57/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0600 - accuracy: 0.7076\n",
      "Epoch 58/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0513 - accuracy: 0.7102\n",
      "Epoch 59/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0501 - accuracy: 0.7105\n",
      "Epoch 60/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0489 - accuracy: 0.7102\n",
      "Epoch 61/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0515 - accuracy: 0.7083\n",
      "Epoch 62/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0478 - accuracy: 0.7097\n",
      "Epoch 63/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0477 - accuracy: 0.7082\n",
      "Epoch 64/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0429 - accuracy: 0.7083\n",
      "Epoch 65/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0424 - accuracy: 0.7096\n",
      "Epoch 66/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0425 - accuracy: 0.7095\n",
      "Epoch 67/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0378 - accuracy: 0.7112\n",
      "Epoch 68/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0322 - accuracy: 0.7122\n",
      "Epoch 69/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0337 - accuracy: 0.7105\n",
      "Epoch 70/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0344 - accuracy: 0.7139\n",
      "Epoch 71/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0287 - accuracy: 0.7111\n",
      "Epoch 72/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0308 - accuracy: 0.7123\n",
      "Epoch 73/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0320 - accuracy: 0.7111\n",
      "Epoch 74/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0271 - accuracy: 0.7102\n",
      "Epoch 75/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0236 - accuracy: 0.7121\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0264 - accuracy: 0.7126\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0263 - accuracy: 0.7115\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 2s 7ms/step - loss: 1.0234 - accuracy: 0.7114\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0234 - accuracy: 0.7121\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0240 - accuracy: 0.7104\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0174 - accuracy: 0.7137\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 3s 7ms/step - loss: 1.0196 - accuracy: 0.7119\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0194 - accuracy: 0.7103\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 3s 8ms/step - loss: 1.0190 - accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "def first_model_LSTM():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def first_model_GRU():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(GRU(256, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def basic_rnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    model.add(SimpleRNN(256))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "basic_rnn = basic_rnn_model()\n",
    "base_LSTM = first_model_LSTM()\n",
    "base_GRU = first_model_GRU()\n",
    "\n",
    "\n",
    "history_basic_rnn_model = basic_rnn.fit(x, labels_encoded, batch_size=128,epochs=100,verbose=1,shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_rnn.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_base_LSTM = first_model_LSTM().fit(x, labels_encoded, batch_size=128, epochs=100, verbose=1, shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_LSTM.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_base_GRU = base_GRU.fit(x, labels_encoded, batch_size=128, epochs=100, verbose=1, shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=3),\n",
    "        ModelCheckpoint(filepath=\"base_model_GRU.h5\", monitor=\"loss\", save_best_only=True),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_base = pd.DataFrame(\n",
    "    [{\n",
    "        \"basic_rnn\": min(history_basic_rnn_model.history[\"loss\"]),\n",
    "        \"base_LSTM\": min(history_base_LSTM.history[\"loss\"]),\n",
    "        \"base_GRU\": min(history_base_GRU.history[\"loss\"]),\n",
    "    }]\n",
    ")\n",
    "plt.bar(results_base.columns, results_base.iloc[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_layers(number_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    if number_layers != 0:\n",
    "        for i in range(number_layers):\n",
    "            model.add(LSTM(256, return_sequences=True))\n",
    "            model.add(LayerNormalization())\n",
    "            model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x,\n",
    "        labels_encoded,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "718/718 [==============================] - 6s 7ms/step - loss: 5.2025 - accuracy: 0.1099\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 4.4856 - accuracy: 0.1642\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 3.8169 - accuracy: 0.2593\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 3.2557 - accuracy: 0.3364\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 7s 10ms/step - loss: 2.8887 - accuracy: 0.3802\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.6179 - accuracy: 0.4165\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.4243 - accuracy: 0.4484\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.2433 - accuracy: 0.4768\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 2.1010 - accuracy: 0.5018\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.9832 - accuracy: 0.5255\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.8848 - accuracy: 0.5434\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.7907 - accuracy: 0.5610\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.7224 - accuracy: 0.5739\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.6596 - accuracy: 0.5876\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.6045 - accuracy: 0.5979\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.5455 - accuracy: 0.6118\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.5022 - accuracy: 0.6180\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.4635 - accuracy: 0.6258\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.4295 - accuracy: 0.6341\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3990 - accuracy: 0.6372\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3636 - accuracy: 0.6442\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3414 - accuracy: 0.6488\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.3167 - accuracy: 0.6537\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.2971 - accuracy: 0.6580\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.2766 - accuracy: 0.6632\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2542 - accuracy: 0.6664\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2393 - accuracy: 0.6715\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.2211 - accuracy: 0.6740\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 6s 9ms/step - loss: 1.2106 - accuracy: 0.6757\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.1957 - accuracy: 0.6793\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1839 - accuracy: 0.6808\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1721 - accuracy: 0.6814\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1620 - accuracy: 0.6842\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.1544 - accuracy: 0.6848\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 5s 8ms/step - loss: 1.1455 - accuracy: 0.6870\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 6s 9ms/step - loss: 1.1330 - accuracy: 0.6906\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1246 - accuracy: 0.6925\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1178 - accuracy: 0.6930\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1129 - accuracy: 0.6943\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.1030 - accuracy: 0.6952\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0967 - accuracy: 0.6964\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0895 - accuracy: 0.6989\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0880 - accuracy: 0.6985\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0803 - accuracy: 0.6995\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0765 - accuracy: 0.7027\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0727 - accuracy: 0.7004\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0658 - accuracy: 0.7023\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0629 - accuracy: 0.7042\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0553 - accuracy: 0.7063\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0559 - accuracy: 0.7030\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0514 - accuracy: 0.7050\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0469 - accuracy: 0.7069\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0443 - accuracy: 0.7060\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0404 - accuracy: 0.7075\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0369 - accuracy: 0.7081\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0345 - accuracy: 0.7079\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0287 - accuracy: 0.7086\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0286 - accuracy: 0.7092\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0270 - accuracy: 0.7089\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0213 - accuracy: 0.7099\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0211 - accuracy: 0.7096\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.0161 - accuracy: 0.7103\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0128 - accuracy: 0.7118\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0143 - accuracy: 0.7112\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0093 - accuracy: 0.7107\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0091 - accuracy: 0.7120\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0068 - accuracy: 0.7114\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0054 - accuracy: 0.7115\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 1.0021 - accuracy: 0.7123\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 1.0012 - accuracy: 0.7145\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9965 - accuracy: 0.7127\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9972 - accuracy: 0.7127\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 7s 10ms/step - loss: 0.9905 - accuracy: 0.7142\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9932 - accuracy: 0.7128\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 6s 8ms/step - loss: 0.9920 - accuracy: 0.7126\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9882 - accuracy: 0.7153\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9891 - accuracy: 0.7139\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9812 - accuracy: 0.7158\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9833 - accuracy: 0.7156\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9809 - accuracy: 0.7163\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9822 - accuracy: 0.7144\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9815 - accuracy: 0.7157\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9790 - accuracy: 0.7165\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9756 - accuracy: 0.7158\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9770 - accuracy: 0.7156\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9760 - accuracy: 0.7154\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9721 - accuracy: 0.7180\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9740 - accuracy: 0.7176\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9743 - accuracy: 0.7160\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9654 - accuracy: 0.7176\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9670 - accuracy: 0.7185\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9660 - accuracy: 0.7176\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9647 - accuracy: 0.7175\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9656 - accuracy: 0.7178\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9647 - accuracy: 0.7176\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9634 - accuracy: 0.7178\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9637 - accuracy: 0.7165\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9591 - accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9614 - accuracy: 0.7177\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 5s 7ms/step - loss: 0.9595 - accuracy: 0.7171\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 11s 13ms/step - loss: 5.1643 - accuracy: 0.1108\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 4.2784 - accuracy: 0.1825\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.7957 - accuracy: 0.2424\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 3.3792 - accuracy: 0.2994\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 3.0763 - accuracy: 0.3378\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 2.8538 - accuracy: 0.3659\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.6820 - accuracy: 0.3877\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.5387 - accuracy: 0.4087\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.4197 - accuracy: 0.4281\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.3055 - accuracy: 0.4440\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.2078 - accuracy: 0.4632\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.1215 - accuracy: 0.4798\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.0423 - accuracy: 0.4917\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.9699 - accuracy: 0.5053\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.9020 - accuracy: 0.5202\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.8465 - accuracy: 0.5292\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.7878 - accuracy: 0.5422\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.7366 - accuracy: 0.5530\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.6916 - accuracy: 0.5632\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.6544 - accuracy: 0.5702\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.6155 - accuracy: 0.5766\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5797 - accuracy: 0.5847\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.5511 - accuracy: 0.5915\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.5202 - accuracy: 0.5989\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.4935 - accuracy: 0.6021\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4658 - accuracy: 0.6082\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.4461 - accuracy: 0.6134\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.4219 - accuracy: 0.6200\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.4020 - accuracy: 0.6255\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3820 - accuracy: 0.6277\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3661 - accuracy: 0.6339\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3480 - accuracy: 0.6364\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3338 - accuracy: 0.6390\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3162 - accuracy: 0.6451\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.3151 - accuracy: 0.6439\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2958 - accuracy: 0.6487\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2828 - accuracy: 0.6495\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2765 - accuracy: 0.6517\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2562 - accuracy: 0.6552\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2492 - accuracy: 0.6599\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2375 - accuracy: 0.6617\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2299 - accuracy: 0.6622\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2279 - accuracy: 0.6625\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2092 - accuracy: 0.6670\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.2058 - accuracy: 0.6658\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1980 - accuracy: 0.6699\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1876 - accuracy: 0.6706\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1774 - accuracy: 0.6715\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1770 - accuracy: 0.6732\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1636 - accuracy: 0.6759\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1586 - accuracy: 0.6792\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1534 - accuracy: 0.6788\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1473 - accuracy: 0.6798\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1501 - accuracy: 0.6796\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1364 - accuracy: 0.6820\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1315 - accuracy: 0.6818\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1285 - accuracy: 0.6844\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.1242 - accuracy: 0.6865\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.1178 - accuracy: 0.6868\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.1114 - accuracy: 0.6900\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1083 - accuracy: 0.6879\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1024 - accuracy: 0.6917\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.1007 - accuracy: 0.6916\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0965 - accuracy: 0.6921\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0880 - accuracy: 0.6941\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0852 - accuracy: 0.6956\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0811 - accuracy: 0.6962\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0795 - accuracy: 0.6950\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0805 - accuracy: 0.6941\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0774 - accuracy: 0.6942\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0732 - accuracy: 0.6962\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0693 - accuracy: 0.6977\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0655 - accuracy: 0.6973\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0589 - accuracy: 0.6994\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0607 - accuracy: 0.6985\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0611 - accuracy: 0.6987\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0570 - accuracy: 0.6981\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0531 - accuracy: 0.6988\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0499 - accuracy: 0.7014\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0511 - accuracy: 0.7007\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0467 - accuracy: 0.7017\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0428 - accuracy: 0.7027\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0391 - accuracy: 0.7049\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0422 - accuracy: 0.7022\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0359 - accuracy: 0.7056\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0348 - accuracy: 0.7027\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0370 - accuracy: 0.7030\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0304 - accuracy: 0.7053\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0292 - accuracy: 0.7051\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0269 - accuracy: 0.7051\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 1.0240 - accuracy: 0.7058\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 1.0236 - accuracy: 0.7042\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0212 - accuracy: 0.7060\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0153 - accuracy: 0.7077\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0167 - accuracy: 0.7075\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0202 - accuracy: 0.7067\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0193 - accuracy: 0.7076\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 8s 11ms/step - loss: 1.0156 - accuracy: 0.7077\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0104 - accuracy: 0.7091\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 8s 12ms/step - loss: 1.0137 - accuracy: 0.7074\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 15s 16ms/step - loss: 5.5007 - accuracy: 0.0798\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 4.7745 - accuracy: 0.1584\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 3.3810 - accuracy: 0.3316\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 2.7865 - accuracy: 0.3977\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.4364 - accuracy: 0.4432\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.1879 - accuracy: 0.4844\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.9919 - accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.8418 - accuracy: 0.5445\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.7147 - accuracy: 0.5659\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6190 - accuracy: 0.5868\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.5362 - accuracy: 0.6035\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4723 - accuracy: 0.6156\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4160 - accuracy: 0.6281\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.3704 - accuracy: 0.6356\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3372 - accuracy: 0.6413\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.3047 - accuracy: 0.6486\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2741 - accuracy: 0.6549\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2483 - accuracy: 0.6599\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.2310 - accuracy: 0.6643\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.2128 - accuracy: 0.6674\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1939 - accuracy: 0.6704\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1774 - accuracy: 0.6746\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1678 - accuracy: 0.6780\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1577 - accuracy: 0.6777\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1383 - accuracy: 0.6826\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1317 - accuracy: 0.6827\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1213 - accuracy: 0.6857\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1154 - accuracy: 0.6872\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1092 - accuracy: 0.6878\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1020 - accuracy: 0.6904\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0935 - accuracy: 0.6918\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0892 - accuracy: 0.6908\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0806 - accuracy: 0.6931\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0754 - accuracy: 0.6945\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0724 - accuracy: 0.6955\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.0674 - accuracy: 0.6955\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.0593 - accuracy: 0.6973\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0582 - accuracy: 0.6986\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0568 - accuracy: 0.6982\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0520 - accuracy: 0.6972\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0464 - accuracy: 0.7023\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0406 - accuracy: 0.7012\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0370 - accuracy: 0.7021\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0351 - accuracy: 0.7025\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0332 - accuracy: 0.7034\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0269 - accuracy: 0.7053\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0281 - accuracy: 0.7037\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.0256 - accuracy: 0.7043\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0190 - accuracy: 0.7063\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0170 - accuracy: 0.7090\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0188 - accuracy: 0.7052\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0141 - accuracy: 0.7079\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0127 - accuracy: 0.7070\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0125 - accuracy: 0.7050\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0085 - accuracy: 0.7077\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.0022 - accuracy: 0.7091\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9971 - accuracy: 0.7107\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9988 - accuracy: 0.7098\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0040 - accuracy: 0.7078\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9959 - accuracy: 0.7107\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9969 - accuracy: 0.7104\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9890 - accuracy: 0.7124\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9903 - accuracy: 0.7100\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9888 - accuracy: 0.7100\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9873 - accuracy: 0.7105\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9867 - accuracy: 0.7119\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9874 - accuracy: 0.7108\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9852 - accuracy: 0.7113\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9801 - accuracy: 0.7107\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9781 - accuracy: 0.7120\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9779 - accuracy: 0.7121\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9730 - accuracy: 0.7124\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9741 - accuracy: 0.7123\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9740 - accuracy: 0.7154\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9715 - accuracy: 0.7137\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9727 - accuracy: 0.7141\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9672 - accuracy: 0.7157\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9692 - accuracy: 0.7131\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9699 - accuracy: 0.7148\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9683 - accuracy: 0.7145\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9660 - accuracy: 0.7142\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9629 - accuracy: 0.7160\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9616 - accuracy: 0.7150\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9602 - accuracy: 0.7159\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9600 - accuracy: 0.7152\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9577 - accuracy: 0.7150\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9579 - accuracy: 0.7148\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9532 - accuracy: 0.7173\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9565 - accuracy: 0.7165\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9568 - accuracy: 0.7181\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9553 - accuracy: 0.7164\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9556 - accuracy: 0.7154\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9554 - accuracy: 0.7153\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9554 - accuracy: 0.7172\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9535 - accuracy: 0.7174\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9501 - accuracy: 0.7168\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9479 - accuracy: 0.7175\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 0.9486 - accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9467 - accuracy: 0.7172\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9493 - accuracy: 0.7179\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 19s 21ms/step - loss: 5.5020 - accuracy: 0.0788\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 5.4420 - accuracy: 0.0802\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 5.4348 - accuracy: 0.0818\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 5.0461 - accuracy: 0.1139\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 3.9920 - accuracy: 0.2306\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 3.3160 - accuracy: 0.3172\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.8672 - accuracy: 0.3724\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 2.5489 - accuracy: 0.4177\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.3185 - accuracy: 0.4550\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.1446 - accuracy: 0.4822\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.0052 - accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.8900 - accuracy: 0.5264\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.7915 - accuracy: 0.5457\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.7136 - accuracy: 0.5603\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.6507 - accuracy: 0.5719\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5939 - accuracy: 0.5835\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.5444 - accuracy: 0.5949\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.5061 - accuracy: 0.6023\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.4602 - accuracy: 0.6094\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.4327 - accuracy: 0.6168\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.4087 - accuracy: 0.6207\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.3822 - accuracy: 0.6259\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3590 - accuracy: 0.6300\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3380 - accuracy: 0.6365\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3198 - accuracy: 0.6392\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.3031 - accuracy: 0.6439\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2862 - accuracy: 0.6463\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2734 - accuracy: 0.6493\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2581 - accuracy: 0.6524\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.2511 - accuracy: 0.6539\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.2363 - accuracy: 0.6567\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 17s 23ms/step - loss: 1.2285 - accuracy: 0.6569\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.2178 - accuracy: 0.6615\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.2114 - accuracy: 0.6628\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1974 - accuracy: 0.6671\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1950 - accuracy: 0.6654\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1858 - accuracy: 0.6690\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1816 - accuracy: 0.6684\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.1709 - accuracy: 0.6722\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1635 - accuracy: 0.6728\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1533 - accuracy: 0.6767\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1556 - accuracy: 0.6750\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1462 - accuracy: 0.6765\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1455 - accuracy: 0.6760\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1367 - accuracy: 0.6784\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.1271 - accuracy: 0.6809\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1245 - accuracy: 0.6799\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1202 - accuracy: 0.6806\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 16s 23ms/step - loss: 1.1174 - accuracy: 0.6827\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1155 - accuracy: 0.6832\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1092 - accuracy: 0.6841\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.1079 - accuracy: 0.6850\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.1013 - accuracy: 0.6855\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0949 - accuracy: 0.6856\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0985 - accuracy: 0.6858\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0972 - accuracy: 0.6879\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0866 - accuracy: 0.6898\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0833 - accuracy: 0.6910\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0869 - accuracy: 0.6882\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0753 - accuracy: 0.6915\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0744 - accuracy: 0.6918\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0712 - accuracy: 0.6923\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0677 - accuracy: 0.6935\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0694 - accuracy: 0.6921\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0687 - accuracy: 0.6931\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0593 - accuracy: 0.6946\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0569 - accuracy: 0.6959\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0584 - accuracy: 0.6972\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0567 - accuracy: 0.6963\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0503 - accuracy: 0.6972\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0520 - accuracy: 0.6980\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0491 - accuracy: 0.6975\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0437 - accuracy: 0.6972\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0423 - accuracy: 0.6961\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0417 - accuracy: 0.6972\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0425 - accuracy: 0.6976\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0405 - accuracy: 0.6992\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0351 - accuracy: 0.6983\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0320 - accuracy: 0.7020\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0295 - accuracy: 0.7017\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0295 - accuracy: 0.7028\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0243 - accuracy: 0.7031\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0278 - accuracy: 0.7019\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0303 - accuracy: 0.6996\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0231 - accuracy: 0.6998\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0239 - accuracy: 0.7019\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0153 - accuracy: 0.7048\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0170 - accuracy: 0.7034\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0207 - accuracy: 0.7030\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0186 - accuracy: 0.7029\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0134 - accuracy: 0.7042\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0151 - accuracy: 0.7037\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0128 - accuracy: 0.7038\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0078 - accuracy: 0.7058\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.0096 - accuracy: 0.7053\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.0050 - accuracy: 0.7068\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 16s 22ms/step - loss: 1.0031 - accuracy: 0.7069\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0029 - accuracy: 0.7054\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 15s 22ms/step - loss: 1.0031 - accuracy: 0.7065\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.0032 - accuracy: 0.7069\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 24s 26ms/step - loss: 5.5058 - accuracy: 0.0796\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 5.4414 - accuracy: 0.0806\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 4.8937 - accuracy: 0.1267\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 4.1396 - accuracy: 0.1997\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 3.7113 - accuracy: 0.2549\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 3.3404 - accuracy: 0.3042\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 3.0206 - accuracy: 0.3427\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 2.7623 - accuracy: 0.3801\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 2.5509 - accuracy: 0.4136\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 2.3797 - accuracy: 0.4376\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 2.2428 - accuracy: 0.4589\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 21s 30ms/step - loss: 2.1272 - accuracy: 0.4788\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 2.0333 - accuracy: 0.4941\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.9361 - accuracy: 0.5153\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.8705 - accuracy: 0.5247\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 1.8036 - accuracy: 0.5404\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.7492 - accuracy: 0.5493\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.7017 - accuracy: 0.5595\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.6496 - accuracy: 0.5712\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.6157 - accuracy: 0.5759\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.5817 - accuracy: 0.5828\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.5454 - accuracy: 0.5915\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.5132 - accuracy: 0.5989\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.4911 - accuracy: 0.6011\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.4648 - accuracy: 0.6069\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.4461 - accuracy: 0.6114\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.4227 - accuracy: 0.6160\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.4040 - accuracy: 0.6192\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.3847 - accuracy: 0.6222\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.3800 - accuracy: 0.6258\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.3653 - accuracy: 0.6268\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.3460 - accuracy: 0.6314\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.3412 - accuracy: 0.6326\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.3264 - accuracy: 0.6360\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.3050 - accuracy: 0.6397\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 17s 24ms/step - loss: 1.3027 - accuracy: 0.6394\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 18s 24ms/step - loss: 1.2897 - accuracy: 0.6448\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 17s 24ms/step - loss: 1.2782 - accuracy: 0.6479\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.2753 - accuracy: 0.6476\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.2617 - accuracy: 0.6493\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.2487 - accuracy: 0.6523\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.2508 - accuracy: 0.6507\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2431 - accuracy: 0.6549\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2367 - accuracy: 0.6554\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2294 - accuracy: 0.6558\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2235 - accuracy: 0.6579\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2188 - accuracy: 0.6589\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.2075 - accuracy: 0.6631\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 1.2061 - accuracy: 0.6617\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1984 - accuracy: 0.6619\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1890 - accuracy: 0.6663\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 1.1833 - accuracy: 0.6688\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.1852 - accuracy: 0.6666\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1773 - accuracy: 0.6670\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1754 - accuracy: 0.6684\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1699 - accuracy: 0.6688\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1595 - accuracy: 0.6719\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1561 - accuracy: 0.6734\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.1567 - accuracy: 0.6728\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1510 - accuracy: 0.6721\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1453 - accuracy: 0.6746\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1399 - accuracy: 0.6759\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1394 - accuracy: 0.6739\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.1344 - accuracy: 0.6779\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1309 - accuracy: 0.6768\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1321 - accuracy: 0.6775\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1233 - accuracy: 0.6808\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1189 - accuracy: 0.6812\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.1195 - accuracy: 0.6812\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1131 - accuracy: 0.6822\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.1096 - accuracy: 0.6821\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.1145 - accuracy: 0.6801\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.1089 - accuracy: 0.6834\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 1.1008 - accuracy: 0.6825\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0999 - accuracy: 0.6851\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0965 - accuracy: 0.6858\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0954 - accuracy: 0.6855\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0947 - accuracy: 0.6858\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0878 - accuracy: 0.6873\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0871 - accuracy: 0.6873\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0839 - accuracy: 0.6871\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.0853 - accuracy: 0.6876\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0812 - accuracy: 0.6902\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.0778 - accuracy: 0.6884\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 20s 28ms/step - loss: 1.0777 - accuracy: 0.6888\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0776 - accuracy: 0.6912\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0741 - accuracy: 0.6906\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0716 - accuracy: 0.6921\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 19s 27ms/step - loss: 1.0730 - accuracy: 0.6903\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0710 - accuracy: 0.6919\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 18s 26ms/step - loss: 1.0642 - accuracy: 0.6928\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0607 - accuracy: 0.6928\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0622 - accuracy: 0.6946\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0576 - accuracy: 0.6935\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0620 - accuracy: 0.6900\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0587 - accuracy: 0.6955\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 18s 25ms/step - loss: 1.0522 - accuracy: 0.6939\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 19s 26ms/step - loss: 1.0556 - accuracy: 0.6928\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.0500 - accuracy: 0.6960\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 20s 27ms/step - loss: 1.0567 - accuracy: 0.6936\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZM0lEQVR4nO3dd3RUBd7G8e+k94QaCAk19BKKisECCFJFQMW6FmyLCyqgIlhARQEVsACrrr6Ka0eaBWmCNEWkhR4gJECANFp6nbnvH8GsUUoSk9wpz+ecOUcmd2ae6zDMk9t+FsMwDERERERciJvZAURERESqmwqQiIiIuBwVIBEREXE5KkAiIiLiclSARERExOWoAImIiIjLUQESERERl+NhdgB7ZLPZOHHiBIGBgVgsFrPjiIiISBkYhkFmZiZhYWG4uV18G48K0HmcOHGCiIgIs2OIiIhIBSQmJhIeHn7RZVSAziMwMBAo/h8YFBRkchoREREpi4yMDCIiIkq+xy9GBeg8ft/tFRQUpAIkIiLiYMpy+IoOghYRERGXowIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5agAiYiIiMtRARIRERGXowIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERqTaGYbBqXwo2m2FqDhUgERERqTbztx7jgY+38PAnW0wtQSpAIiIiUi3iUjOZ+M0eADo1rIGbm8W0LCpAIiIiUuXyCq2M+nw7uYVWro6szSPdm5maRwVIREREqtwrS/YRm5xJ7QAvZt4WZerWH1ABEhERkSq2bHcSn/x6BICZt3akbqCPyYlUgERERKQKHTuTw7j5OwEY0b0Z17aoY3KiYipAIiIiUiUKrTYe+2I7GXlFdIwI4Yk+LcyOVEIFSERERKrEGysPsO3oWQJ9PJh1Ryc83e2ndthPEhEREXEa6w+m8c7aQwBMu6kDETX9TE5UmgqQiIiIVKq0zHzGfLUDw4A7uzZkYIf6Zkf6CxUgERERqTQ2m8HYeTGczMqnZWggE29oY3ak81IBEhERkUrzn/XxrD94Eh9PN2bf2QkfT3ezI52XCpCIiIhUim1HzzB9+X4AXryxLc1DA01OdGEqQCIiIvK3pecW8ujn2ymyGQyKCuPWyyLMjnRRKkAiIiLytxiGwfgFOzl+NpeGNf2YMrQdFou5oy4uRQVIRERE/pbPfzvK0t3JeLpbmH1nJwJ9PM2OdEkqQCIiIlJhsckZvPTdXgCe7teKDuEh5gYqIxUgERERqZCcgiJGfb6d/CIbPVvW4f6rmpgdqcxUgERERKRCXvx2L3GpWdQN9Gb6sCjc3Oz7uJ8/UgESERGRcvsm5jhfbUnEYoE3b+9IrQBvsyOViwqQiIiIlMuRU9k8u2g3AI/2jKRbs9omJyo/FSAREREps4IiG49+sZ2s/CKuaFyTx3o1NztShagAiYiISJm9vjyWncfSCfHz5M3bO+Lh7phVwjFTi4iISLVbHZvC++sTAHj9lijCQnxNTlRxKkAiIiJyScnpeTz59U4A7uvWmOvbhJqc6O9RARIREZGLstoMRn+1ndPZBbQNC2LCgFZmR/rbVIBERETkoub8FMev8afx83Jn1h2d8PZwNzvS36YCJCIiIhf0W8Jp3vzxAACvDG1H0zoBJieqHCpAIiIicl5nsgt4/Mvt2Ay4uXM4QzuFmx2p0qgAiYiIyF8YhsFT83eSlJ5H09r+vDS4rdmRKpWpBWjdunUMGjSIsLAwLBYLixcvvuRj1qxZQ+fOnfH29iYyMpK5c+decNlp06ZhsVgYPXp0pWUWERFxBXN/OcyP+1Lwcndj1p2d8Pf2MDtSpTK1AGVnZxMVFcWcOXPKtHxCQgIDBw6kZ8+exMTEMHr0aB588EGWL1/+l2U3b97Me++9R4cOHSo7toiIiFPbfTydqT/EAvDswNa0DQs2OVHlM7XO9e/fn/79+5d5+XfffZcmTZowY8YMAFq3bs2GDRt444036Nu3b8lyWVlZ3HXXXbz//vu8/PLLlZ5bRETEWWXlF/HoF9spsNro0yaUe6IbmR2pSjjUMUAbN26kd+/epe7r27cvGzduLHXfyJEjGThw4F+WvZD8/HwyMjJK3URERFzRxMW7STiZTViwD6/d0gGLxWJ2pCrhUDv0kpOTCQ0tfeXJ0NBQMjIyyM3NxdfXly+//JJt27axefPmMj/v1KlTefHFFys7roiIiENZsPUYC7cfx93Nwtt3dCLEz8vsSFXGobYAXUpiYiKPP/44n332GT4+PmV+3IQJE0hPTy+5JSYmVmFKERER+3MoLYvnv9kNwJjezbmscU2TE1Uth9oCVK9ePVJSUkrdl5KSQlBQEL6+vmzdupXU1FQ6d+5c8nOr1cq6deuYPXs2+fn5uLv/9eqV3t7eeHt7V3l+ERERe5RXaGXU59vJKbDSrVktHukRaXakKudQBSg6Opoffvih1H0rV64kOjoagF69erFr165SPx8+fDitWrXi6aefPm/5ERERcXVTf9jHvqQMavl78eZtHXF3c87jfv7I1AKUlZVFXFxcyZ8TEhKIiYmhZs2aNGzYkAkTJnD8+HH++9//AjBixAhmz57NuHHjuP/++1m9ejXz5s1jyZIlAAQGBtKuXbtSr+Hv70+tWrX+cr+IiIjA8j3JfLzxCAAzbo2iblDZDyFxZKYeA7RlyxY6depEp06dABg7diydOnVi4sSJACQlJXH06NGS5Zs0acKSJUtYuXIlUVFRzJgxgw8++KDUKfAiIiJSNsfP5jJu/k4A/nltU3q0rGtyoupjMQzDMDuEvcnIyCA4OJj09HSCgoLMjiMiIlLpiqw2bv/Pr2w5coaoiBC+/mc0Xh6OfW5Ueb6/HXtNRUREpELe/PEgW46cIdDbg1m3d3L48lNerrW2IiIiws9xJ5mzpvgY3Ck3tadhLT+TE1U/FSAREREXcjIrn9FfxWAYcMcVEQyKCjM7kilUgERERFyEzWbwxLwdpGXm0yI0gIk3tDU7kmlUgERERFzEBxviWXsgDR9PN2bf2RlfL9e9Pp4KkIiIiAvYfvQMry3bD8CkQW1pERpociJzqQCJiIg4ufTcQh79YjtFNoOBHepz++URZkcynQqQiIiIEzMMg2cW7uLYmVzCa/gy9ab2WCzOP+riUlSAREREnNiXmxNZsisJDzcLs+/sTJCPp9mR7IIKkIiIiJM6kJLJC9/uAWBcv5Z0jAgxN5AdUQESERFxQrkFVkZ+to38IhvdW9Thwaubmh3JrqgAiYiIOKGXvt/DwdQs6gR6M+PWKNzcdNzPH6kAiYiIOJnvdpzgi98SsVjgzds6UjvA2+xIdkcFSERExIkcPZXDMwt3ATCyRyRXRdY2OZF9UgESERFxEgVFNh79cjuZ+UVc1qgGo3s3NzuS3VIBEhERcRIzVuxnR+JZgn09eeuOTni462v+QvR/RkRExAn8tD+V99bFA/DaLR1oEOJrciL7pgIkIiLi4FIy8nhi3g4A7o1uRN+29UxOZP9UgERERByY1WYw5qsYTmcX0Lp+EBMGtDY7kkNQARIREXFg76yJ45dDp/Dzcmf2nZ3w8XQ3O5JDUAESERFxUJsPn+aNHw8CMHlwO5rVCTA5keNQARIREXFAZ3MKePyL7VhtBjd1asDNXcLNjuRQVIBEREQcjGEYPDV/JyfS82hS25+XhrQzO5LDUQESERFxMP/deISVe1Pwcndj1h2dCPD2MDuSw1EBEhERcSB7TqTzypJ9AEwY0Ip2DYJNTuSYVIBEREQcRHZ+EY9+vp0Cq43erUO5r1tjsyM5LBUgERERBzHxmz3En8ymfrAPr9/SAYvFYnYkh6UCJCIi4gAWbjvGgm3HcLPAW7d3ooa/l9mRHJoKkIiIiJ2LT8viucW7ARjduwVXNKlpciLHpwIkIiJix/KLrIz6fDs5BVaubFqTkT0jzY7kFFSARERE7NjUH2LZm5RBTX8v3rq9E+5uOu6nMqgAiYiI2KmVe1OY+8thAGYMiyI0yMfcQE5EBUhERMQOnTiby1PzdwDw0DVN6NmqrsmJnIsKkIiIiJ0pstp4/MvtnM0ppEN4ME/1bWV2JKejAiQiImJn3l51kM2HzxDg7cGsOzrh5aGv68qm/6MiIiJ25JdDJ5n1UxwAU25qT6Na/iYnck4qQCIiInbiVFY+o7+MwTDgtssiuDEqzOxITksFSERExA7YbAZPfL2D1Mx8IusG8MKNbc2O5NRUgEREROzA/21IYM3+NLw93Jh9Zyd8vdzNjuTUVIBERERMtiPxLK8uiwVg4qA2tKoXZHIi56cCJCIiYqKMvEIe/WI7RTaDAe3rcecVDc2O5BJUgERERExiGAbPLNzF0dM5NAjxZepNHbBYNOqiOqgAiYiImGTelkS+35mEh5uFWXd2ItjX0+xILkMFSERExAQHUzKZ9O0eAJ7s25LODWuYnMi1qACJiIhUs7xCK6M+305eoY1rmtfm4Wuamh3J5agAiYiIVLOXvt/L/pRMagd4M/PWjri56bif6qYCJCIiUo2W7Ezi801HsVjgzds6UifQ2+xILkkFSEREpJokns5h/MKdADzSvRlXN69tciLXpQIkIiJSDQqtNh79YjuZeUV0bhjCmOtbmB3JpakAiYiIVIPpK/YTk3iWIB8P3r6jE57u+go2k/7vi4iIVLG1B9J4b208AK/d0oHwGn4mJxIVIBERkSqUmpHH2K9iALj7ykb0a1ff3EACqACJiIhUGZvNYMy8GE5lF9CqXiDPDmxtdiQ5RwVIRESkiryz9hA/x53C19Od2Xd2wsfT3exIco6pBWjdunUMGjSIsLAwLBYLixcvvuRj1qxZQ+fOnfH29iYyMpK5c+eW+vnUqVO5/PLLCQwMpG7dugwZMoT9+/dXzQqIiIhcwNYjp5m58gAALw5uS2TdQJMTyR+ZWoCys7OJiopizpw5ZVo+ISGBgQMH0rNnT2JiYhg9ejQPPvggy5cvL1lm7dq1jBw5kl9//ZWVK1dSWFhInz59yM7OrqrVEBERKeVsTgGPfRGD1WYwpGMYw7qEmx1J/sRiGIZhdggAi8XCokWLGDJkyAWXefrpp1myZAm7d+8uue/222/n7NmzLFu27LyPSUtLo27duqxdu5Zrr722TFkyMjIIDg4mPT2doKCgcq2HiIi4NsMwGPHpVpbvSaFxLT++f+waArw9zI7lEsrz/e1QxwBt3LiR3r17l7qvb9++bNy48YKPSU9PB6BmzZoXXCY/P5+MjIxSNxERkYr49NcjLN+Tgqe7hVl3dFb5sVMOVYCSk5MJDQ0tdV9oaCgZGRnk5ub+ZXmbzcbo0aO56qqraNeu3QWfd+rUqQQHB5fcIiIiKj27iIg4v70nMpi8ZB8A4/u3pn14sMmJ5EIcqgCV18iRI9m9ezdffvnlRZebMGEC6enpJbfExMRqSigiIs4ip6CIUV9so6DIRq9Wdbn/qsZmR5KLcKjtcvXq1SMlJaXUfSkpKQQFBeHr61vq/lGjRvH999+zbt06wsMvfvCZt7c33t6axisiIhU38Zs9xKdlUy/Ih9eHRWGxWMyOJBfhUFuAoqOjWbVqVan7Vq5cSXR0dMmfDcNg1KhRLFq0iNWrV9OkSZPqjikiIi5m0fZjzN96DDcLvHl7R2r6e5kdSS7B1AKUlZVFTEwMMTExQPFp7jExMRw9ehQo3jV1zz33lCw/YsQI4uPjGTduHLGxsfz73/9m3rx5jBkzpmSZkSNH8umnn/L5558TGBhIcnIyycnJ5z1GSERE5O9KOJnNc4uKz05+rFdzrmxay+REUhamnga/Zs0aevbs+Zf77733XubOnct9993H4cOHWbNmTanHjBkzhr179xIeHs7zzz/PfffdV/LzC21y/Oijj0otdzE6DV5ERMoiv8jKze/8wu7jGXRtUpPPH7oSdzft+jJLeb6/7eY6QPZEBUhERMripe/28uHPCdTw8+SHx6+hfrDvpR8kVcZprwMkIiJiL37cm8KHPycAMH1YlMqPg1EBEhERKaek9FyenL8DgAeubkKv1qGXeITYGxUgERGRciiy2nj8ixjO5hTSvkEw4/q1NDuSVIAKkIiISDnMWh3Hb4dP4+/lzqw7OuHt4W52JKkAFSAREZEy2njoFLNWHwRgyk3taVzb3+REUlEqQCIiImVwOruA0V9tx2bAsC7hDO7YwOxI8jeoAImIiFyCYRg8+fUOUjLyaVbHnxcHtzU7kvxNKkAiIiKX8H8bElgdm4qXhxuz7+yMn5dDjdKU81ABEhERuYidx87y6rJYAJ6/oQ2t6+sCuc5ABUhEROQCMvMKefSL7RRaDfq1rcc/ujY0O5JUEhUgERGR8zAMg2cX7ebIqRwahPjy6s0dLjhvUhyPCpCIiMh5fL31GN/uOIG7m4W37+hIsJ+n2ZGkEqkAiYiI/ElcaiaTvtkDwBN9WtClUU2TE0llUwESERH5g7xCK6M+305uoZWrI2sz4tpmZkeSKqACJCIi8gcvL9lLbHImtQO8mHlbFG5uOu7HGakAiYiInLN0VxKf/noUgJm3dqRuoI/JiaSqqACJiIgAiadzGLdgJwAjujfj2hZ1TE4kVUkFSEREXF6h1cbjX24nM6+ITg1DeKJPC7MjSRVTARIREZc3c+UBth09S6CPB2/f3glPd309Oju9wyIi4tLWHUjjnTWHAHj15g5E1PQzOZFUBxUgERFxWamZeYydFwPAXV0bMqB9fXMDSbVRARIREZdksxk8MW8HJ7MKaBkayPM3tDE7klQjFSCRcrLaDLMjiEgleG9dPOsPnsTH043Zd3bCx9Pd7EhSjVSARMphzk9xtHhuKcv3JJsdRUT+hq1HzjB9xX4AXryxLc1DA01OJNVNBUikjGISzzJjxX6sNoOXvttLXqHV7EgiUgHpOYU89sV2rDaDG6PCuPWyCLMjiQlUgETKIK/QyhPzYvh979fxs7l8+usRc0OJSLkZhsH4hTs5fjaXhjX9eGVoOywWjbpwRSpAImUwY8V+DqVlUzfQmwn9WwEwa3Uc6TmFJicTkfL4bNNRlu5OxtPdwuw7OxHo42l2JDGJCpDIJWw+fJoPNiQAMO3m9jxwdRNahAaQnlvIv9fGmZxORMpqX1IGL32/F4Cn+7WiQ3iIuYHEVCpAIheRU1DEk1/vwDBgWJdwrmsVioe7G0/3K94K9NHPhzl+NtfklCJyKTkFRTz6xXYKimz0bFmH+69qYnYkMZkKkMhFTFsay5FTOYQF+/D8oP9dI+S6VnXp2qQmBUU2Zq44YGJCESmLF77dQ1xqFqFB3kwfFoWbm477cXUqQCIX8HPcSf67sfhA51dv6UDQH44VsFgsTBjQGoCF24+xLynDlIwicmnfxBxn3pZjWCzw5m2dqBXgbXYksQMqQCLnkZlXyLj5O4Hiy+Nf07zOX5bpGBHCwA71MYziLUUiYn8On8zm2UW7AXj0uuZEN6tlciKxFypAIufxypJ9HD+bS0RNX545t6XnfJ7q0xIPNwtrD6Txc9zJakwoIpdSUGTj0S+2k5VfxBWNa/LYdZFmRxI7ogIk8ic/7U/ly82JALx+SxT+3h4XXLZxbX/u6toQgKlL92HTmAwRu/Haslh2HU8nxM+TN2/viIe7vvLkf/S3QeQP0nMKGb+geNfX8Ksac2XTS28uf7RXcwK8Pdh9PIPvdp6o6ogiUgarY1NKLl/x+i1RhIX4mpxI7I0KkMgfvPjdHlIy8mlS259xfVuV6TG1A7z557VNAZi+Yj/5RRqRIWKm5PQ8npi3Ayj+Reb6NqEmJxJ7pAIkcs6KPcks3H4cNwtMHxaFr1fZJ0M/cE0T6gZ6k3g6l09/PVqFKUXkYqw2g8e/3M6ZnELahgUxvn/ZfpER16MCJAKczi7gmUW7AHjo2qZ0aVSjXI/38/JgzPUtAJi9+iDpuRqRIWKG2avj2JRwGn8vd2bf2Rlvj7L/IiOuRQVIBHj+m92czCqgRWgAY3q3qNBzDOsSTmTdAM7kFPLu2kOVnFBELmVT/CneWlV8YdKXh7ajSW1/kxOJPVMBEpf3/c4TLNmZhLubhRnDOuLjWbHfGP84IuPDDQkkpWtEhkh1OZNdwONfxmAz4ObO4QztFG52JLFzKkDi0tIy83l+cfFF0kb2jKR9ePDfer7eretyReOa5GtEhki1MQyDJ7/eQXJGHk3r+PPS4LZmRxIHoAIkLsswDJ5ZtIszOYW0qR/EqJ5//yJpFouF8QOKtwIt2HaM2GSNyBCpah/9fJhVsal4ebgx645OF712l8jvVIDEZS3afpyVe1PwdLcw87YovDwq5+PQuWEN+rerh82AVzUiQ6RK7TqWztSl+wB4bmBr2ob9va244jpUgMQlJafnMenbPQCM7t2CVvWCKvX5n+pbPCLjp/1pbDx0qlKfW0SKZeUX8egX2yi0GvRpE8rdVzYyO5I4EBUgcTmGYfD0gp1k5hURFRFSchHDytS0TgB3XKERGSJVxTAMnlu0i8OncggL9uG1WzpgsVjMjiUORAVIXM5XmxNZeyANLw83ZgzrUGXzgR7r1Rx/L3d2Hktnya6kKnkNEVe1YNtxFsecwN3Nwtt3dCLEz8vsSOJgVIDEpSSezmHy93uB4knukXUDq+y16gR68/C1zQB4ffl+CopsVfZaIq4kLjWr5OzNMb2bc1njmiYnEkekAiQuw2YzGDd/J9kFVi5rVIP7r25S5a/54DVNqBPozdHTOXy26UiVv56Is8svsvLoF9vJLbRyVWQtHunx98/eFNekAiQu45Nfj7Ax/hS+nu5MHxaFu1vVHy/g7+3B6N7NAZi1Oo6MPI3IEPk7/rM2nn1JGdTy9+KNWztWy+dYnJMKkLiEwyezmXbulPTx/VvRuBovkX/bZRE0rePP6ewC3tOIDJEKO3wym1k/xQEwcVAb6gb5mJxIHJkKkDg9q634KrG5hVaim9aq9lNl/zgi4/82JJCcnletry/iDAzD4PlvdlNQZOPqyNrcGBVmdiRxcCpA4vQ+3JDAliNn8Pdy57VbOuBmwibzPm1C6dKoBnmFNt5YqREZIuW1ZFcS6w+exMvDjclD2umUd/nbVIDEqcWlZvL6iv0APHdDGyJq+pmSw2Kx8My5ERlfb03kYEqmKTlEHFFGXiEvfVd89ua/ejTTlHepFCpA4rSKrDaemLeDgiIb3VvU4fbLI0zN06VRTfq2DS0ekbFMIzJEymrmigOkZubTpLY/I7o3MzuOOIkKFaCPP/6YJUuWlPx53LhxhISE0K1bN44cKfupvuvWrWPQoEGEhYVhsVhYvHjxJR+zZs0aOnfujLe3N5GRkcydO/cvy8yZM4fGjRvj4+ND165d+e2338qcSZzHe+vi2XEsnUAfD6bd3N4uNpmP69cKdzcLP+5LZVO8RmSIXMrOY2f578bDAEwe3A4fT3dzA4nTqFABmjJlCr6+vgBs3LiROXPm8Nprr1G7dm3GjBlT5ufJzs4mKiqKOXPmlGn5hIQEBg4cSM+ePYmJiWH06NE8+OCDLF++vGSZr776irFjxzJp0iS2bdtGVFQUffv2JTU1tXwrKQ5tX1IGb/5YfKzNC4PaUj/Y1+RExZrVCSjZEjVlaSyGoREZIhditRk8u2g3NgMGdwzj6ua1zY4kTsRiVOBfYD8/P2JjY2nYsCFPP/00SUlJ/Pe//2XPnj306NGDtLS08gexWFi0aBFDhgy54DJPP/00S5YsYffu3SX33X777Zw9e5Zly5YB0LVrVy6//HJmz54NgM1mIyIigkcffZTx48eXKUtGRgbBwcGkp6cTFFS5QzKl6hUU2Rgy52f2JmXQu3Uo79/TxS62/vwuNTOPHq+vIafAypw7OzOwQ32zI4nYpY9/Ocykb/cQ6OPBqie6UzdQp73LxZXn+7tCW4ACAgI4dap48/2KFSu4/vrrAfDx8SE3N7ciT1kmGzdupHfv3qXu69u3Lxs3bgSgoKCArVu3llrGzc2N3r17lyxzPvn5+WRkZJS6ieOa/VMce5MyCPHzZMpN9ne2SN1AHx66pngA6+vLYzUiQ+Q8UjLyeH158QkM4/q1UvmRSlehAnT99dfz4IMP8uCDD3LgwAEGDBgAwJ49e2jcuHFl5islOTmZ0NDQUveFhoaSkZFBbm4uJ0+exGq1nneZ5OTkCz7v1KlTCQ4OLrlFRJh7sKxU3K5j6cw5d6G0l4e0s9t/NB+6tim1A7w4fCqHL347anYcEbsz+fu9ZOUXERURwp1XNDQ7jjihChWgOXPmEB0dTVpaGgsWLKBWrVoAbN26lTvuuKNSA1aHCRMmkJ6eXnJLTEw0O5JUQH6RlSe+jsFqMxjYoT43dLDfC6UFeHvweO8WALy96iCZGpEhUmLtgTS+35mEmwVeGdJO4y6kSnhU5EEhISElx9j80Ysvvvi3A11MvXr1SElJKXVfSkoKQUFB+Pr64u7ujru7+3mXqVev3gWf19vbG29v7yrJLNXnjZUHOZCSRe0ALyYPbmd2nEu6/fIIPtqQQPzJbP6zLp4n+rQ0O5KI6fIKrUz8pvg4z/u6NaFdg2CTE4mzqtAWoGXLlrFhw4aSP8+ZM4eOHTty5513cubMmUoL92fR0dGsWrWq1H0rV64kOjoaAC8vL7p06VJqGZvNxqpVq0qWEee07egZ/rOueM7WlKHtqenvZXKiS/N0d2Ncv+LS88H6BFIzNCJD5N8/xXHkVA71gnwY26eF2XHEiVWoAD311FMlBwrv2rWLJ554ggEDBpCQkMDYsWPL/DxZWVnExMQQExMDFJ/mHhMTw9GjxcdETJgwgXvuuadk+REjRhAfH8+4ceOIjY3l3//+N/PmzSt16v3YsWN5//33+fjjj9m3bx+PPPII2dnZDB8+vCKrKg4gt8DKk/N2YDPgpk4N6NP2wlv77E3ftvXo3DCE3EIrb/x40Ow4IqaKS83inXMDgycNakOAd4V2UoiUSYX+diUkJNCmTRsAFixYwA033MCUKVPYtm1byQHRZbFlyxZ69uxZ8uffy9O9997L3LlzSUpKKilDAE2aNGHJkiWMGTOGt956i/DwcD744AP69u1bssxtt91GWloaEydOJDk5mY4dO7Js2bK/HBgtzuP15fuJP5lNaJA3kwa1NTtOuVgsFiYMaM2wdzfy1eajPHB1YyLrBpodS6TaGYbB84t3U2g16NmyDv3aOc4vMuKYKnQdoJo1a7JhwwbatGnD1VdfzT333MPDDz/M4cOHadOmDTk5OVWRtdroOkCOY1P8KW5//1cMA+YOv5weLeuaHalCHvrvFlbuTeH6NqG8f89lZscRqXaLth9jzFc78PZw48ex3U2b2yeOrcqvA3T11VczduxYJk+ezG+//cbAgQMBOHDgAOHh4RV5SpFyy84v4sn5OzCM4gOKHbX8ADx9bkTGyr0pbD582uw4ItUqPaeQl7/fB8BjvZqr/Ei1qFABmj17Nh4eHsyfP5933nmHBg0aALB06VL69etXqQFFLmTKD/tIPJ1LgxBfnh3Y2uw4f0tk3QBuvezciIwf9mlEhriUV5fHciq7gMi6ASUXCRWpahXaBebstAvM/q0/mMbd/1c85PbzB7vSLdLxZwSlZuTR/fU15BZaeeeuzvRvrxEZ4vy2HjnDze/8AsBXD19J16a1TE4kjqw8398VPsTearWyePFi9u0r3mzZtm1bbrzxRtzdNalXqlZGXiHj5u8E4J7oRk5RfgDqBvnw0DVNeHt1HK8t30/vNqF4uldoI62IQyiy2nh20S4AbukSrvIj1apC/7rGxcXRunVr7rnnHhYuXMjChQv5xz/+Qdu2bTl06FBlZxQpZfJ3e0lKz6NRLT/G929ldpxK9XD3ZtTy9yLhZDZfakSGOLm5vxwmNjmTED9PJjjZZ1nsX4UK0GOPPUazZs1ITExk27ZtbNu2jaNHj9KkSRMee+yxys4oUmLVvhS+3noMiwWmD4vCz8u5rhNSPCKjOQBvrTpIVn6RyYlEqsaJs7nMXHkAgAn9W1ErQFfjl+pVoQK0du1aXnvtNWrWrFlyX61atZg2bRpr166ttHAif3Q2p4DxC4s3lz9wVRMub1zzEo9wTHdc0ZDGtfw4mVXA++vizY4jUiVe/G4POQVWLmtUg2FdNIBaql+FCpC3tzeZmZl/uT8rKwsvL/sfQSCOadK3e0jLzKdZHX+e7Ou8c7OKR2QU7w54f308qZkakSHO5ce9KSzfk4KHm4WXh7bDTcNOxQQVKkA33HADDz/8MJs2bcIwDAzD4Ndff2XEiBHceOONlZ1RhGW7k/gm5gRu53Z9+Xg698H2/dvVo2NECDkFVt7SiAxxIjkFRUz6dg8AD1zThFb1dKatmKNCBejtt9+mWbNmREdH4+Pjg4+PD926dSMyMpI333yzkiOKqzuVlc+zi4qnQ4/o3oxODWuYnKjqWSyWkoNCv9ycyKG0LJMTiVSOt1fFcfxs8fW7Hu/V3Ow44sIqdARpSEgI33zzDXFxcSWnwbdu3ZrIyMhKDSdiGAbPLd7NqewCWtULLDlA2BV0bVqL3q3r8uO+VF5bFst7d2tEhji2/cmZfLC++Li2F29s63QnMYhjKfPfvktNef/pp59K/nvmzJkVTyTyB9/uOMHS3cl4uFmYPiwKbw/n3vX1Z0/3a8Xq2FSW70lhy+HTXOakB36L87PZDJ5bvIsim0GfNqH0bqMB1WKuMheg7du3l2k5i0UHs0nlSM3IY+I3xccKjLouknYNgk1OVP2ahwZy62URfLk5kalLY5k/IlqfMXFI87ceY/PhM/h5uTPpxrZmxxEpewH64xYekapmGAYTFu4iPbeQdg2CGNnTdXevjrm+BYtjjrP1yBmW70mhX7t6ZkcSKZfT2QVMWVp8uMSY3i1oEOJrciKRCh4ELVLV5m89xqrYVLzc3Zh5a0eXHgkRGuTDg1cXD4h8bXksRVabyYlEymfqD/s4m1NIq3qB3HdVY7PjiAAqQGKHTpzN5aXv9gLFWz9ahAaanMh8/+zelJr+XsSnZfPVlkSz44iU2ab4U3y99RgArwxt79K/zIh90d9EsSuGYfD0gp1k5hfRqWEID1/b1OxIdiHQx5PHriveDfjGyoNka0SGOICCIhvPLS6+hMUdVzSkSyPnv4SFOA4VILErn/92lPUHT+Lt4cb0YVG46wqxJe7s2ohGtfw4mZXPB+sTzI4jckkfbIjnYGoWtfy9eLqf8169XRyTCpDYjcTTObyypPhAyXH9WtGsToDJieyLl4cbT50bAfLeukOkZeabnEjkwhJP5/D2quKrmD87sDUhfhqTJPZFBUjsgs1m8OTXO8gpsHJFk5oM79bY7Eh2aWD7+kSFB5NTYC35chGxN4ZhMPGb3eQV2riyaU2GdmpgdiSRv1ABErsw95fDbEo4jZ+XO9NvidJwxAuwWCyM798aKN5dGK8RGWKHlu9J5qf9aXi6W3h5SHtdu0rskgqQmC4+LYvXlscCMGFAaxrW8jM5kX2LblaL61rVxWozeH35frPjiJSSlV/EC98Wn8U5onszIutqV7bYJxUgMZX13K6vvEIbV0fW5h9dG5odySE83a8VbhZYujuZrUfOmB1HpMQbKw+QnJFHw5p+Ln0BU7F/KkBiqvfXx7Pt6FkCvT149ZYO2lReRi3rBXJLl3AApi3dh2EYJicSgd3H0/no5+IzFF8a3BYfT9ea3SeORQVITHMgJZOZKw4A8PwNbXR5/HIac30LvD3c2Hz4DD/uSzU7jrg4q83g2cW7sRkwsEN9erSsa3YkkYtSARJTFFptPDFvBwVWGz1b1mHYZeFmR3I49YN9eeDqJkDxViCNyBAzffHbUXYkniXA24OJN7QxO47IJakAiSneWXOIXcfTCfb1ZNrN2vVVUSN6NKOGnyeH0rJLxg2IVLe0zHxeXVZ8IsOTfVoQGuRjciKRS1MBkmq350R6yTVsXryxrf6x/BuCfDx59LrmAMxceYCcAo3IkOr3ypK9ZOYV0b5BMHdHNzY7jkiZqABJtSooKt71VWQz6Ns2lMEdw8yO5PDuurIhETV9ScvM5/80IkOq2c9xJ1kccwKLBV4Z2k7ja8RhqABJtXp71UFikzOp6e/FK0N1gbTK4O3hzpN9ikdkvLv2ECezNCJDqkdeobVk2Ok9VzaiQ3iIuYFEykEFSKrNjsSzvLP2EAAvD2lH7QBvkxM5j0EdwmjfIJjsAiuzNCJDqsl7a+NJOJlNnUBvnuirYafiWFSApFrkFVp54usdWG0Gg6LCGNC+vtmRnIqbm4UJ/VsB8NmmoySczDY5kTi7hJPZzFkTB8DEG9oQ5ONpciKR8lEBkmoxc+UB4lKzqBPozUs3tjU7jlPqFlmbHi3rUGQzmK4RGVKFfh92WlBk45rmtbmhg36hEcejAiRVbsvh07y/Ph6AqUPbU8Pfy+REzuvpfq2wWGDJriS2H9WIDKka3+1MYv3Bk3h5uDF5cDsdyycOSQVIqlROQRFPfr0Dw4BbuoTTu02o2ZGcWuv6QdzcufiiklOXxmpEhlS69NxCJn9fPOx0VM9IGtf2NzmRSMWoAEmVem3Zfg6fyqF+sA8TB+nqsNVh7LkRGb8lnGZ1rEZkSOWasWI/aZn5NK3tzz+7NzU7jkiFqQBJlfnl0Enm/nIYgFdv7qCDJKtJWIgvw6/6fURGrEZkSKXZkXiWT349AhSfyentoWGn4rhUgKRKZOUXMW7+TgDu7NqQa1vUMTmRa3mkRzNC/Dw5mJrFgm0akSF/X5HVxjOLdmEYMLRTA7pF1jY7ksjfogIkVeKVJfs4diaX8Bq+PDOgtdlxXE6wryejekYCxWfg5RZYTU4kju6TX4+w50QGQT4e+kyLU1ABkkq3Zn8qX/x2FIDXb4kiwNvD5ESu6e7oRoTX8CUlI58Pf9aIDKm45PQ8Zqw4AMDT/VtRJ1AXMRXHpwIklSo9t5DxC3YBcF+3xkQ3q2VyItf1xxEZ76w5xCmNyJAKmvz9XrLyi+jUMIQ7Lm9odhyRSqECJJXqxe/2kJyRR5Pa/jzdr5XZcVzejVFhtA0LIiu/iFmr48yOIw7op/2pLNmVhLubhVeGtMdNw07FSagASaVZuTeFhduO42aB6cM64OulM0TMVjwio/h4jc82HeHIKY3IkLLLK7Qy8ZviYafDuzWmTViQyYlEKo8KkFSKM9kFTFhYvOvroWua0qVRTZMTye+ubl6ba1vUodBq8LpGZEg5zF4dR+LpXOoH+zD6+hZmxxGpVCpAUime/2Y3J7PyaV43gDH6h9LujD83IuP7nUnsSDxrdhxxAHGpmby37hAAkwa11ckM4nRUgORvW7Izie93Fh8jMOPWKHw8tevL3rQJC2JopwYATF26TyMy5KIMw+DZRbsptBr0alWXvm01wkacjwqQ/C1pmfk8t7h419e/ejSjQ3iIuYHkgp7o0xIvDzd+jT/Nmv1pZscRO7Zw23E2JZzGx9ONF25sq2Gn4pRUgKTCin9L3MWZnEJa1w/i0euamx1JLqJBiC/3dWsMFI/IsNq0FUj+6mxOAa/8sA+Ax3u1IKKmn8mJRKqGCpBU2OKY46zYm4Knu4UZw6Lw8tBfJ3s3skckwb6e7E/J1IgMOa9Xl8VyOruAFqEBPHhNE7PjiFQZfWNJhSSn5zHpmz0APHZdc50e6yCC/TwZ2bMZADNXaESGlLb1yGm++C0RgJeHtMfTXV8R4rz0t1vKzTAMxi/cSUZeER3Cg3mkRzOzI0k53BPdmAYhviRn5PHRLxqRIcUKrTaeXVR8zZ9bLwvniia6lIU4NxUgKbd5WxJZsz8NLw83ZgyLwkO/JToUH093nuhTfKmCd346xOnsApMTiT346OcEYpMzqeHnyfj+GnYqzk/fXFIux87kMPn74gMkn7i+Bc1DA01OJBUxpGMDWtcPIjO/iNkakeHyjp/N5Y2VBwGYMKA1Nf29TE4kUvVUgKTMbDaDpxfsJCu/iC6NavDgNU3NjiQVVDwio3hW2ye/HibxdI7JicRML3y7h9xCK1c0rsktncPNjiNSLUwvQHPmzKFx48b4+PjQtWtXfvvttwsuW1hYyEsvvUSzZs3w8fEhKiqKZcuWlVrGarXy/PPP06RJE3x9fWnWrBmTJ0/Whd8qwWebjvBz3Cl8PN2YPiwKdw1FdGjXtqjD1ZG1NSLDxa3Yk8zKvSl4uFl4eWg7DTsVl2FqAfrqq68YO3YskyZNYtu2bURFRdG3b19SU1PPu/xzzz3He++9x6xZs9i7dy8jRoxg6NChbN++vWSZV199lXfeeYfZs2ezb98+Xn31VV577TVmzZpVXavllI6cymbKD7FA8ViFJrX9TU4klWH8ua1A3+44wa5j6SankeqWnV/EC98Wn8350LVNaaFd2uJCTC1AM2fO5KGHHmL48OG0adOGd999Fz8/Pz788MPzLv/JJ5/wzDPPMGDAAJo2bcojjzzCgAEDmDFjRskyv/zyC4MHD2bgwIE0btyYW265hT59+lx0y5JcnM1m8NTXO8kttBLdtBb3RDc2O5JUknYNghnSMQzQiAxX9Paqg5xIzyO8hi+P6UKm4mJMK0AFBQVs3bqV3r17/y+Mmxu9e/dm48aN531Mfn4+Pj4+pe7z9fVlw4YNJX/u1q0bq1at4sCBAwDs2LGDDRs20L9//wtmyc/PJyMjo9RN/ufDnxP47fBp/L3cee2WDtpE7mSe6NMSL3c3fjl0irUHNCLDVcQmZ/DBhuLLILw0uC2+XprhJ67FtAJ08uRJrFYroaGlh+yFhoaSnJx83sf07duXmTNncvDgQWw2GytXrmThwoUkJSWVLDN+/Hhuv/12WrVqhaenJ506dWL06NHcddddF8wydepUgoODS24RERGVs5JOIC41i9fOHR/y7MA2uiy+E4qo6cc90Y0AjchwFTZb8bBTq82gX9t6XNdKw07F9Zh+EHR5vPXWWzRv3pxWrVrh5eXFqFGjGD58OG5u/1uNefPm8dlnn/H555+zbds2Pv74Y6ZPn87HH398weedMGEC6enpJbfExMTqWB27V2S18cTXOygosnFtizrccYWKobMadV0kQT4exCZnsmj7cbPjSBWbtyWRrUfO4O/lzqQb25gdR8QUphWg2rVr4+7uTkpKSqn7U1JSqFev3nkfU6dOHRYvXkx2djZHjhwhNjaWgIAAmjb93+nYTz31VMlWoPbt23P33XczZswYpk6desEs3t7eBAUFlboJvLcunh2JZwn08eDVm9trIrQTC/Hz4l89IwGYuWI/eYUakeGsTmXlM3Vp8QkNY65vQf1gX5MTiZjDtALk5eVFly5dWLVqVcl9NpuNVatWER0dfdHH+vj40KBBA4qKiliwYAGDBw8u+VlOTk6pLUIA7u7u2Gy2yl0BJxebnMGbPxYfRzVpUFv9I+kC7uvWmLBgH06k5zH3l8Nmx5EqMuWHWNJzC2ldP4j7ujU2O46IaUzdBTZ27Fjef/99Pv74Y/bt28cjjzxCdnY2w4cPB+Cee+5hwoQJJctv2rSJhQsXEh8fz/r16+nXrx82m41x48aVLDNo0CBeeeUVlixZwuHDh1m0aBEzZ85k6NCh1b5+jqrQauOJeTsotBr0bl2Xmzs3MDuSVAMfT3fG9mkJwJyf4jijERlOZ+OhUyzYdgyLBaYMbacxNuLSPMx88dtuu420tDQmTpxIcnIyHTt2ZNmyZSUHRh89erTU1py8vDyee+454uPjCQgIYMCAAXzyySeEhISULDNr1iyef/55/vWvf5GamkpYWBj//Oc/mThxYnWvnsOavTqOPScyCPHzZMpN2vXlSoZ2asAH6+OJTc5kzk9xPHeDjg9xFgVFNp5bvAuAO69oSKeGNUxOJGIui6ELf/xFRkYGwcHBpKenu9zxQLuPpzNkzs8U2QzevqMTN0aFmR1Jqtma/anc99FmvNzdWPVEd5355yTm/BTH68v3UzvAi1VjexDs52l2JJFKV57vb23/lBL5RVbGzouhyGYwoH09BnWob3YkMUH3FnXo1qwWBVYbM1ZoRIYzOHoqh7dXFQ87fW5gG5UfEVSA5A/e/PEgB1KyqOXvxeTB7bTry0VZLBYm9G8NwOKYE+w+rhEZjswwDJ7/Zjf5RTa6NavF4I7aqisCKkByzrajZ3hv7SEAXhnanloB3iYnEjO1Dw8u2f057dwp0+KYlu5OZu2BNLzc3Zg8RL/YiPxOBUjIK7Ty5Nc7sBkwpGMY/dqd/zpM4lqe6tsST3cLG+JOsk4jMhxSZl4hL35XPOx0RI9mNKsTYHIiEfuhAiS8vnw/8WnZ1A305sUb25kdR+xERE0/7r6yMQBTl8Zi04gMhzNz5QFSMvJpVMuPf/VoZnYcEbuiAuTifks4zYc/Fw9EfPXmDjo4Ukp59LpIAn082JeUweIYjchwJLuPp/PxuQtaTh7cDh9PDTsV+SMVIBeWnV/Ek1/vwDDg1svC6dmqrtmRxM7U8PfikXNbDmasOKARGQ7CajN4dtEubAYMigrj2hZ1zI4kYndUgFzYtKWxHD2dQ1iwjy54Jxd0/1VNqBfkw/Gzufx342Gz40gZfL7pCDuOpRPo7cHzA1ubHUfELqkAuaif407yya9HAHjtliiCfLTrS86veERGC6D4KuFnczQiw56lZubx2rLi6zc91a8ldYN8TE4kYp9UgFxQZl4h4+bvBODuKxtxdfPaJicSe3dz53BahgaSkVfEv9ccMjuOXMTL3+8jM7+IDuHB3NW1kdlxROyWCpALevn7fRw/m0vDmn6M79/K7DjiANzdLCV/V+b+cphjZ3JMTiTns/5gGt/uOIGbBV4Z0h53N13zR+RCVIBczOrYFL7akojFAtOHReHvbeo8XHEgPVrW4cqmNSkosjFzxQGz48if5BVaeX7xbgDuiW5M+/BgkxOJ2DcVIBdyNqeA8QuKp0Hff1UTrmhS0+RE4kj+OCJjUcxx9p7IMDmR/NE7aw5x+FQOdQO9eeLcMVsicmEqQC7khW/3kJqZT9M6/jzVt6XZccQBRUWEcEOH+hgGTFumERn2Ij4ti3fOHZs1aVBbAnVSg8glqQC5iGW7k1kcU3xswIxhUboomlTY7yMy1h1IY8PBk2bHcXm/DzstsNro3qIOA9prlI1IWagAuYBTWfk8u6h419c/uzejU8MaJicSR9aoln/J2UVTl+7TiAyTfbvjBD/HncLbw42XBrfVsFORMlIBcnK//3Z4KruAlqGBjO7d3OxI4gQevS6SAG8P9pzI4NsdJ8yO47LScwqZ/P1eoPg9aVTL3+REIo5DBcjJfbcziR92JePhZmHGrVF4e2jXl/x9tQK8S0ZkvL58P/lFGpFhhtdXxHIyq4Bmdfx56NqmZscRcSgqQE4sNTOPid8UnxY7smck7RrotFipPPdf1YTQIG+On83lk41HzI7jcrYfPcNnm44C8PKQ9vrlRqScVICclGEYPLNwF2dzCmkbFsSo6yLNjiROxtfLnbHXF59uPWt1HOk5hSYnch1FVhvPLtqNYcBNnRsQ3ayW2ZFEHI4KkJNasO04P+5LxdO9eNeXp7veaql8N3cOp3ndANJzC/n32jiz47iMjzceYW9SBsG+njwzQMNORSpC34pOKCk9lxe/2wPA6N4taFUvyORE4qw83N1KRmR89PNhjp/NNTmR80tKz2XmiuJhp+P7t6J2gLfJiUQckwqQkzEMg3Hzd5KZV0RURAj/1IGRUsWua1WXK5poREZ1eem7vWQXWOncMITbLoswO46Iw1IBcjJf/JbI+oMn8fZwY8awKDy060uqmMViKdkNs3D7MfYlaURGVVkdm8LS3cm4u1l4ZWh73DTsVKTC9O3oRBJP5/DKkuJrgjzVtyWRdQNMTiSuomNECAPbF4/IeFUjMqpEboGVid8U79p+4OomtK6vXdsif4cKkJOw2Qyemr+D7AIrlzeuwfCrmpgdSVzMU31b4uFmYc3+NH6J04iMyjZr9UGOncklLNiHx3vpgqYif5cKkJP478bD/Bp/Gl9Pd6YPi8Jdm8almjWu7c9dXRsCMHVprEZkVKIDKZn8Z108AC/c2BZ/bw+TE4k4PhUgJ5BwMrtkMveEAa10OXwxzaO9muPv5c6u4+l8t1MjMiqDYRg8t2g3RTaD3q1D6dNWw05FKoMKkIOz2gye/HoHeYU2ujWrxT/ODakUMUPtAG9GdC8ekTF9hUZkVIb5W4/x2+Hirbsv3NjG7DgiTkMFyMH934Z4th45Q4C3B6/d0kFnhYjpHrimCXUDvUk8ncunvx41O45DO5NdwJQf9gEwundzwmv4mZxIxHmoADmwgymZTD933ZXnb2itfxzFLvh5eTDm3IiM2asPkp6rERkVNW1pLGdyCmkZGsj9V+vEBpHKpALkoIqsNp74egcFRTZ6tqzDrbogmtiRYV3CaVbHnzM5hby79pDZcRzS5sOn+WpLIgCvDG2ncTYilUyfKAf1zppD7DyWTpCPB9Nu7oDFol1fYj+KR2QUXxzxww0JJKVrREZ5FFptPLtoFwC3Xx7BZY1rmpxIxPmoADmgvScyeHv1QQBeHNyW0CAfkxOJ/FXv1nW5vHEN8otsvLFSIzLK4/82JHAgJYua/l483a+V2XFEnJIKkIMpKLIxdl4MhVaDPm1CGdKxgdmRRM7LYrGUbAWav/UY+5MzTU7kGBJP5/Dmj8WF8ZkBranh72VyIhHnpALkYGatPkhsciY1/Dx5ZWh77foSu9alUQ36t6uHTSMyysQwDF74dg95hTa6NqnJzZ31C45IVVEBciA7Es/y7zXFB5S+PKQ9dQK9TU4kcmlP9W2Ju5uF1bGpbDx0yuw4dm3F3hRWxabi6W7hlaHt9AuOSBVSAXIQeYVWnvh6B1abwQ0d6jOwQ32zI4mUSdM6Adx5xe8jMvZpRMYFZOcX8cK3xcNOH762KZF1A01OJOLcVIAcxBsrDxCXmkXtAG8mD25ndhyRcnns3IiMncfSWbIryew4dunNHw+QlJ5HRE1fRvXUsFORqqYC5AC2HjnNf9YXD0KcelN7HRQpDqdOoDcPX1s8IuP15fspKLKZnMi+7D2RwYc/HwbgpRvb4evlbm4gERegAmTncgusPPn1TgwDburcgOvbhJodSaRCHrymCbUDvDl6OofPNh0xO47dsNkMnl28C6vNYED7evRsVdfsSCIuQQXIzr26LJaEk9nUC/Jh0qC2ZscRqTB/bw/GXF+8a2fW6jgy8zQiA+DLzYlsP3oWfy93Jt6gz7hIdVEBsmMbD51i7i+HAZh2c3uCfT3NDSTyN912WQRN6/hzOruA99bGmx3HdCez8pm2tHjY6RN9WlIvWBc1FakuKkB2Kiu/iKfm7wDgjisi6NFSm8XF8Xm4uzGub/GVjT/YEE9yep7Jicw1Zck+MvKKaBsWxD3RjcyOI+JSVIDs1JQf9nHsTC4NQnx5dmAbs+OIVJq+bUPp0qgGeYW2kiseu6JfDp1k4fbjWCzwytD2eGjYqUi10ifODq07kMbnm44C8PqwDgR4e5icSKTyWCwWnhlQvBVo3pZEDqa43oiM/CIrzy3eDcA/ujaiY0SIuYFEXJAKkJ1Jzy3k6QU7Abg3uhHdmtU2OZFI5evSqCZ924a67IiM/6yNJz4tm9oB3jzZt6XZcURckgqQnZn8/V6S0vNoXMuPp/trCrQ4r3H9WuHuZuHHfalsinedERmHT2Yz66c4AJ6/obVObhAxiQqQHflxbwrztx7DYoHpw6Lw89KuL3FezeoEcPvlEQBMWRqLYTj/iAzDMHj+m90UFNm4OrI2N0aFmR1JxGWpANmJM9kFjF+4C4AHr27CZY1rmpxIpOo93rs5fl7u7Eg8yw+7ks2OU+WW7Epi/cGTeHm4MXmIhp2KmEkFyE5M/HYPJ7PyiawbwBN9dEyAuIa6gT48dE1TAF5fHkuh1XlHZGTkFfLSd3sB+FePZjSp7W9yIhHXpgJkB37YlcR3O07g7mZhxrAofDw1B0hcx0PXNqV2gBeHT+XwxW9HzY5TZWauOEBqZj5Navszonszs+OIuDwVIJOdzMovOR32ke7NiNLpsOJiArw9eLxX8YiMt3486JQjMnYeO8t/Nx4GYPLgdvolR8QOqACZyDAMnl20i9PZBbSqF8hj574ERFzN7Vc0pEltf05lF/D+OucakWG1GTy7aDc2AwZ3DOPq5rq0hYg9ML0AzZkzh8aNG+Pj40PXrl357bffLrhsYWEhL730Es2aNcPHx4eoqCiWLVv2l+WOHz/OP/7xD2rVqoWvry/t27dny5YtVbkaFfJNzAmW70nBw83CjFuj8PIw/e0QMYWnuxvjzl0P5/31CaRmOM+IjE9/PcKu4+kE+njw7MDWZscRkXNM/cb96quvGDt2LJMmTWLbtm1ERUXRt29fUlNTz7v8c889x3vvvcesWbPYu3cvI0aMYOjQoWzfvr1kmTNnznDVVVfh6enJ0qVL2bt3LzNmzKBGjRrVtVplkpKRx8Rvind9PdarOW3Dgk1OJGKufu3q0alhCLmFVt748aDZcSpFSkYery/fDxRf96huoIaditgLi2HixTe6du3K5ZdfzuzZswGw2WxERETw6KOPMn78+L8sHxYWxrPPPsvIkSNL7rv55pvx9fXl008/BWD8+PH8/PPPrF+/vsK5MjIyCA4OJj09naCgoAo/z4UYhsH9czfz0/402jcIZuG/uuGpOUAibD58mmHvbsTNAivGXEtk3UCzI/0toz7fxvc7k4iKCGHhI91wd9Np7yJVqTzf36Z96xYUFLB161Z69+79vzBubvTu3ZuNGzee9zH5+fn4+JT+DcrX15cNGzaU/Pnbb7/lsssuY9iwYdStW5dOnTrx/vvvXzRLfn4+GRkZpW5V6estx/hpfxpe7m7MuDVK5UfknMsb1+T6Nr+PyNhvdpy/Ze2BNL7fmYSbBV4Z0k7lR8TOmPbNe/LkSaxWK6GhoaXuDw0NJTn5/BdE69u3LzNnzuTgwYPYbDZWrlzJwoULSUpKKlkmPj6ed955h+bNm7N8+XIeeeQRHnvsMT7++OMLZpk6dSrBwcElt4iIiMpZyfM4fjaXl74vvhbI2D4taBHq2L/hilS2p/u1xM0CK/emsPnwabPjVEheobVkF/d93ZrQroF2cYvYG4fa9PDWW2/RvHlzWrVqhZeXF6NGjWL48OG4uf1vNWw2G507d2bKlCl06tSJhx9+mIceeoh33333gs87YcIE0tPTS26JiYlVkt8wDJ6ev5Os/CI6NwwpuQCciPxPZN1Abru8IQBTftjnkCMy/v1THEdO5VAvyIexfVqYHUdEzsO0AlS7dm3c3d1JSUkpdX9KSgr16tU772Pq1KnD4sWLyc7O5siRI8TGxhIQEEDTpv8rEvXr16dNmzalHte6dWuOHr3wBda8vb0JCgoqdasKn246yoa4k/h4ujF9WJQ2iYtcwJjezfH1dGf70bMs3+NYIzLiUrN4Z+0hACYNakOAt2b6idgj0wqQl5cXXbp0YdWqVSX32Ww2Vq1aRXR09EUf6+PjQ4MGDSgqKmLBggUMHjy45GdXXXUV+/eXPnbgwIEDNGrUqHJXoAI6hofQIjSAcX1b0bROgNlxROxW3SAfHrymCVB8LJCjjMgwDIPnF++m0GrQs2Ud+rU7/y9zImI+U3eBjR07lvfff5+PP/6Yffv28cgjj5Cdnc3w4cMBuOeee5gwYULJ8ps2bWLhwoXEx8ezfv16+vXrh81mY9y4cSXLjBkzhl9//ZUpU6YQFxfH559/zn/+859SZ46ZpX14MN89ejX3dWtsdhQRu/fwtU2p5e9FwslsvtxcNbulK9vimONsjD+Ft4cbLw3WsFMRe2ZqAbrtttuYPn06EydOpGPHjsTExLBs2bKSA6OPHj1a6gDnvLw8nnvuOdq0acPQoUNp0KABGzZsICQkpGSZyy+/nEWLFvHFF1/Qrl07Jk+ezJtvvsldd91V3at3Xt4e7rhp15fIJQX6eJZcHf2tHw+QlV9kcqKLS88p5OXv9wHF1/aKqOlnciIRuRhTrwNkr6r6OkAiUjYFRTb6vLGWw6dyeLxXc8Zcb78HFD+zaBefbzpKZN0AfnjsGl3ZXcQEDnEdIBGRS/HycOOpvq0AeH99PKmZ9jkiY+uRM3y+qfhEi1eGtFP5EXEA+pSKiF0b0L4eUREh5BRYecsOR2QUWW08u2gXALd0Cadr01omJxKRslABEhG7ZrFYeKZ/8VagLzcncigty+REpc395TCxyZmE+Hky4VxOEbF/KkAiYve6Nq1Fr1Z1sdoMXlsWa3acEifO5jJz5QEAJvRvRa0Ab5MTiUhZqQCJiEN4un8r3CywfE8KW4/Yx4iMF7/bQ06Blcsa1WBYl6oboSMilU8FSEQcQovQwJKSMeWHWNNHZPy4N4Xle1LwcLPw8tB2uryFiINRARIRhzHm+hb4eLqx9cgZVuxNufQDqkhOQRGTvt0DwAPXNKFVPV0uQ8TRqACJiMOoF+zDA1f/PiIjliKTRmS8vSqO42dzaRDiy+PnLtYoIo5FBUhEHMo/uzejpr8X8WnZfLWl+kdk7E/O5IP18QC8eGNb/Lw07FTEEakAiYhDCfLx5NHrIgF4Y+VBsqtxRIbNZvDc4l0U2Qz6tAmld5vQanttEalcKkAi4nDu6tqIhjX9OJmVzwfrE6rtdedvPcbmw2fw83Jn0o1tq+11RaTyqQCJiMMpHpHREoD31h0iLTO/yl/zdHYBU5YWDzsd07sFDUJ8q/w1RaTqqACJiEMa2L4+HcKDySmw8vaqqh+RMfWHfZzNKaRVvUDuu6pxlb+eiFQtFSARcUhubhbGnxs98cVvR4mvwhEZm+JP8fXWYwC8MrQ9nu76p1PE0elTLCIOq1uz2vRsWYcim8Hry/dXyWsUFNl4bvFuAO64oiFdGtWoktcRkeqlAiQiDm18/9a4WWDp7mS2HT1T6c//wYZ4DqZmUcvfi6f7taz05xcRc6gAiYhDa1kvkJs7hwPFx+lU5oiMxNM5JccXPTuwNSF+XpX23CJiLhUgEXF4Y/u0wNvDjc2Hz/DjvtRKeU7DMJj4zW7yCm1c2bQmQzs1qJTnFRH7oAIkIg6vfrAv958bkTFt6b5KGZGxfE8yP+1Pw9PdwstD2mOxaNipiDNRARIRp/BIj2bU8PPkUFp2yRlbFZWVX8QL3+4FYET3ZkTWDaiMiCJiR1SARMQpBPl4Muq64sGkM1ceIKeg4iMy3lh5gOSMPBrW9GNkz8jKiigidkQFSEScxj+ubEh4DV/SMvP5vwqOyNh9PJ2Pfi5+7EuD2+Lj6V6ZEUXETqgAiYjT8PZw/8OIjHhOZpVvRIbVZvDs4t3YDBjYoT49WtatipgiYgdUgETEqQzqEEb7BsFk5Rcxq5wjMr747Sg7Es8S4O3BxBvaVFFCEbEHKkAi4lTc3CxMODci47NNRzl8MrtMj0vLzOfVZbEAPNmnBaFBPlWWUUTMpwIkIk6nW2Rturco34iMV5bsJTOviPYNgrk7unHVBhQR06kAiYhTGt+/FRYLLNmVxPZLjMj4Oe4ki2NOYLHAK0Pb4e6ma/6IODsVIBFxSq3rB3FTp3MjMpbGXnBERl6htWTY6T1XNqJDeEh1RRQRE6kAiYjTeqJPC7w83Pgt4TSrY88/IuO9tfEknMymTqA3T/TVsFMRV6ECJCJOKyzEl+FXNQZg2tLYv4zISDiZzZw1cQBMvKENQT6e1R1RREyiAiQiTu1f3SMJ9vXkYGoWC7b9b0TG78NOC4psXNO8Njd0qG9iShGpbipAIuLUgv08efS64nEWM1ceILfACsB3O5NYf/AkXh5uTB7cTsNORVyMCpCIOL27oxvRIMSXlIx8Pvw5gfTcQiZ/XzzsdFTPSBrX9jc5oYhUNxUgEXF6fxyR8c6aQ0z8Zjdpmfk0re3PP7s3NTmdiJhBBUhEXMKNUWG0DQsiK7+Ib2JOAPDykHZ4e2jYqYgrUgESEZdQPCKjdcmfh3ZqQLfI2iYmEhEzqQCJiMu4unltbr88gnYNgnhmQOtLP0BEnJaH2QFERKrTtJs7mB1BROyAtgCJiIiIy1EBEhEREZejAiQiIiIuRwVIREREXI4KkIiIiLgcFSARERFxOSpAIiIi4nJUgERERMTlqACJiIiIy1EBEhEREZejAiQiIiIuRwVIREREXI4KkIiIiLgcFSARERFxOR5mB7BHhmEAkJGRYXISERERKavfv7d//x6/GBWg88jMzAQgIiLC5CQiIiJSXpmZmQQHB190GYtRlprkYmw2GydOnCAwMBCLxVKpz52RkUFERASJiYkEBQVV6nPbA62f43P2dXT29QPnX0etn+OrqnU0DIPMzEzCwsJwc7v4UT7aAnQebm5uhIeHV+lrBAUFOe1fbND6OQNnX0dnXz9w/nXU+jm+qljHS235+Z0OghYRERGXowIkIiIiLkcFqJp5e3szadIkvL29zY5SJbR+js/Z19HZ1w+cfx21fo7PHtZRB0GLiIiIy9EWIBEREXE5KkAiIiLiclSARERExOWoAImIiIjLUQGqAnPmzKFx48b4+PjQtWtXfvvtt4su//XXX9OqVSt8fHxo3749P/zwQzUlrZjyrN/cuXOxWCylbj4+PtWYtnzWrVvHoEGDCAsLw2KxsHjx4ks+Zs2aNXTu3Blvb28iIyOZO3duleesqPKu35o1a/7y/lksFpKTk6sncDlNnTqVyy+/nMDAQOrWrcuQIUPYv3//JR/nSJ/BiqyjI30O33nnHTp06FBygbzo6GiWLl160cc40vtX3vVzpPfufKZNm4bFYmH06NEXXc6M91AFqJJ99dVXjB07lkmTJrFt2zaioqLo27cvqamp513+l19+4Y477uCBBx5g+/btDBkyhCFDhrB79+5qTl425V0/KL7SZ1JSUsntyJEj1Zi4fLKzs4mKimLOnDllWj4hIYGBAwfSs2dPYmJiGD16NA8++CDLly+v4qQVU971+93+/ftLvYd169atooR/z9q1axk5ciS//vorK1eupLCwkD59+pCdnX3BxzjaZ7Ai6wiO8zkMDw9n2rRpbN26lS1btnDdddcxePBg9uzZc97lHe39K+/6geO8d3+2efNm3nvvPTp06HDR5Ux7Dw2pVFdccYUxcuTIkj9brVYjLCzMmDp16nmXv/XWW42BAweWuq9r167GP//5zyrNWVHlXb+PPvrICA4OrqZ0lQswFi1adNFlxo0bZ7Rt27bUfbfddpvRt2/fKkxWOcqyfj/99JMBGGfOnKmWTJUtNTXVAIy1a9decBlH+wz+WVnW0ZE/h4ZhGDVq1DA++OCD8/7M0d8/w7j4+jnqe5eZmWk0b97cWLlypdG9e3fj8ccfv+CyZr2H2gJUiQoKCti6dSu9e/cuuc/NzY3evXuzcePG8z5m48aNpZYH6Nu37wWXN1NF1g8gKyuLRo0aERERccnfdByNI71/f0fHjh2pX78+119/PT///LPZccosPT0dgJo1a15wGUd/D8uyjuCYn0Or1cqXX35JdnY20dHR513Gkd+/sqwfOOZ7N3LkSAYOHPiX9+Z8zHoPVYAq0cmTJ7FarYSGhpa6PzQ09ILHTCQnJ5dreTNVZP1atmzJhx9+yDfffMOnn36KzWajW7duHDt2rDoiV7kLvX8ZGRnk5uaalKry1K9fn3fffZcFCxawYMECIiIi6NGjB9u2bTM72iXZbDZGjx7NVVddRbt27S64nCN9Bv+srOvoaJ/DXbt2ERAQgLe3NyNGjGDRokW0adPmvMs64vtXnvVztPcO4Msvv2Tbtm1MnTq1TMub9R5qGrxUqejo6FK/2XTr1o3WrVvz3nvvMXnyZBOTSVm0bNmSli1blvy5W7duHDp0iDfeeINPPvnExGSXNnLkSHbv3s2GDRvMjlJlyrqOjvY5bNmyJTExMaSnpzN//nzuvfde1q5de8GS4GjKs36O9t4lJiby+OOPs3LlSrs/WFsFqBLVrl0bd3d3UlJSSt2fkpJCvXr1zvuYevXqlWt5M1Vk/f7M09OTTp06ERcXVxURq92F3r+goCB8fX1NSlW1rrjiCrsvFaNGjeL7779n3bp1hIeHX3RZR/oM/lF51vHP7P1z6OXlRWRkJABdunRh8+bNvPXWW7z33nt/WdYR37/yrN+f2ft7t3XrVlJTU+ncuXPJfVarlXXr1jF79mzy8/Nxd3cv9Riz3kPtAqtEXl5edOnShVWrVpXcZ7PZWLVq1QX370ZHR5daHmDlypUX3R9sloqs359ZrVZ27dpF/fr1qypmtXKk96+yxMTE2O37ZxgGo0aNYtGiRaxevZomTZpc8jGO9h5WZB3/zNE+hzabjfz8/PP+zNHev/O52Pr9mb2/d7169WLXrl3ExMSU3C677DLuuusuYmJi/lJ+wMT3sEoPsXZBX375peHt7W3MnTvX2Lt3r/Hwww8bISEhRnJysmEYhnH33Xcb48ePL1n+559/Njw8PIzp06cb+/btMyZNmmR4enoau3btMmsVLqq86/fiiy8ay5cvNw4dOmRs3brVuP322w0fHx9jz549Zq3CRWVmZhrbt283tm/fbgDGzJkzje3btxtHjhwxDMMwxo8fb9x9990ly8fHxxt+fn7GU089Zezbt8+YM2eO4e7ubixbtsysVbio8q7fG2+8YSxevNg4ePCgsWvXLuPxxx833NzcjB9//NGsVbioRx55xAgODjbWrFljJCUlldxycnJKlnH0z2BF1tGRPofjx4831q5dayQkJBg7d+40xo8fb1gsFmPFihWGYTj++1fe9XOk9+5C/nwWmL28hypAVWDWrFlGw4YNDS8vL+OKK64wfv3115Kfde/e3bj33ntLLT9v3jyjRYsWhpeXl9G2bVtjyZIl1Zy4fMqzfqNHjy5ZNjQ01BgwYICxbds2E1KXze+nff/59vs63XvvvUb37t3/8piOHTsaXl5eRtOmTY2PPvqo2nOXVXnX79VXXzWaNWtm+Pj4GDVr1jR69OhhrF692pzwZXC+dQNKvSeO/hmsyDo60ufw/vvvNxo1amR4eXkZderUMXr16lVSDgzD8d+/8q6fI713F/LnAmQv76HFMAyjarcxiYiIiNgXHQMkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5agAiYiIiMtRARIRERGXowIkInavR48ejB492uwYIuJEVIBERETE5agAiYhcQkFBgdkRRKSSqQCJiEP55JNPuOyyywgMDKRevXrceeedpKamAmAYBpGRkUyfPr3UY2JiYrBYLMTFxQFw9uxZHnzwQerUqUNQUBDXXXcdO3bsKFn+hRdeoGPHjnzwwQc0adIEHx8fAObPn0/79u3x9fWlVq1a9O7dm+zs7GpacxGpTCpAIuJQCgsLmTx5Mjt27GDx4sUcPnyY++67DwCLxcL999/PRx99VOoxH330Eddeey2RkZEADBs2jNTUVJYuXcrWrVvp3LkzvXr14vTp0yWPiYuLY8GCBSxcuJCYmBiSkpK44447uP/++9m3bx9r1qzhpptuQvOkRRyTpsGLiN3r0aMHHTt25M033/zLz7Zs2cLll19OZmYmAQEBnDhxgoYNG/LLL79wxRVXUFhYSFhYGNOnT+fee+9lw4YNDBw4kNTUVLy9vUueJzIyknHjxvHwww/zwgsvMGXKFI4fP06dOnUA2LZtG126dOHw4cM0atSoulZdRKqItgCJiEPZunUrgwYNomHDhgQGBtK9e3cAjh49CkBYWBgDBw7kww8/BOC7774jPz+fYcOGAbBjxw6ysrKoVasWAQEBJbeEhAQOHTpU8jqNGjUqKT8AUVFR9OrVi/bt2zNs2DDef/99zpw5U12rLSKVTAVIRBxGdnY2ffv2JSgoiM8++4zNmzezaNEioPSByg8++CBffvklubm5fPTRR9x22234+fkBkJWVRf369YmJiSl1279/P0899VTJc/j7+5d6bXd3d1auXMnSpUtp06YNs2bNomXLliQkJFTDmotIZfMwO4CISFnFxsZy6tQppk2bRkREBFC8C+zPBgwYgL+/P++88w7Lli1j3bp1JT/r3LkzycnJeHh40Lhx43K9vsVi4aqrruKqq65i4sSJNGrUiEWLFjF27Ni/tV4iUv20BUhEHEbDhg3x8vJi1qxZxMfH8+233zJ58uS/LOfu7s59993HhAkTaN68OdHR0SU/6927N9HR0QwZMoQVK1Zw+PBhfvnlF5599tnzlqnfbdq0iSlTprBlyxaOHj3KwoULSUtLo3Xr1lWyriJStVSARMRh1KlTh7lz5/L111/Tpk0bpk2b9pdT3n/3wAMPUFBQwPDhw0vdb7FY+OGHH7j22msZPnw4LVq04Pbbb+fIkSOEhoZe8LWDgoJYt24dAwYMoEWLFjz33HPMmDGD/v37V+o6ikj10FlgIuKU1q9fT69evUhMTLxosRER16QCJCJOJT8/n7S0NO69917q1avHZ599ZnYkEbFD2gUmIk7liy++oFGjRpw9e5bXXnvN7DgiYqe0BUhERERcjrYAiYiIiMtRARIRERGXowIkIiIiLkcFSERERFyOCpCIiIi4HBUgERERcTkqQCIiIuJyVIBERETE5fw/mrk5Z6jJCvcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0, 5):\n",
    "    history_layers = tune_model_layers(i)\n",
    "    results.append({\n",
    "            \"layers\": i,\n",
    "            \"loss\": min(history_layers.history[\"loss\"]),\n",
    "            \"accuracy\": max(history_layers.history[\"accuracy\"]),\n",
    "        })\n",
    "    \n",
    "results_layers = pd.DataFrame(results)\n",
    "sns.lineplot(x=\"layers\", y=\"loss\", data=results_layers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_LSTM(lstm_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "\n",
    "    for i in range(2):\n",
    "        model.add(LSTM(lstm_size, return_sequences=True))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(LSTM(lstm_size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        x,\n",
    "        labels_encoded,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=10, min_delta=0.0001, restore_best_weights=True\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "718/718 [==============================] - 13s 14ms/step - loss: 5.4908 - accuracy: 0.0796\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 5.0120 - accuracy: 0.0992\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 4.7394 - accuracy: 0.1314\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 4.4949 - accuracy: 0.1631\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 4.2709 - accuracy: 0.2031\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 4.0637 - accuracy: 0.2319\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.8879 - accuracy: 0.2525\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.7611 - accuracy: 0.2641\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.6434 - accuracy: 0.2766\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.5431 - accuracy: 0.2924\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.4515 - accuracy: 0.3062\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.3765 - accuracy: 0.3160\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.3015 - accuracy: 0.3280\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.2382 - accuracy: 0.3370\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.1746 - accuracy: 0.3438\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.1221 - accuracy: 0.3514\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 3.0694 - accuracy: 0.3569\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 3.0250 - accuracy: 0.3635\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.9926 - accuracy: 0.3657\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.9507 - accuracy: 0.3729\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 9s 12ms/step - loss: 2.9219 - accuracy: 0.3743\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.8837 - accuracy: 0.3761\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.8541 - accuracy: 0.3842\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.8280 - accuracy: 0.3890\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.7984 - accuracy: 0.3927\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.7779 - accuracy: 0.3943\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.7542 - accuracy: 0.3956\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.7311 - accuracy: 0.4026\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.7074 - accuracy: 0.4045\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.6902 - accuracy: 0.4070\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.6633 - accuracy: 0.4097\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.6613 - accuracy: 0.4106\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.6333 - accuracy: 0.4155\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.6175 - accuracy: 0.4151\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.6014 - accuracy: 0.4190\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.5968 - accuracy: 0.4193\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.5798 - accuracy: 0.4224\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.5625 - accuracy: 0.4237\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.5555 - accuracy: 0.4249\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.5412 - accuracy: 0.4261\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.5303 - accuracy: 0.4271\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.5137 - accuracy: 0.4324\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.5054 - accuracy: 0.4329\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4974 - accuracy: 0.4344\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.4802 - accuracy: 0.4351\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.4674 - accuracy: 0.4393\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.4625 - accuracy: 0.4407\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4505 - accuracy: 0.4422\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.4439 - accuracy: 0.4425\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4310 - accuracy: 0.4452\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4256 - accuracy: 0.4438\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.4168 - accuracy: 0.4431\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4065 - accuracy: 0.4469\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3998 - accuracy: 0.4481\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3917 - accuracy: 0.4484\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3821 - accuracy: 0.4489\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.3734 - accuracy: 0.4526\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3683 - accuracy: 0.4527\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3647 - accuracy: 0.4529\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.3511 - accuracy: 0.4538\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.3410 - accuracy: 0.4544\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.3370 - accuracy: 0.4575\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3390 - accuracy: 0.4543\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3202 - accuracy: 0.4594\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3238 - accuracy: 0.4591\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3083 - accuracy: 0.4622\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.3070 - accuracy: 0.4628\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.3035 - accuracy: 0.4620\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 2.2999 - accuracy: 0.4627\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2863 - accuracy: 0.4656\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2831 - accuracy: 0.4648\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.2810 - accuracy: 0.4652\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.2730 - accuracy: 0.4678\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.2686 - accuracy: 0.4662\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.2562 - accuracy: 0.4692\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.2467 - accuracy: 0.4702\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2527 - accuracy: 0.4708\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2479 - accuracy: 0.4720\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.2368 - accuracy: 0.4714\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2362 - accuracy: 0.4723\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 2.2335 - accuracy: 0.4747\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2236 - accuracy: 0.4731\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2248 - accuracy: 0.4742\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2146 - accuracy: 0.4739\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2179 - accuracy: 0.4751\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2207 - accuracy: 0.4729\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2039 - accuracy: 0.4769\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2049 - accuracy: 0.4751\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1931 - accuracy: 0.4807\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2003 - accuracy: 0.4797\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1893 - accuracy: 0.4795\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1848 - accuracy: 0.4774\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.1801 - accuracy: 0.4821\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.1818 - accuracy: 0.4816\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.1744 - accuracy: 0.4825\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 2.1672 - accuracy: 0.4848\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1672 - accuracy: 0.4825\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.1611 - accuracy: 0.4880\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.1537 - accuracy: 0.4871\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 9s 13ms/step - loss: 2.1541 - accuracy: 0.4855\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 14s 14ms/step - loss: 5.3166 - accuracy: 0.0937\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 4.6088 - accuracy: 0.1646\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 4.0900 - accuracy: 0.2323\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 3.7572 - accuracy: 0.2871\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 3.5163 - accuracy: 0.3226\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 3.3169 - accuracy: 0.3469\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 3.1371 - accuracy: 0.3689\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.9889 - accuracy: 0.3843\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.8475 - accuracy: 0.4022\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.7371 - accuracy: 0.4139\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.6364 - accuracy: 0.4260\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.5500 - accuracy: 0.4357\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4788 - accuracy: 0.4454\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.4149 - accuracy: 0.4524\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3542 - accuracy: 0.4643\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.3020 - accuracy: 0.4688\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2550 - accuracy: 0.4789\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.2038 - accuracy: 0.4871\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1696 - accuracy: 0.4919\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1347 - accuracy: 0.4964\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.1001 - accuracy: 0.5009\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.0712 - accuracy: 0.5061\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.0408 - accuracy: 0.5098\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 2.0079 - accuracy: 0.5169\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.9888 - accuracy: 0.5199\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.9661 - accuracy: 0.5240\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.9422 - accuracy: 0.5288\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.9273 - accuracy: 0.5290\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.9009 - accuracy: 0.5348\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.8837 - accuracy: 0.5385\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.8716 - accuracy: 0.5393\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.8481 - accuracy: 0.5440\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.8300 - accuracy: 0.5470\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.8189 - accuracy: 0.5502\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7982 - accuracy: 0.5522\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7906 - accuracy: 0.5523\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7830 - accuracy: 0.5555\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.7619 - accuracy: 0.5578\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7469 - accuracy: 0.5608\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 10s 15ms/step - loss: 1.7485 - accuracy: 0.5587\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7281 - accuracy: 0.5636\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7213 - accuracy: 0.5654\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.7064 - accuracy: 0.5670\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6958 - accuracy: 0.5711\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6867 - accuracy: 0.5727\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6799 - accuracy: 0.5732\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6693 - accuracy: 0.5761\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6603 - accuracy: 0.5779\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6540 - accuracy: 0.5777\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6465 - accuracy: 0.5795\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6417 - accuracy: 0.5802\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6389 - accuracy: 0.5812\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6313 - accuracy: 0.5847\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6265 - accuracy: 0.5847\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6160 - accuracy: 0.5858\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5999 - accuracy: 0.5893\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.6020 - accuracy: 0.5903\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5974 - accuracy: 0.5879\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.5866 - accuracy: 0.5920\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5879 - accuracy: 0.5900\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5759 - accuracy: 0.5931\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5685 - accuracy: 0.5945\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5697 - accuracy: 0.5930\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5632 - accuracy: 0.5941\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.5590 - accuracy: 0.5955\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.5516 - accuracy: 0.5988\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5485 - accuracy: 0.5986\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.5349 - accuracy: 0.6028\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5358 - accuracy: 0.6009\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5315 - accuracy: 0.6015\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5261 - accuracy: 0.6000\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5263 - accuracy: 0.6030\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5201 - accuracy: 0.6044\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5175 - accuracy: 0.6043\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 10s 13ms/step - loss: 1.5081 - accuracy: 0.6066\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5064 - accuracy: 0.6069\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.5048 - accuracy: 0.6068\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.5013 - accuracy: 0.6081\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4917 - accuracy: 0.6085\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.4980 - accuracy: 0.6074\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4941 - accuracy: 0.6080\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4828 - accuracy: 0.6105\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4784 - accuracy: 0.6124\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4703 - accuracy: 0.6139\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4756 - accuracy: 0.6112\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4718 - accuracy: 0.6129\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4709 - accuracy: 0.6112\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4637 - accuracy: 0.6141\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4665 - accuracy: 0.6156\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.4585 - accuracy: 0.6138\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.4505 - accuracy: 0.6168\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.4489 - accuracy: 0.6157\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4499 - accuracy: 0.6170\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4487 - accuracy: 0.6185\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4411 - accuracy: 0.6188\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4377 - accuracy: 0.6225\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4343 - accuracy: 0.6203\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 10s 14ms/step - loss: 1.4376 - accuracy: 0.6195\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.4342 - accuracy: 0.6194\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 11s 15ms/step - loss: 1.4270 - accuracy: 0.6200\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 17s 19ms/step - loss: 5.1714 - accuracy: 0.1161\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 4.0687 - accuracy: 0.2422\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 3.4457 - accuracy: 0.3284\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 3.0400 - accuracy: 0.3738\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.7566 - accuracy: 0.4119\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.5320 - accuracy: 0.4414\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.3647 - accuracy: 0.4638\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.2191 - accuracy: 0.4842\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.1009 - accuracy: 0.5050\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.9989 - accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.9132 - accuracy: 0.5347\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.8372 - accuracy: 0.5473\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.7815 - accuracy: 0.5569\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.7197 - accuracy: 0.5685\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6768 - accuracy: 0.5763\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6302 - accuracy: 0.5832\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6024 - accuracy: 0.5881\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.5605 - accuracy: 0.5985\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.5377 - accuracy: 0.6028\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4964 - accuracy: 0.6100\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4767 - accuracy: 0.6149\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4568 - accuracy: 0.6183\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4293 - accuracy: 0.6246\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4148 - accuracy: 0.6292\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3972 - accuracy: 0.6289\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3797 - accuracy: 0.6362\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3722 - accuracy: 0.6322\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3514 - accuracy: 0.6382\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3366 - accuracy: 0.6414\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3245 - accuracy: 0.6443\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3093 - accuracy: 0.6483\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3058 - accuracy: 0.6490\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2894 - accuracy: 0.6516\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2889 - accuracy: 0.6521\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2786 - accuracy: 0.6563\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2656 - accuracy: 0.6570\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2565 - accuracy: 0.6598\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2489 - accuracy: 0.6600\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2481 - accuracy: 0.6597\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2378 - accuracy: 0.6613\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2313 - accuracy: 0.6630\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2192 - accuracy: 0.6664\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2103 - accuracy: 0.6676\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2036 - accuracy: 0.6677\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2025 - accuracy: 0.6706\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1982 - accuracy: 0.6691\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1894 - accuracy: 0.6723\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1905 - accuracy: 0.6731\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1873 - accuracy: 0.6716\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1770 - accuracy: 0.6755\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1832 - accuracy: 0.6725\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1734 - accuracy: 0.6762\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1693 - accuracy: 0.6763\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1664 - accuracy: 0.6765\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1594 - accuracy: 0.6773\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1578 - accuracy: 0.6790\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1545 - accuracy: 0.6792\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1496 - accuracy: 0.6792\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1486 - accuracy: 0.6806\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1406 - accuracy: 0.6818\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1334 - accuracy: 0.6860\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1384 - accuracy: 0.6824\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1301 - accuracy: 0.6820\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1296 - accuracy: 0.6846\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1255 - accuracy: 0.6843\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1265 - accuracy: 0.6847\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1233 - accuracy: 0.6868\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1211 - accuracy: 0.6864\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1129 - accuracy: 0.6885\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1101 - accuracy: 0.6886\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1113 - accuracy: 0.6888\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1058 - accuracy: 0.6901\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1057 - accuracy: 0.6892\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1083 - accuracy: 0.6881\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1001 - accuracy: 0.6885\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0999 - accuracy: 0.6903\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0956 - accuracy: 0.6902\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0961 - accuracy: 0.6919\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0935 - accuracy: 0.6888\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0881 - accuracy: 0.6928\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0878 - accuracy: 0.6917\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0900 - accuracy: 0.6923\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0882 - accuracy: 0.6936\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0803 - accuracy: 0.6946\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0823 - accuracy: 0.6936\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0811 - accuracy: 0.6939\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0775 - accuracy: 0.6934\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0741 - accuracy: 0.6964\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0769 - accuracy: 0.6951\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0726 - accuracy: 0.6972\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0717 - accuracy: 0.6968\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0777 - accuracy: 0.6937\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0668 - accuracy: 0.6956\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0689 - accuracy: 0.6956\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0654 - accuracy: 0.6961\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0657 - accuracy: 0.6964\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0612 - accuracy: 0.6994\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0606 - accuracy: 0.6986\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0637 - accuracy: 0.6978\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0572 - accuracy: 0.6994\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 16s 17ms/step - loss: 5.5056 - accuracy: 0.0789\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.4402 - accuracy: 0.0795\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.4351 - accuracy: 0.0817\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.4311 - accuracy: 0.0813\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.4281 - accuracy: 0.0822\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.1219 - accuracy: 0.1089\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 4.1499 - accuracy: 0.2027\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 3.6682 - accuracy: 0.2510\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 3.3529 - accuracy: 0.2874\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 3.1024 - accuracy: 0.3194\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.8577 - accuracy: 0.3563\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.6510 - accuracy: 0.3858\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.4823 - accuracy: 0.4133\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.3364 - accuracy: 0.4355\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.2164 - accuracy: 0.4552\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.1087 - accuracy: 0.4766\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.0232 - accuracy: 0.4910\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.9402 - accuracy: 0.5061\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.8738 - accuracy: 0.5189\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.8095 - accuracy: 0.5318\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.7557 - accuracy: 0.5434\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.7046 - accuracy: 0.5536\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.6605 - accuracy: 0.5638\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.6264 - accuracy: 0.5688\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.5907 - accuracy: 0.5768\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.5569 - accuracy: 0.5859\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.5302 - accuracy: 0.5906\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4982 - accuracy: 0.5986\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4779 - accuracy: 0.6019\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4559 - accuracy: 0.6076\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4326 - accuracy: 0.6113\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.4167 - accuracy: 0.6152\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3934 - accuracy: 0.6217\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3710 - accuracy: 0.6269\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3589 - accuracy: 0.6275\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.3482 - accuracy: 0.6302\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3214 - accuracy: 0.6371\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3129 - accuracy: 0.6364\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.3048 - accuracy: 0.6402\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2876 - accuracy: 0.6446\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2802 - accuracy: 0.6449\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2690 - accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2589 - accuracy: 0.6510\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2478 - accuracy: 0.6512\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2365 - accuracy: 0.6562\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2282 - accuracy: 0.6566\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.2200 - accuracy: 0.6608\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.2107 - accuracy: 0.6618\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2052 - accuracy: 0.6615\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1978 - accuracy: 0.6630\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1885 - accuracy: 0.6650\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1798 - accuracy: 0.6696\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1789 - accuracy: 0.6664\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1670 - accuracy: 0.6709\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 1.1667 - accuracy: 0.6698\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1627 - accuracy: 0.6743\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1582 - accuracy: 0.6730\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1451 - accuracy: 0.6773\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1432 - accuracy: 0.6783\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1381 - accuracy: 0.6757\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1318 - accuracy: 0.6815\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1272 - accuracy: 0.6811\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1258 - accuracy: 0.6823\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1207 - accuracy: 0.6817\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1116 - accuracy: 0.6846\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1092 - accuracy: 0.6848\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1061 - accuracy: 0.6852\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.1021 - accuracy: 0.6846\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0939 - accuracy: 0.6882\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0977 - accuracy: 0.6874\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0899 - accuracy: 0.6872\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0898 - accuracy: 0.6889\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0841 - accuracy: 0.6902\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0814 - accuracy: 0.6912\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.0839 - accuracy: 0.6899\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0741 - accuracy: 0.6917\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0734 - accuracy: 0.6906\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0699 - accuracy: 0.6916\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0681 - accuracy: 0.6936\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0614 - accuracy: 0.6934\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0614 - accuracy: 0.6955\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0681 - accuracy: 0.6933\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0573 - accuracy: 0.6954\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0571 - accuracy: 0.6938\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0532 - accuracy: 0.6973\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0502 - accuracy: 0.6974\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0520 - accuracy: 0.6944\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0423 - accuracy: 0.6984\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0440 - accuracy: 0.6964\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0443 - accuracy: 0.6986\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0388 - accuracy: 0.7005\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0374 - accuracy: 0.6990\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 13s 17ms/step - loss: 1.0377 - accuracy: 0.6994\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0361 - accuracy: 0.7003\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0292 - accuracy: 0.7008\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0266 - accuracy: 0.7038\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0296 - accuracy: 0.7021\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0277 - accuracy: 0.7023\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0223 - accuracy: 0.7031\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 13s 17ms/step - loss: 1.0258 - accuracy: 0.7013\n",
      "Epoch 1/100\n",
      "718/718 [==============================] - 18s 20ms/step - loss: 5.5070 - accuracy: 0.0790\n",
      "Epoch 2/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 5.4436 - accuracy: 0.0796\n",
      "Epoch 3/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 5.4393 - accuracy: 0.0812\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 5.4373 - accuracy: 0.0820\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 5.4323 - accuracy: 0.0807\n",
      "Epoch 6/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 5.4302 - accuracy: 0.0828\n",
      "Epoch 7/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 5.4284 - accuracy: 0.0814\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 5.4245 - accuracy: 0.0821\n",
      "Epoch 9/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 5.4044 - accuracy: 0.0817\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 4.5937 - accuracy: 0.1593\n",
      "Epoch 11/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 3.9072 - accuracy: 0.2262\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 3.5020 - accuracy: 0.2640\n",
      "Epoch 13/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 3.2009 - accuracy: 0.2975\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.9663 - accuracy: 0.3261\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 2.7893 - accuracy: 0.3480\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.6524 - accuracy: 0.3655\n",
      "Epoch 17/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 2.5312 - accuracy: 0.3881\n",
      "Epoch 18/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.4290 - accuracy: 0.4041\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 2.3408 - accuracy: 0.4209\n",
      "Epoch 20/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.2676 - accuracy: 0.4347\n",
      "Epoch 21/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 2.2130 - accuracy: 0.4440\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 2.1626 - accuracy: 0.4564\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 2.1153 - accuracy: 0.4645\n",
      "Epoch 24/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 2.0692 - accuracy: 0.4743\n",
      "Epoch 25/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 2.0305 - accuracy: 0.4806\n",
      "Epoch 26/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.9966 - accuracy: 0.4890\n",
      "Epoch 27/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.9591 - accuracy: 0.4958\n",
      "Epoch 28/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.9356 - accuracy: 0.4999\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.8994 - accuracy: 0.5088\n",
      "Epoch 30/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.8745 - accuracy: 0.5146\n",
      "Epoch 31/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.8459 - accuracy: 0.5209\n",
      "Epoch 32/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.8190 - accuracy: 0.5254\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.7935 - accuracy: 0.5284\n",
      "Epoch 34/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.7619 - accuracy: 0.5353\n",
      "Epoch 35/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.7422 - accuracy: 0.5400\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.7189 - accuracy: 0.5448\n",
      "Epoch 37/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.6973 - accuracy: 0.5490\n",
      "Epoch 38/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.6737 - accuracy: 0.5525\n",
      "Epoch 39/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.6603 - accuracy: 0.5586\n",
      "Epoch 40/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.6352 - accuracy: 0.5602\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.6178 - accuracy: 0.5659\n",
      "Epoch 42/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.6014 - accuracy: 0.5695\n",
      "Epoch 43/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5942 - accuracy: 0.5731\n",
      "Epoch 44/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5765 - accuracy: 0.5773\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5642 - accuracy: 0.5781\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5460 - accuracy: 0.5855\n",
      "Epoch 47/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.5325 - accuracy: 0.5852\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5145 - accuracy: 0.5888\n",
      "Epoch 49/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.5075 - accuracy: 0.5909\n",
      "Epoch 50/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.4948 - accuracy: 0.5931\n",
      "Epoch 51/100\n",
      "718/718 [==============================] - 15s 21ms/step - loss: 1.4822 - accuracy: 0.5971\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.4733 - accuracy: 0.6004\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.4631 - accuracy: 0.6014\n",
      "Epoch 54/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.4519 - accuracy: 0.6039\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.4408 - accuracy: 0.6057\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.4326 - accuracy: 0.6083\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.4219 - accuracy: 0.6110\n",
      "Epoch 58/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.4048 - accuracy: 0.6145\n",
      "Epoch 59/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.4088 - accuracy: 0.6144\n",
      "Epoch 60/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.4003 - accuracy: 0.6161\n",
      "Epoch 61/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3842 - accuracy: 0.6172\n",
      "Epoch 62/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.3773 - accuracy: 0.6202\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3734 - accuracy: 0.6204\n",
      "Epoch 64/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3631 - accuracy: 0.6243\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3514 - accuracy: 0.6252\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3484 - accuracy: 0.6282\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3370 - accuracy: 0.6271\n",
      "Epoch 68/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.3354 - accuracy: 0.6285\n",
      "Epoch 69/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.3262 - accuracy: 0.6325\n",
      "Epoch 70/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3184 - accuracy: 0.6326\n",
      "Epoch 71/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.3094 - accuracy: 0.6344\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.3037 - accuracy: 0.6363\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.3003 - accuracy: 0.6391\n",
      "Epoch 74/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2902 - accuracy: 0.6366\n",
      "Epoch 75/100\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.2935 - accuracy: 0.6368\n",
      "Epoch 76/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2845 - accuracy: 0.6384\n",
      "Epoch 77/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2785 - accuracy: 0.6412\n",
      "Epoch 78/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2711 - accuracy: 0.6445\n",
      "Epoch 79/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2668 - accuracy: 0.6442\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2612 - accuracy: 0.6459\n",
      "Epoch 81/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2606 - accuracy: 0.6456\n",
      "Epoch 82/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2528 - accuracy: 0.6453\n",
      "Epoch 83/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2427 - accuracy: 0.6494\n",
      "Epoch 84/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2423 - accuracy: 0.6503\n",
      "Epoch 85/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2417 - accuracy: 0.6479\n",
      "Epoch 86/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2276 - accuracy: 0.6542\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.2329 - accuracy: 0.6515\n",
      "Epoch 88/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.2230 - accuracy: 0.6535\n",
      "Epoch 89/100\n",
      "718/718 [==============================] - 15s 20ms/step - loss: 1.2176 - accuracy: 0.6553\n",
      "Epoch 90/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.2128 - accuracy: 0.6553\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2141 - accuracy: 0.6552\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 1.2050 - accuracy: 0.6561\n",
      "Epoch 93/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2025 - accuracy: 0.6593\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.2034 - accuracy: 0.6574\n",
      "Epoch 95/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1916 - accuracy: 0.6593\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.1936 - accuracy: 0.6601\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1879 - accuracy: 0.6610\n",
      "Epoch 98/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1867 - accuracy: 0.6630\n",
      "Epoch 99/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1824 - accuracy: 0.6650\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 1.1764 - accuracy: 0.6651\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "lstm_sizes = [32, 64, 128, 256, 512]\n",
    "for i in lstm_sizes:\n",
    "    history_layers = tune_model_LSTM(i)\n",
    "    results.append({\n",
    "            \"units\": i,\n",
    "            \"loss\": min(history_layers.history[\"loss\"]),\n",
    "            \"accuracy\": max(history_layers.history[\"accuracy\"]),\n",
    "        })\n",
    "results_units = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x='units', y=\"loss\", data=results_units)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>2.153675</td>\n",
       "      <td>0.487999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>1.426981</td>\n",
       "      <td>0.622512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>1.057209</td>\n",
       "      <td>0.699395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>1.022263</td>\n",
       "      <td>0.703750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512</td>\n",
       "      <td>1.176413</td>\n",
       "      <td>0.665091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units      loss  accuracy\n",
       "0     32  2.153675  0.487999\n",
       "1     64  1.426981  0.622512\n",
       "2    128  1.057209  0.699395\n",
       "3    256  1.022263  0.703750\n",
       "4    512  1.176413  0.665091"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    for i in range(2):\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 5.5033 - accuracy: 0.0788\n",
      "Epoch 1: loss improved from inf to 5.50303, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 16s 18ms/step - loss: 5.5030 - accuracy: 0.0788\n",
      "Epoch 2/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 5.0671 - accuracy: 0.1165\n",
      "Epoch 2: loss improved from 5.50303 to 5.06666, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 5.0667 - accuracy: 0.1165\n",
      "Epoch 3/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 3.7472 - accuracy: 0.2742\n",
      "Epoch 3: loss improved from 5.06666 to 3.74549, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 3.7455 - accuracy: 0.2746\n",
      "Epoch 4/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 3.0388 - accuracy: 0.3602\n",
      "Epoch 4: loss improved from 3.74549 to 3.03883, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 3.0388 - accuracy: 0.3602\n",
      "Epoch 5/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 2.6026 - accuracy: 0.4204\n",
      "Epoch 5: loss improved from 3.03883 to 2.60265, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 2.6026 - accuracy: 0.4204\n",
      "Epoch 6/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 2.2941 - accuracy: 0.4657\n",
      "Epoch 6: loss improved from 2.60265 to 2.29429, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.2943 - accuracy: 0.4656\n",
      "Epoch 7/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 2.0647 - accuracy: 0.5045\n",
      "Epoch 7: loss improved from 2.29429 to 2.06458, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 2.0646 - accuracy: 0.5046\n",
      "Epoch 8/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.8773 - accuracy: 0.5379\n",
      "Epoch 8: loss improved from 2.06458 to 1.87731, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.8773 - accuracy: 0.5379\n",
      "Epoch 9/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.7316 - accuracy: 0.5653\n",
      "Epoch 9: loss improved from 1.87731 to 1.73141, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.7314 - accuracy: 0.5653\n",
      "Epoch 10/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.6233 - accuracy: 0.5860\n",
      "Epoch 10: loss improved from 1.73141 to 1.62327, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.6233 - accuracy: 0.5860\n",
      "Epoch 11/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.5343 - accuracy: 0.6026\n",
      "Epoch 11: loss improved from 1.62327 to 1.53468, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.5347 - accuracy: 0.6025\n",
      "Epoch 12/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.4649 - accuracy: 0.6182\n",
      "Epoch 12: loss improved from 1.53468 to 1.46493, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4649 - accuracy: 0.6182\n",
      "Epoch 13/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.4024 - accuracy: 0.6281\n",
      "Epoch 13: loss improved from 1.46493 to 1.40264, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.4026 - accuracy: 0.6281\n",
      "Epoch 14/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.3514 - accuracy: 0.6410\n",
      "Epoch 14: loss improved from 1.40264 to 1.35144, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3514 - accuracy: 0.6410\n",
      "Epoch 15/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.3243 - accuracy: 0.6447\n",
      "Epoch 15: loss improved from 1.35144 to 1.32431, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.3243 - accuracy: 0.6447\n",
      "Epoch 16/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.2844 - accuracy: 0.6519\n",
      "Epoch 16: loss improved from 1.32431 to 1.28438, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.2844 - accuracy: 0.6519\n",
      "Epoch 17/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.2607 - accuracy: 0.6588\n",
      "Epoch 17: loss improved from 1.28438 to 1.26101, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.2610 - accuracy: 0.6587\n",
      "Epoch 18/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.2363 - accuracy: 0.6629\n",
      "Epoch 18: loss improved from 1.26101 to 1.23635, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2363 - accuracy: 0.6629\n",
      "Epoch 19/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.2065 - accuracy: 0.6677\n",
      "Epoch 19: loss improved from 1.23635 to 1.20651, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.2065 - accuracy: 0.6677\n",
      "Epoch 20/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.1912 - accuracy: 0.6729\n",
      "Epoch 20: loss improved from 1.20651 to 1.19168, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1917 - accuracy: 0.6727\n",
      "Epoch 21/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.1810 - accuracy: 0.6722\n",
      "Epoch 21: loss improved from 1.19168 to 1.18104, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1810 - accuracy: 0.6723\n",
      "Epoch 22/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.1678 - accuracy: 0.6761\n",
      "Epoch 22: loss improved from 1.18104 to 1.16783, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1678 - accuracy: 0.6761\n",
      "Epoch 23/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.1524 - accuracy: 0.6804\n",
      "Epoch 23: loss improved from 1.16783 to 1.15243, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1524 - accuracy: 0.6804\n",
      "Epoch 24/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.1419 - accuracy: 0.6813\n",
      "Epoch 24: loss improved from 1.15243 to 1.14193, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1419 - accuracy: 0.6814\n",
      "Epoch 25/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.1306 - accuracy: 0.6849\n",
      "Epoch 25: loss improved from 1.14193 to 1.13046, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1305 - accuracy: 0.6850\n",
      "Epoch 26/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.1201 - accuracy: 0.6870\n",
      "Epoch 26: loss improved from 1.13046 to 1.12053, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.1205 - accuracy: 0.6870\n",
      "Epoch 27/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.1099 - accuracy: 0.6889\n",
      "Epoch 27: loss improved from 1.12053 to 1.10977, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.1098 - accuracy: 0.6889\n",
      "Epoch 28/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0996 - accuracy: 0.6903\n",
      "Epoch 28: loss improved from 1.10977 to 1.09930, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0993 - accuracy: 0.6906\n",
      "Epoch 29/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0957 - accuracy: 0.6903\n",
      "Epoch 29: loss improved from 1.09930 to 1.09567, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0957 - accuracy: 0.6903\n",
      "Epoch 30/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0883 - accuracy: 0.6907\n",
      "Epoch 30: loss improved from 1.09567 to 1.08825, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0883 - accuracy: 0.6908\n",
      "Epoch 31/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0804 - accuracy: 0.6929\n",
      "Epoch 31: loss improved from 1.08825 to 1.08063, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0806 - accuracy: 0.6928\n",
      "Epoch 32/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0818 - accuracy: 0.6930\n",
      "Epoch 32: loss did not improve from 1.08063\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0822 - accuracy: 0.6929\n",
      "Epoch 33/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0732 - accuracy: 0.6958\n",
      "Epoch 33: loss improved from 1.08063 to 1.07320, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0732 - accuracy: 0.6958\n",
      "Epoch 34/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.0696 - accuracy: 0.6959\n",
      "Epoch 34: loss improved from 1.07320 to 1.06943, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0694 - accuracy: 0.6960\n",
      "Epoch 35/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0610 - accuracy: 0.6966\n",
      "Epoch 35: loss improved from 1.06943 to 1.06103, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0610 - accuracy: 0.6966\n",
      "Epoch 36/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0612 - accuracy: 0.6978\n",
      "Epoch 36: loss did not improve from 1.06103\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0612 - accuracy: 0.6978\n",
      "Epoch 37/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.0505 - accuracy: 0.7002\n",
      "Epoch 37: loss improved from 1.06103 to 1.05060, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0506 - accuracy: 0.7002\n",
      "Epoch 38/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0521 - accuracy: 0.6984\n",
      "Epoch 38: loss did not improve from 1.05060\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0524 - accuracy: 0.6983\n",
      "Epoch 39/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.0439 - accuracy: 0.6998\n",
      "Epoch 39: loss improved from 1.05060 to 1.04365, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 1.0437 - accuracy: 0.6998\n",
      "Epoch 40/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0450 - accuracy: 0.6994\n",
      "Epoch 40: loss did not improve from 1.04365\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0448 - accuracy: 0.6995\n",
      "Epoch 41/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0386 - accuracy: 0.7015\n",
      "Epoch 41: loss improved from 1.04365 to 1.03863, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0386 - accuracy: 0.7015\n",
      "Epoch 42/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0375 - accuracy: 0.7000\n",
      "Epoch 42: loss improved from 1.03863 to 1.03714, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0371 - accuracy: 0.7001\n",
      "Epoch 43/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0322 - accuracy: 0.7024\n",
      "Epoch 43: loss improved from 1.03714 to 1.03319, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0332 - accuracy: 0.7022\n",
      "Epoch 44/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 1.0300 - accuracy: 0.7039\n",
      "Epoch 44: loss improved from 1.03319 to 1.02949, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0295 - accuracy: 0.7040\n",
      "Epoch 45/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0251 - accuracy: 0.7046\n",
      "Epoch 45: loss improved from 1.02949 to 1.02515, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0251 - accuracy: 0.7046\n",
      "Epoch 46/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0243 - accuracy: 0.7036\n",
      "Epoch 46: loss improved from 1.02515 to 1.02431, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0243 - accuracy: 0.7036\n",
      "Epoch 47/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.0200 - accuracy: 0.7046\n",
      "Epoch 47: loss improved from 1.02431 to 1.02001, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0200 - accuracy: 0.7046\n",
      "Epoch 48/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0139 - accuracy: 0.7050\n",
      "Epoch 48: loss improved from 1.02001 to 1.01387, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0139 - accuracy: 0.7050\n",
      "Epoch 49/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0143 - accuracy: 0.7063\n",
      "Epoch 49: loss did not improve from 1.01387\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0153 - accuracy: 0.7060\n",
      "Epoch 50/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0144 - accuracy: 0.7056\n",
      "Epoch 50: loss did not improve from 1.01387\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0146 - accuracy: 0.7056\n",
      "Epoch 51/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 1.0089 - accuracy: 0.7084\n",
      "Epoch 51: loss improved from 1.01387 to 1.00882, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 1.0088 - accuracy: 0.7085\n",
      "Epoch 52/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0090 - accuracy: 0.7054\n",
      "Epoch 52: loss did not improve from 1.00882\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0090 - accuracy: 0.7054\n",
      "Epoch 53/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0053 - accuracy: 0.7090\n",
      "Epoch 53: loss improved from 1.00882 to 1.00534, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0053 - accuracy: 0.7090\n",
      "Epoch 54/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 1.0017 - accuracy: 0.7076\n",
      "Epoch 54: loss improved from 1.00534 to 1.00185, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 1.0018 - accuracy: 0.7076\n",
      "Epoch 55/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 1.0035 - accuracy: 0.7077\n",
      "Epoch 55: loss did not improve from 1.00185\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 1.0035 - accuracy: 0.7077\n",
      "Epoch 56/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9977 - accuracy: 0.7088\n",
      "Epoch 56: loss improved from 1.00185 to 0.99771, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9977 - accuracy: 0.7088\n",
      "Epoch 57/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9912 - accuracy: 0.7118\n",
      "Epoch 57: loss improved from 0.99771 to 0.99119, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9912 - accuracy: 0.7118\n",
      "Epoch 58/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9919 - accuracy: 0.7106\n",
      "Epoch 58: loss did not improve from 0.99119\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9924 - accuracy: 0.7103\n",
      "Epoch 59/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9954 - accuracy: 0.7094\n",
      "Epoch 59: loss did not improve from 0.99119\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9954 - accuracy: 0.7094\n",
      "Epoch 60/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9918 - accuracy: 0.7108\n",
      "Epoch 60: loss did not improve from 0.99119\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9916 - accuracy: 0.7109\n",
      "Epoch 61/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9882 - accuracy: 0.7111\n",
      "Epoch 61: loss improved from 0.99119 to 0.98774, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9877 - accuracy: 0.7112\n",
      "Epoch 62/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9898 - accuracy: 0.7114\n",
      "Epoch 62: loss did not improve from 0.98774\n",
      "718/718 [==============================] - 11s 16ms/step - loss: 0.9897 - accuracy: 0.7115\n",
      "Epoch 63/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9873 - accuracy: 0.7124\n",
      "Epoch 63: loss improved from 0.98774 to 0.98730, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9873 - accuracy: 0.7124\n",
      "Epoch 64/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 0.9850 - accuracy: 0.7111\n",
      "Epoch 64: loss improved from 0.98730 to 0.98445, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9844 - accuracy: 0.7112\n",
      "Epoch 65/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9846 - accuracy: 0.7111\n",
      "Epoch 65: loss did not improve from 0.98445\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9846 - accuracy: 0.7111\n",
      "Epoch 66/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9797 - accuracy: 0.7117\n",
      "Epoch 66: loss improved from 0.98445 to 0.97966, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9797 - accuracy: 0.7117\n",
      "Epoch 67/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9776 - accuracy: 0.7137\n",
      "Epoch 67: loss improved from 0.97966 to 0.97761, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9776 - accuracy: 0.7137\n",
      "Epoch 68/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9767 - accuracy: 0.7132\n",
      "Epoch 68: loss improved from 0.97761 to 0.97586, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9759 - accuracy: 0.7134\n",
      "Epoch 69/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9748 - accuracy: 0.7138\n",
      "Epoch 69: loss improved from 0.97586 to 0.97491, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9749 - accuracy: 0.7139\n",
      "Epoch 70/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9706 - accuracy: 0.7120\n",
      "Epoch 70: loss improved from 0.97491 to 0.97099, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9710 - accuracy: 0.7120\n",
      "Epoch 71/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9731 - accuracy: 0.7127\n",
      "Epoch 71: loss did not improve from 0.97099\n",
      "718/718 [==============================] - 14s 20ms/step - loss: 0.9730 - accuracy: 0.7127\n",
      "Epoch 72/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9720 - accuracy: 0.7131\n",
      "Epoch 72: loss did not improve from 0.97099\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 0.9720 - accuracy: 0.7131\n",
      "Epoch 73/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9683 - accuracy: 0.7144\n",
      "Epoch 73: loss improved from 0.97099 to 0.96828, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 17ms/step - loss: 0.9683 - accuracy: 0.7144\n",
      "Epoch 74/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9649 - accuracy: 0.7168\n",
      "Epoch 74: loss improved from 0.96828 to 0.96487, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9649 - accuracy: 0.7169\n",
      "Epoch 75/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9659 - accuracy: 0.7147\n",
      "Epoch 75: loss did not improve from 0.96487\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9659 - accuracy: 0.7147\n",
      "Epoch 76/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 0.9666 - accuracy: 0.7128\n",
      "Epoch 76: loss did not improve from 0.96487\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9659 - accuracy: 0.7129\n",
      "Epoch 77/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9653 - accuracy: 0.7152\n",
      "Epoch 77: loss did not improve from 0.96487\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9650 - accuracy: 0.7154\n",
      "Epoch 78/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9608 - accuracy: 0.7158\n",
      "Epoch 78: loss improved from 0.96487 to 0.96045, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9605 - accuracy: 0.7160\n",
      "Epoch 79/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 0.9626 - accuracy: 0.7147\n",
      "Epoch 79: loss did not improve from 0.96045\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9627 - accuracy: 0.7147\n",
      "Epoch 80/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9654 - accuracy: 0.7152\n",
      "Epoch 80: loss did not improve from 0.96045\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9654 - accuracy: 0.7152\n",
      "Epoch 81/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9595 - accuracy: 0.7150\n",
      "Epoch 81: loss improved from 0.96045 to 0.95941, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9594 - accuracy: 0.7150\n",
      "Epoch 82/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9568 - accuracy: 0.7155\n",
      "Epoch 82: loss improved from 0.95941 to 0.95710, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9571 - accuracy: 0.7154\n",
      "Epoch 83/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9606 - accuracy: 0.7140\n",
      "Epoch 83: loss did not improve from 0.95710\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9606 - accuracy: 0.7141\n",
      "Epoch 84/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9572 - accuracy: 0.7170\n",
      "Epoch 84: loss did not improve from 0.95710\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 0.9580 - accuracy: 0.7168\n",
      "Epoch 85/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9540 - accuracy: 0.7176\n",
      "Epoch 85: loss improved from 0.95710 to 0.95371, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 0.9537 - accuracy: 0.7177\n",
      "Epoch 86/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9566 - accuracy: 0.7159\n",
      "Epoch 86: loss did not improve from 0.95371\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 0.9563 - accuracy: 0.7159\n",
      "Epoch 87/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9518 - accuracy: 0.7168\n",
      "Epoch 87: loss improved from 0.95371 to 0.95175, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 0.9518 - accuracy: 0.7168\n",
      "Epoch 88/100\n",
      "715/718 [============================>.] - ETA: 0s - loss: 0.9530 - accuracy: 0.7170\n",
      "Epoch 88: loss did not improve from 0.95175\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9528 - accuracy: 0.7171\n",
      "Epoch 89/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.7164\n",
      "Epoch 89: loss did not improve from 0.95175\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9519 - accuracy: 0.7164\n",
      "Epoch 90/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9501 - accuracy: 0.7166\n",
      "Epoch 90: loss improved from 0.95175 to 0.95047, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9505 - accuracy: 0.7164\n",
      "Epoch 91/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.7169\n",
      "Epoch 91: loss improved from 0.95047 to 0.94828, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 0.9483 - accuracy: 0.7169\n",
      "Epoch 92/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9478 - accuracy: 0.7178\n",
      "Epoch 92: loss improved from 0.94828 to 0.94776, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9478 - accuracy: 0.7178\n",
      "Epoch 93/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9494 - accuracy: 0.7172\n",
      "Epoch 93: loss did not improve from 0.94776\n",
      "718/718 [==============================] - 14s 19ms/step - loss: 0.9492 - accuracy: 0.7172\n",
      "Epoch 94/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9464 - accuracy: 0.7174\n",
      "Epoch 94: loss improved from 0.94776 to 0.94643, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9464 - accuracy: 0.7174\n",
      "Epoch 95/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9494 - accuracy: 0.7187\n",
      "Epoch 95: loss did not improve from 0.94643\n",
      "718/718 [==============================] - 13s 19ms/step - loss: 0.9493 - accuracy: 0.7187\n",
      "Epoch 96/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.7180\n",
      "Epoch 96: loss improved from 0.94643 to 0.94367, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 13s 18ms/step - loss: 0.9437 - accuracy: 0.7180\n",
      "Epoch 97/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.7164\n",
      "Epoch 97: loss did not improve from 0.94367\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9437 - accuracy: 0.7164\n",
      "Epoch 98/100\n",
      "717/718 [============================>.] - ETA: 0s - loss: 0.9439 - accuracy: 0.7174\n",
      "Epoch 98: loss did not improve from 0.94367\n",
      "718/718 [==============================] - 12s 16ms/step - loss: 0.9439 - accuracy: 0.7174\n",
      "Epoch 99/100\n",
      "716/718 [============================>.] - ETA: 0s - loss: 0.9423 - accuracy: 0.7181\n",
      "Epoch 99: loss improved from 0.94367 to 0.94178, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9418 - accuracy: 0.7183\n",
      "Epoch 100/100\n",
      "718/718 [==============================] - ETA: 0s - loss: 0.9402 - accuracy: 0.7172\n",
      "Epoch 100: loss improved from 0.94178 to 0.94022, saving model to rnn_final_model.h5\n",
      "718/718 [==============================] - 12s 17ms/step - loss: 0.9402 - accuracy: 0.7172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f04df72b30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = final_model()\n",
    "final_model.fit(\n",
    "    x,\n",
    "    labels_encoded,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"loss\", patience=10, min_delta=0.0001, restore_best_weights=True),\n",
    "        ModelCheckpoint(\"rnn_final_model.h5\", monitor=\"loss\", save_best_only=True, verbose=1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_N_words_unique(seed_texts, top_p=1, N_words=10):\n",
    "    generated_texts = []\n",
    "    for seed_text in seed_texts:\n",
    "        current_generated_text = seed_text\n",
    "        for i in range(N_words):\n",
    "            seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                [seed_sequence], maxlen=max_sequence_len - 1\n",
    "            )\n",
    "            predictions = final_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            cumulative_probs = np.cumsum(predictions[sorted_indices])\n",
    "            selected_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            selected_probs = predictions[selected_indices] / np.sum(\n",
    "                predictions[selected_indices]\n",
    "            )\n",
    "\n",
    "            next_index = np.random.choice(selected_indices, p=selected_probs)\n",
    "            next_word = tokenizer.index_word[next_index]\n",
    "\n",
    "            if (\n",
    "                next_word is None\n",
    "                or next_word == \"end_token\"\n",
    "                or len(current_generated_text.split()) >= N_words + len(seed_text)\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            current_generated_text += \" \" + next_word\n",
    "            seed_text += \" \" + next_word\n",
    "\n",
    "        generated_texts.append(current_generated_text)\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_texts = [\n",
    "    \"embrace each day\",\n",
    "    \"radiate some\",\n",
    "    \"believe that\",\n",
    "    \"life's actual purpose is\",\n",
    "    \"dance through each and every\",\n",
    "    \"let your time and energy\",\n",
    "    \"every person is\",\n",
    "    \"our country Singapore is\",\n",
    "    \"planet earth is\",\n",
    "    \"morning and evening would make it\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embrace each day with a heart full of gratitude for it will lift\n",
      "radiate some authenticity for it is the essence of true beauty resides\n",
      "believe that truly exists and let it light up the lives of\n",
      "life's actual purpose is a tapestry woven with threads of love and laughter for\n",
      "dance through each and every heart you touch the soul from its burdens intertwine creating\n",
      "let your time and energy to chase your dreams and aspirations and dreams take flight\n",
      "every person is a tribute to the boundless power of the heart to\n",
      "our country Singapore is a pledge to tread lightly on our planet offers of\n",
      "planet earth is a reminder of the gift of life on our planet\n",
      "morning and evening would make it ignites the world around you towards joy and contentment reside\n"
     ]
    }
   ],
   "source": [
    "predicted_texts = predict_next_N_words_unique(seed_texts)\n",
    "for text in predicted_texts:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in .h5 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=max_sequence_len - 1))\n",
    "    for i in range(2):\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(LayerNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = final_model()\n",
    "final_model.load_weights(\"rnn_final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_N_words_unique(seed_texts, top_p=1, N_words=10):\n",
    "    generated_texts = []\n",
    "    for seed_text in seed_texts:\n",
    "        current_generated_text = seed_text\n",
    "        for i in range(N_words):\n",
    "            seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                [seed_sequence], maxlen=max_sequence_len - 1\n",
    "            )\n",
    "            predictions = final_model.predict(padded_sequence, verbose=0)[0]\n",
    "\n",
    "            sorted_indices = np.argsort(predictions)[::-1]\n",
    "            cumulative_probs = np.cumsum(predictions[sorted_indices])\n",
    "            selected_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            selected_probs = predictions[selected_indices] / np.sum(\n",
    "                predictions[selected_indices]\n",
    "            )\n",
    "\n",
    "            next_index = np.random.choice(selected_indices, p=selected_probs)\n",
    "            next_word = tokenizer.index_word[next_index]\n",
    "\n",
    "            if (\n",
    "                next_word is None\n",
    "                or next_word == \"end_token\"\n",
    "                or len(current_generated_text.split()) >= N_words + len(seed_text)\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            current_generated_text += \" \" + next_word\n",
    "            seed_text += \" \" + next_word\n",
    "\n",
    "        generated_texts.append(current_generated_text)\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embrace each day with a heart full of gratitude for it will lift\n",
      "radiate some confidence and let it be the beacon that brightens the\n",
      "believe that truly exists and you will conquer any challenge brings new\n",
      "life's actual purpose is a testament to the beauty of our uniqueness and love\n",
      "dance through each and every breath let gratitude fill your lungs and heart this morning\n",
      "let your time and energy to chase your dreams and aspirations and actions be the\n",
      "every person is a testament to our commitment to our planet's future of\n",
      "our country Singapore is a pledge to tread lightly on our planet offers of\n",
      "planet earth is a step towards miracles the extraordinary moments that defy logic\n",
      "morning and evening would make it spreads like wildfire to sparks change and wisdom and love\n"
     ]
    }
   ],
   "source": [
    "predicted_texts = predict_next_N_words_unique(seed_texts)\n",
    "for text in predicted_texts:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
