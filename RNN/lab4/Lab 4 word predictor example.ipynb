{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67aa7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 quote\n",
      "0       The sun sets, painting the sky with warm hues.\n",
      "1       Birds chirp, announcing the dawn of a new day.\n",
      "2           Waves crash, a symphony of nature's power.\n",
      "3       Leaves rustle, whispering secrets to the wind.\n",
      "4        Raindrops dance, a gentle rhythm on the roof.\n",
      "..                                                 ...\n",
      "495  Gamma-ray bursts illuminate, the cosmos in bri...\n",
      "496  Solar flares surge, in fiery displays of solar...\n",
      "497  Interstellar dust dances, in the cosmic ballet...\n",
      "498  Cosmic strings resonate, with the symphony of ...\n",
      "499  Stellar remnants whisper, the tales of ancient...\n",
      "\n",
      "[500 rows x 1 columns]\n",
      "There are 500 quotes in this dataset.\n",
      "['The sun sets, painting the sky with warm hues.', 'Birds chirp, announcing the dawn of a new day.', \"Waves crash, a symphony of nature's power.\", 'Leaves rustle, whispering secrets to the wind.', 'Raindrops dance, a gentle rhythm on the roof.']\n"
     ]
    }
   ],
   "source": [
    "# Read data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('lab4 data.csv')\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Convert data to a list.\n",
    "data = list(data.quote.values)\n",
    "print(f'There are {len(data)} quotes in this dataset.')\n",
    "print(data[:5]) # Print the first five quotes in your list. If you print everything ... it will blow up ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76724dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'of': 2, 'in': 3, 'climate': 4, 'and': 5, 'cosmic': 6, 'a': 7, 'like': 8, 'change': 9, 'to': 10, 'with': 11, 'stars': 12, 'whispers': 13, 'night': 14, 'sky': 15, 'on': 16, 'sea': 17, 'time': 18, 'echoes': 19, 'energy': 20, 'shadows': 21, 'dreams': 22, 'through': 23, 'secrets': 24, 'stardust': 25, 'leaves': 26, 'for': 27, 'dance': 28, 'clouds': 29, 'their': 30, 'from': 31, 'ancient': 32, 'solar': 33, 'tales': 34, 'universe': 35, 'snowflakes': 36, 'earth': 37, 'laughter': 38, 'whisper': 39, 'carbon': 40, 'tears': 41, 'embrace': 42, 'petals': 43, 'voices': 44, 'heart': 45, 'distant': 46, 'celestial': 47, 'rising': 48, 'emissions': 49, 'ecosystems': 50, 'flames': 51, 'rivers': 52, 'carry': 53, 'thunder': 54, 'weave': 55, 'tapestry': 56, 'resonate': 57, 'water': 58, 'exoplanets': 59, 'wind': 60, 'flicker': 61, 'butterflies': 62, 'wings': 63, 'breeze': 64, 'breezes': 65, 'linger': 66, 'rainbows': 67, 'footsteps': 68, 'sunbeams': 69, 'weaves': 70, 'cradle': 71, 'life': 72, 'levels': 73, 'renewable': 74, 'interstellar': 75, 'stellar': 76, 'revealing': 77, 'lightning': 78, 'oceans': 79, 'as': 80, 'caress': 81, 'morning': 82, 'autumn': 83, 'cosmos': 84, 'patterns': 85, 'ice': 86, 'global': 87, 'warming': 88, 'communities': 89, 'waves': 90, 'symphony': 91, 'world': 92, 'white': 93, 'stories': 94, 'flutter': 95, 'delicate': 96, 'into': 97, 'canvas': 98, 'bridge': 99, 'light': 100, 'fabric': 101, 'breath': 102, \"heart's\": 103, 'weather': 104, 'events': 105, 'ocean': 106, 'coastal': 107, 'greenhouse': 108, 'resources': 109, 'loss': 110, 'impacts': 111, 'threaten': 112, 'affects': 113, 'action': 114, 'sustainable': 115, 'dark': 116, 'bursts': 117, 'sun': 118, 'painting': 119, 'raindrops': 120, 'leaving': 121, 'its': 122, 'joy': 123, 'glow': 124, 'confetti': 125, 'part': 126, 'blue': 127, 'echo': 128, 'arms': 129, 'cleanse': 130, \"night's\": 131, 'by': 132, 'fade': 133, 'soul': 134, 'serenade': 135, 'slumber': 136, 'moments': 137, 'ballet': 138, 'bear': 139, 'space': 140, 'exacerbates': 141, 'gas': 142, 'adaptation': 143, 'measures': 144, 'involves': 145, 'black': 146, 'holes': 147, 'quasars': 148, 'supernovae': 149, 'pulsars': 150, 'asteroids': 151, 'comets': 152, 'rays': 153, 'gamma': 154, 'ray': 155, 'flares': 156, 'dust': 157, 'strings': 158, 'hues': 159, \"nature's\": 160, 'rustle': 161, 'gentle': 162, 'rhythm': 163, 'fall': 164, 'drift': 165, 'journey': 166, 'human': 167, 'depths': 168, 'align': 169, 'embers': 170, 'storm': 171, 'beckon': 172, 'gather': 173, 'history': 174, 'love': 175, 'entwine': 176, 'glisten': 177, 'fireflies': 178, 'ages': 179, 'pirouette': 180, 'galaxies': 181, 'dances': 182, 'tell': 183, 'dew': 184, 'weeps': 185, 'illuminate': 186, 'promise': 187, 'witness': 188, 'rain': 189, 'tender': 190, 'stretch': 191, 'canyon': 192, 'across': 193, 'air': 194, 'temperatures': 195, 'rise': 196, 'deforestation': 197, 'harms': 198, 'is': 199, 'affecting': 200, 'melting': 201, 'are': 202, 'agriculture': 203, 'biodiversity': 204, 'combat': 205, 'transitioning': 206, 'reduction': 207, 'practices': 208, 'include': 209, 'reduce': 210, 'planetary': 211, 'neutron': 212, 'potential': 213, 'microwave': 214, 'background': 215, 'galactic': 216, 'dwarfs': 217, 'nebulae': 218, 'creation': 219, 'remnants': 220, 'warm': 221, 'power': 222, 'twinkle': 223, 'diamonds': 224, 'flow': 225, 'silent': 226, 'smiles': 227, 'warmth': 228, 'rumbles': 229, 'sway': 230, 'fading': 231, 'silence': 232, 'choreography': 233, 'hearts': 234, 'pierce': 235, 'shimmer': 236, 'against': 237, 'velvet': 238, 'lands': 239, 'land': 240, 'reaching': 241, 'radiate': 242, 'bass': 243, 'note': 244, 'breathe': 245, 'void': 246, 'descend': 247, 'surface': 248, 'lullaby': 249, 'realm': 250, 'ethereal': 251, 'waltz': 252, 'changing': 253, 'mysteries': 254, 'eternity': 255, 'spiral': 256, 'paint': 257, 'electric': 258, 'hum': 259, 'retreat': 260, \"garden's\": 261, 'color': 262, 'cascade': 263, 'stones': 264, 'shells': 265, 'flit': 266, 'living': 267, 'reality': 268, 'chambers': 269, 'level': 270, 'fossil': 271, 'fuels': 272, 'reduces': 273, 'marine': 274, 'threatens': 275, 'availability': 276, 'infrastructure': 277, 'contribute': 278, 'extreme': 279, 'disrupt': 280, 'arctic': 281, 'vulnerable': 282, 'coral': 283, 'requires': 284, 'resilient': 285, 'changes': 286, 'mitigation': 287, 'essential': 288, 'investments': 289, 'transportation': 290, 'nurseries': 291, 'an': 292, 'fireworks': 293, 'unimaginable': 294, 'collisions': 295, 'elements': 296, 'atmospheres': 297, 'announcing': 298, 'dawn': 299, 'new': 300, 'day': 301, 'roof': 302, 'casting': 303, 'walls': 304, 'bloom': 305, 'vibrant': 306, 'mountains': 307, 'stand': 308, 'mark': 309, 'inspire': 310, 'spirit': 311, 'illuminating': 312, 'darkness': 313, 'strength': 314, 'trees': 315, 'close': 316, 'candles': 317, 'snow': 318, 'roots': 319, 'soil': 320, 'soothe': 321, 'balm': 322, 'imprints': 323, 'music': 324, \"soul's\": 325, \"autumn's\": 326, \"shore's\": 327, 'waiting': 328, 'hearth': 329, 'past': 330, 'memory': 331, 'scent': 332, 'off': 333, 'chill': 334, 'unfurl': 335, 'loom': 336, 'ballets': 337, 'stability': 338, 'passage': 339, 'play': 340, 'twilight': 341, 'twirl': 342, \"earth's\": 343, 'harmonious': 344, 'murmurs': 345, 'gleam': 346, 'forgotten': 347, 'moonbeams': 348, 'vast': 349, 'expanse': 350, 'spring': 351, 'windows': 352, 'existence': 353, 'reach': 354, 'fingers': 355, 'golden': 356, 'seasons': 357, 'emerge': 358, 'drops': 359, 'mingles': 360, \"winter's\": 361, 'lost': 362, 'descent': 363, 'spin': 364, 'kisses': 365, 'edge': 366, 'childhood': 367, \"morning's\": 368, 'cotton': 369, 'midnight': 370, 'summer': 371, 'ear': 372, 'gold': 373, 'glide': 374, 'gossamer': 375, 'float': 376, 'cradles': 377, 'before': 378, 'garden': 379, 'glimmers': 380, 'petal': 381, 'old': 382, 'lanterns': 383, \"light's\": 384, 'tumble': 385, 'sketches': 386, 'sing': 387, 'ballad': 388, 'unfurls': 389, 'banner': 390, 'lingers': 391, 'rhythmic': 392, 'polar': 393, 'heat': 394, 'leads': 395, 'acidification': 396, 'disrupts': 397, 'food': 398, 'security': 399, 'caps': 400, 'increase': 401, 'leading': 402, 'heatwaves': 403, 'accelerates': 404, 'intensifies': 405, 'pollution': 406, 'refugees': 407, 'contributes': 408, 'spread': 409, 'glacier': 410, 'currents': 411, 'sheets': 412, 'reefs': 413, 'illnesses': 414, 'release': 415, 'stored': 416, 'rainfall': 417, 'scarcity': 418, 'result': 419, 'economic': 420, 'sources': 421, 'traditional': 422, 'weakens': 423, \"ecosystems'\": 424, 'resilience': 425, 'systems': 426, 'environmental': 427, 'building': 428, 'reducing': 429, 'technologies': 430, 'help': 431, 'species': 432, 'education': 433, 'glaciers': 434, 'at': 435, 'wildlife': 436, 'pricing': 437, 'industries': 438, 'disaster': 439, 'freshwater': 440, 'promote': 441, 'efficiency': 442, 'degradation': 443, 'includes': 444, 'brilliance': 445, 'matter': 446, 'enigmatic': 447, 'pulse': 448, 'medium': 449, 'vibrate': 450, 'sculpt': 451, 'cool': 452, 'reaches': 453, 'expansion': 454, \"system's\": 455, 'bombard': 456, 'dazzle': 457, 'landscape': 458, 'long': 459, 'brilliant': 460, 'system': 461, 'astrophysical': 462, 'fiery': 463, 'displays': 464, 'evolution': 465, 'resonance': 466, 'clusters': 467, 'propels': 468, 'traverse': 469, 'surge': 470, 'sets': 471, 'birds': 472, 'chirp': 473, 'crash': 474, 'whispering': 475, 'blanketing': 476, 'flowers': 477, 'palette': 478, 'shaping': 479, 'constant': 480, 'sentinels': 481, 'passes': 482, 'everything': 483, 'fueling': 484, 'universal': 485, 'language': 486, 'expressing': 487, 'emotion': 488, 'sharing': 489, 'brighten': 490, 'gesture': 491, 'kindness': 492, 'raw': 493, 'strikes': 494, 'roar': 495, 'reminder': 496, 'boundless': 497, 'caressing': 498, 'lengthen': 499, 'draws': 500, 'shore': 501, 'soft': 502, 'melts': 503, 'giving': 504, 'way': 505, 'rebirth': 506, 'holding': 507, 'onto': 508, 'delve': 509, 'seeking': 510, 'sustenance': 511, 'arch': 512, 'between': 513, 'troubled': 514, 'sand': 515, 'flows': 516, 'stirring': 517, \"twilight's\": 518, 'crunch': 519, 'beneath': 520, 'canopy': 521, 'above': 522, 'blend': 523, 'harmonizing': 524, 'song': 525, 'hushes': 526, 'subsides': 527, 'washing': 528, 'away': 529, 'silver': 530, 'lining': 531, 'comfort': 532, 'stillness': 533, 'blankets': 534, 'quietude': 535, 'chamber': 536, 'connect': 537, 'bridging': 538, 'heal': 539, 'releasing': 540, 'pent': 541, 'up': 542, 'sorrows': 543, 'bubbles': 544, 'contagious': 545, 'mirth': 546, 'cloak': 547, 'inviting': 548, 'watery': 549, 'warding': 550, 'kissed': 551, 'guiding': 552, 'lights': 553, 'congregation': 554, 'meander': 555, 'heavens': 556, 'marches': 557, 'footprints': 558, 'igniting': 559, 'heals': 560, 'wounded': 561, 'souls': 562, 'purging': 563, 'inner': 564, 'turmoil': 565, 'share': 566, 'held': 567, 'spreading': 568, 'grumbles': 569, 'pierces': 570, 'ebb': 571, 'carrying': 572, 'wake': 573, 'swallowed': 574, 'shift': 575, 'morphing': 576, 'crinkle': 577, 'arrival': 578, 'orchestrating': 579, 'pictures': 580, 'adorning': 581, 'surrendering': 582, 'anchor': 583, 'providing': 584, 'harmony': 585, 'calm': 586, 'soothing': 587, 'restless': 588, 'melodies': 589, 'grateful': 590, 'longing': 591, 'creating': 592, 'symphonies': 593, 'curtain': 594, 'wane': 595, 'creeps': 596, 'quench': 597, 'thirst': 598, 'disperse': 599, 'vanish': 600, 'traces': 601, 'magic': 602, 'resound': 603, 'bouncing': 604, 'dewdrops': 605, 'jewels': 606, 'reverberate': 607, 'still': 608, 'waters': 609, 'sparkles': 610, 'autumnal': 611, 'promises': 612, 'weary': 613, 'banks': 614, 'melodic': 615, 'corridors': 616, 'morph': 617, 'ever': 618, 'converge': 619, 'constellation': 620, 'patter': 621, 'percussion': 622, 'fire': 623, 'crackles': 624, 'hearths': 625, 'tangle': 626, 'locks': 627, 'auburn': 628, 'hair': 629, 'ripples': 630, 'harmonize': 631, 'unity': 632, 'graceful': 633, 'cheek': 634, 'corners': 635, 'blooming': 636, 'gardens': 637, 'quilt': 638, 'touch': 639, 'fragile': 640, 'cascades': 641, 'waterfall': 642, 'starlight': 643, 'constellations': 644, 'own': 645, 'moon': 646, \"wind's\": 647, 'hope': 648, 'cathedral': 649, 'wisps': 650, 'cobblestone': 651, 'meadow': 652, 'nature': 653, 'alight': 654, 'imagination': 655, 'spins': 656, 'clock': 657, 'reflecting': 658, 'whispered': 659, 'flickering': 660, 'firelight': 661, 'flowing': 662, 'veins': 663, 'settles': 664, 'distance': 665, 'paints': 666, 'intertwine': 667, 'sleep': 668, 'leave': 669, 'path': 670, 'spirits': 671, 'awaken': 672, 'dormant': 673, 'arc': 674, \"breeze's\": 675, 'fragrant': 676, 'sparkle': 677, \"window's\": 678, 'glass': 679, 'shimmers': 680, 'melt': 681, 'contributing': 682, 'burning': 683, 'trap': 684, 'atmosphere': 685, 'causing': 686, 'more': 687, 'frequent': 688, 'severe': 689, 'absorbing': 690, 'forests': 691, 'exacerbating': 692, 'glacial': 693, 'visible': 694, 'sign': 695, \"change's\": 696, 'impact': 697, 'crop': 698, 'yields': 699, 'displaces': 700, 'damages': 701, 'gases': 702, 'amplifies': 703, 'effect': 704, 'hurricanes': 705, 'linked': 706, 'droughts': 707, 'endangering': 708, 'wildfires': 709, 'destruction': 710, 'fisheries': 711, 'flee': 712, 'areas': 713, 'impacted': 714, 'disease': 715, 'vectors': 716, 'mosquitoes': 717, 'permafrost': 718, 'releases': 719, 'methane': 720, 'potent': 721, 'disproportionately': 722, 'cities': 723, 'miami': 724, 'york': 725, 'millions': 726, 'people': 727, 'disrupted': 728, 'globally': 729, 'dioxide': 730, 'respiratory': 731, 'forest': 732, 'fires': 733, 'amplifying': 734, 'causes': 735, 'losses': 736, 'suffering': 737, 'paris': 738, 'agreement': 739, 'aims': 740, 'scale': 741, 'effects': 742, 'felt': 743, 'all': 744, 'continents': 745, 'footprint': 746, 'crucial': 747, 'mitigating': 748, 'low': 749, 'lying': 750, 'island': 751, 'nations': 752, 'farming': 753, 'decline': 754, 'snowpack': 755, 'influence': 756, 'face': 757, 'displacement': 758, 'due': 759, 'conditions': 760, 'experience': 761, 'rapid': 762, 'offer': 763, 'alternatives': 764, 'scientists': 765, 'study': 766, 'cores': 767, 'understand': 768, 'historical': 769, 'reforestation': 770, 'efforts': 771, 'sequester': 772, 'international': 773, 'cooperation': 774, 'agreements': 775, 'vital': 776, 'mangroves': 777, 'invasive': 778, 'shellfish': 779, 'reef': 780, 'frequency': 781, 'intensity': 782, 'awareness': 783, 'fostering': 784, 'fresh': 785, 'pollutants': 786, 'decades': 787, 'extinction': 788, 'create': 789, 'green': 790, 'jobs': 791, 'rate': 792, 'double': 793, 'average': 794, 'migration': 795, 'ability': 796, 'adapt': 797, 'growing': 798, 'encourages': 799, 'improved': 800, 'preparedness': 801, 'quality': 802, 'supply': 803, 'downstream': 804, 'adoption': 805, 'dependency': 806, 'conflicts': 807, 'some': 808, 'regions': 809, 'forestry': 810, 'hybrid': 811, 'vehicles': 812, 'wetlands': 813, 'precipitation': 814, 'capture': 815, 'storage': 816, 'technology': 817, 'can': 818, 'agricultural': 819, 'productivity': 820, 'protection': 821, 'flood': 822, 'defenses': 823, 'growth': 824, 'sustainability': 825, 'strategies': 826, 'improvements': 827, 'alter': 828, 'natural': 829, 'habitats': 830, 'desertification': 831, 'options': 832, 'health': 833, 'related': 834, 'indigenous': 835, 'knowledge': 836, 'empowers': 837, 'individuals': 838, 'take': 839, 'risk': 840, 'flooding': 841, 'incentivizes': 842, 'livelihoods': 843, 'farmers': 844, 'fishermen': 845, 'consumption': 846, 'aquifers': 847, 'erosion': 848, 'use': 849, 'management': 850, 'clean': 851, 'production': 852, 'public': 853, 'pinwheels': 854, 'nebulas': 855, 'birthing': 856, 'devour': 857, 'insatiable': 858, 'hunger': 859, 'blaze': 860, 'beacons': 861, 'explode': 862, 'scattering': 863, 'shrouds': 864, 'hiding': 865, 'presence': 866, 'sending': 867, 'signals': 868, 'orbit': 869, 'streak': 870, 'trails': 871, 'wonder': 872, 'race': 873, 'flare': 874, 'drifts': 875, 'resonating': 876, 'newborn': 877, 'rings': 878, 'speed': 879, 'harbor': 880, 'early': 881, 'tapestries': 882, 'chaos': 883, 'beauty': 884, 'insignificance': 885, 'warp': 886, 'very': 887, 'forge': 888, 'expands': 889, 'driving': 890, \"universe's\": 891, 'accelerating': 892, 'beacon': 893, 'lighthouses': 894, 'allure': 895, 'exploration': 896, 'far': 897, 'oort': 898, 'cloud': 899, 'transient': 900, 'unleash': 901, 'torrents': 902, 'charged': 903, 'particles': 904, 'sculpts': 905, 'ripple': 906, 'winds': 907, 'shape': 908, 'nebular': 909, 'collapse': 910, 'densities': 911, 'hold': 912, 'be': 913, 'unraveled': 914, 'gone': 915, 'birthplaces': 916, 'future': 917, 'conceal': 918, 'gravitational': 919, 'seed': 920, 'invisible': 921, 'web': 922, 'beat': 923, 'heartbeats': 924, 'homes': 925, 'scars': 926, 'eons': 927, 'edges': 928, 'signatures': 929, 'signal': 930, 'cataclysms': 931, 'dying': 932, 'erupt': 933, 'extinguished': 934, 'pulsate': 935, 'metronomes': 936, 'tantalize': 937, 'possibilities': 938, 'extraterrestrial': 939, 'hums': 940, 'congregations': 941, 'legacy': 942, 'sentinel': 943, 'crossroads': 944, 'furthest': 945, 'realms': 946, 'elemental': 947, 'pulsating': 948, 'beams': 949, 'diversity': 950, 'voyage': 951, 'fringes': 952, 'haste': 953, 'display': 954, \"sun's\": 955, 'fury': 956, 'intricacy': 957, 'essence': 958, \"life's\": 959, 'entice': 960, 'habitability': 961, 'primordial': 962, 'reveal': 963, 'veil': 964, 'scatter': 965, 'alchemy': 966, 'toward': 967, 'uncertain': 968, 'fate': 969, 'timekeepers': 970, 'intrigue': 971, 'diverse': 972, 'etched': 973, 'stone': 974, 'frozen': 975, 'frontier': 976, 'brief': 977, 'luminaries': 978}\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation.\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer() # Instantiate a Tokenizer object.\n",
    "tokenizer.fit_on_texts(data) # Fit your Tokenizer on your dote that index starts with 1, not 0.ataset. This will map words in your dataset to integers.\n",
    "print(tokenizer.word_index) # N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb71aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n"
     ]
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index) + 1 # Because index 0 is reserved for padding.\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba82c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun sets, painting the sky with warm hues.\n",
      "[1, 118, 471, 119, 1, 15, 11, 221, 159]\n",
      "\n",
      "Birds chirp, announcing the dawn of a new day.\n",
      "[472, 473, 298, 1, 299, 2, 7, 300, 301]\n",
      "\n",
      "Waves crash, a symphony of nature's power.\n",
      "[90, 474, 7, 91, 2, 160, 222]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Under the Tokenizer object, you can convert texts to sequences of integers.\n",
    "# Be careful of the indexing of your data list, and how you use the \".texts_to_sequences\" method.\n",
    "\n",
    "print(data[0])\n",
    "print(tokenizer.texts_to_sequences([data[0]])[0])\n",
    "print()\n",
    "\n",
    "print(data[1])\n",
    "print(tokenizer.texts_to_sequences([data[1]])[0])\n",
    "print()\n",
    "\n",
    "print(data[2])\n",
    "print(tokenizer.texts_to_sequences([data[2]])[0]) #There is this extra [0] at the end, because the output is a list of list.\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ef12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of quotes: 500\n"
     ]
    }
   ],
   "source": [
    "# The following for-loop converts each of the 500 quotes from texts to integers.\n",
    "\n",
    "input_sequence = []\n",
    "\n",
    "for datapoint in data:\n",
    "    token_list = tokenizer.texts_to_sequences([datapoint])[0] # Again, this extra [0] is because the output\n",
    "                                                              # is a list of list.\n",
    "    input_sequence.append(token_list)\n",
    "\n",
    "print(f'Total number of quotes: {len(input_sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e673d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 3111\n"
     ]
    }
   ],
   "source": [
    "# Let us now create input-output pairs.\n",
    "# The input is, say, every one word and the corresponding output is the following word.\n",
    "# So, let us create a list of \"two words\".\n",
    "\n",
    "input_output_pair1 = []\n",
    "for datapoint in input_sequence:\n",
    "    for i in range(len(datapoint) - 1): # Why -1?\n",
    "        input_output_pair1.append(datapoint[i:i + 2])\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pair1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d32e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 2611\n"
     ]
    }
   ],
   "source": [
    "# We can similarly build input-output pairs where the input has two words.\n",
    "# The input is say, every two words and the corresponding output is the third word.\n",
    "# So, let us create a list of \"three words\".\n",
    "\n",
    "input_output_pair2 = []\n",
    "for datapoint in input_sequence:\n",
    "    for i in range(len(datapoint) - 2): # Why -2?\n",
    "        input_output_pair2.append(datapoint[i:i + 3])\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pair2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f04d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input-output pairs is: 500\n"
     ]
    }
   ],
   "source": [
    "# We can similarly build input-output pairs where the input has N-1 words.\n",
    "# The input is say, every N-1 words and the corresponding output is the last word.\n",
    "# So, let us create a list of \"N words\", i.e. the entire quote.\n",
    "\n",
    "input_output_pairN = []\n",
    "for datapoint in input_sequence:\n",
    "    input_output_pairN.append(datapoint) # Why no nested for-loop?\n",
    "        \n",
    "print(f'Total number of input-output pairs is: {len(input_output_pairN)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7d9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original quotes:\n",
      "[[1, 118, 471, 119, 1, 15, 11, 221, 159], [472, 473, 298, 1, 299, 2, 7, 300, 301]]\n",
      "\n",
      "Each quote chopped into phrases of two words:\n",
      "[[1, 118], [118, 471], [471, 119], [119, 1], [1, 15], [15, 11], [11, 221], [221, 159], [472, 473], [473, 298], [298, 1], [1, 299], [299, 2], [2, 7], [7, 300], [300, 301], [90, 474], [474, 7], [7, 91], [91, 2]]\n",
      "\n",
      "Each quote chopped into phrases of three words:\n",
      "[[1, 118, 471], [118, 471, 119], [471, 119, 1], [119, 1, 15], [1, 15, 11], [15, 11, 221], [11, 221, 159], [472, 473, 298], [473, 298, 1], [298, 1, 299], [1, 299, 2], [299, 2, 7], [2, 7, 300], [7, 300, 301], [90, 474, 7], [474, 7, 91], [7, 91, 2], [91, 2, 160], [2, 160, 222], [26, 161, 475]]\n",
      "\n",
      "Each quote itself:\n",
      "[[1, 118, 471, 119, 1, 15, 11, 221, 159], [472, 473, 298, 1, 299, 2, 7, 300, 301], [90, 474, 7, 91, 2, 160, 222], [26, 161, 475, 24, 10, 1, 60], [120, 28, 7, 162, 163, 16, 1, 302], [12, 223, 8, 224, 3, 1, 14, 15], [36, 164, 476, 1, 92, 3, 93], [51, 61, 303, 21, 16, 1, 304], [477, 305, 77, 160, 306, 478], [29, 165, 479, 94, 3, 1, 15], [52, 225, 7, 480, 166, 10, 1, 17], [307, 308, 226, 481, 2, 1, 37], [18, 482, 121, 122, 309, 16, 483], [22, 310, 484, 1, 167, 311], [38, 19, 7, 485, 486, 2, 123], [41, 164, 487, 168, 2, 488], [13, 53, 489, 24, 11, 1, 14], [227, 490, 7, 491, 2, 492, 5, 228], [54, 229, 7, 91, 2, 493, 222], [78, 494, 312, 1, 313]]\n"
     ]
    }
   ],
   "source": [
    "# Notice each quote has been chopped into phrases comprising two words, three words, or the entire quote.\n",
    "\n",
    "print('Original quotes:')\n",
    "print(input_sequence[0:2])\n",
    "print()\n",
    "print('Each quote chopped into phrases of two words:')\n",
    "print(input_output_pair1[:20])\n",
    "print()\n",
    "print('Each quote chopped into phrases of three words:')\n",
    "print(input_output_pair2[:20])\n",
    "print()\n",
    "print('Each quote itself:')\n",
    "print(input_output_pairN[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe40f5",
   "metadata": {},
   "source": [
    "# NOTE: You should expand this part to generate more input-output pairs.\n",
    "\n",
    "## This is a way of generating more training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a2391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1 118]\n",
      " [  0   0   0 ...   0 118 471]\n",
      " [  0   0   0 ...   0 471 119]\n",
      " ...\n",
      " [  0   0   0 ... 138   2 219]\n",
      " [  0   0   0 ...   2   1  35]\n",
      " [  0   0   0 ...   2  32 978]]\n"
     ]
    }
   ],
   "source": [
    "# Let us combine our sequences of different lengths by padding them so that all have the same length,\n",
    "# and then converting to a numpy array.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "input_output_pair = input_output_pair1 + input_output_pair2 + input_output_pairN\n",
    "max_sequence_len = max([len(x) for x in input_output_pair]) # This is to get the longest sequence of texts.\n",
    "                                                            # And let us pad all datapoints to be of this length.\n",
    "input_output_pair = np.array(pad_sequences(input_output_pair, maxlen=max_sequence_len, padding='pre'))\n",
    "print(input_output_pair) # All datapoints now have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc7b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   1]\n",
      " [  0   0   0 ...   0   0 118]\n",
      " [  0   0   0 ...   0   0 471]\n",
      " ...\n",
      " [  0   0   0 ...   6 138   2]\n",
      " [  0   0   0 ...  91   2   1]\n",
      " [  0   0   0 ...  34   2  32]]\n",
      "[118 471 119 ... 219  35 978]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have an array, let us slice the last word as the output.\n",
    "# The preceding words are the input.\n",
    "\n",
    "X = input_output_pair[:,:-1]\n",
    "y = input_output_pair[:,-1]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2f7aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Let us convert y to be a one-hot array.\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "y = utils.to_categorical(y, num_classes=total_words)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e72b5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "(6222, 13)\n",
      "(6222, 979)\n"
     ]
    }
   ],
   "source": [
    "print(total_words)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38db9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALLY. Let us create a simple model using LSTM.\n",
    "# NOTE: You should try various other RNN models with other architectures.\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Input is max sequence length - 1, as we've removed the last word for the label.\n",
    "input_len = max_sequence_len - 1 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer.\n",
    "model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# Add LSTM layer with 256 units.\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# Control overfitting.\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6a251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 10)            9790      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               273408    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 979)               251603    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 534801 (2.04 MB)\n",
      "Trainable params: 534801 (2.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570de845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good idea to save your weights, after spending *hours* (!?) training ...\n",
    "\n",
    "model.save_weights('lab4 weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load them later.\n",
    "\n",
    "model.load_weights('lab4 weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ea684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a function to generate next word, given some input.\n",
    "\n",
    "def predict_next_word(input_text):\n",
    "    tokens = tokenizer.texts_to_sequences([input_text])[0] # Convert your input text into integers.\n",
    "    tokens = pad_sequences([tokens], maxlen=max_sequence_len-1, padding='pre') # Pad your sequence of integers to the\n",
    "                                                                               # required length that the model accepts.\n",
    "    prediction = np.argmax(model.predict(tokens, verbose=0), axis=1) # Get the prediction of your model.\n",
    "    prediction = tokenizer.sequences_to_texts([prediction])[0] # Since the model output is an integer, use your tokenizer\n",
    "                                                               # to get the corresponding word!\n",
    "                                                               # That is the prediction of your model for the next word!\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cec86f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun sets level\n",
      "twinkle like ripples\n",
      "entwine creating level\n",
      "stellar remnants level\n",
      "solar flares souls\n"
     ]
    }
   ],
   "source": [
    "# Now let us test with some input texts.\n",
    "\n",
    "test_texts = ['sun sets',\n",
    "              'twinkle like',\n",
    "              'entwine creating',\n",
    "              'stellar remnants',\n",
    "              'solar flares']\n",
    "for text in test_texts:\n",
    "    prediction = predict_next_word(text)\n",
    "    print(text, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f08c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own function to predict the next N words.\n",
    "\n",
    "def predict_next_N_words(input_text, N_words=10):\n",
    "    return # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e45f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_next_N_words(test_texts) # ???\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e314cb1",
   "metadata": {},
   "source": [
    "# Your model will predict a fixed word, given a fixed input.\n",
    "\n",
    "## How to make your model be \"creative\", and predict different possible inputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
